//===--------------- Ripple.cpp - Expand RIpple intrinsics ----------------===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This pass expands Ripple intrinsics.
//
//===----------------------------------------------------------------------===//

#include "llvm/Transforms/Ripple/Ripple.h"
#include "llvm/ADT/SetVector.h"
#include "llvm/ADT/StringRef.h"
#include "llvm/Analysis/CFG.h"
#include "llvm/Analysis/GlobalsModRef.h"
#include "llvm/Analysis/InstructionSimplify.h"
#include "llvm/Analysis/VectorUtils.h"
#include "llvm/AsmParser/Parser.h"
#include "llvm/Bitcode/BitcodeReader.h"
#include "llvm/CodeGen/TargetLowering.h"
#include "llvm/CodeGen/TargetSubtargetInfo.h"
#include "llvm/Demangle/Demangle.h"
#include "llvm/IR/AttributeMask.h"
#include "llvm/IR/Constants.h"
#include "llvm/IR/DerivedTypes.h"
#include "llvm/IR/DiagnosticInfo.h"
#include "llvm/IR/DiagnosticPrinter.h"
#include "llvm/IR/InstIterator.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/IR/Intrinsics.h"
#include "llvm/IR/IntrinsicsRipple.h"
#include "llvm/IR/Metadata.h"
#include "llvm/IR/PatternMatch.h"
#include "llvm/Support/Regex.h"
#include "llvm/Target/TargetMachine.h"
#include "llvm/Transforms/Utils/BasicBlockUtils.h"
#include "llvm/Transforms/Utils/Cloning.h"
#include "llvm/Transforms/Utils/Evaluator.h"
#include "llvm/Transforms/Utils/LoopUtils.h"
#include <cassert>
#include <queue>

using namespace llvm;
using namespace llvm::PatternMatch;

#define DEBUG_TYPE "ripple"

static cl::opt<bool> AnalyzeRipple(
    "ripple-vectorize-analyze", cl::Hidden, cl::init(false),
    cl::desc("Analyze vector code generated by the Ripple Pass."));

static cl::opt<bool> PadToTargetSIMDLength("ripple-pad-to-target-simd",
                                           cl::Hidden, cl::init(false));

namespace llvm {
namespace RippleCL {

llvm::cl::list<std::string>
    RippleLibs("ripple-lib",
               llvm::cl::desc("Path to a ripple library file (LLVM IR)"),
               llvm::cl::Hidden /* internal option for the time being */);

} // namespace RippleCL
} // namespace llvm

namespace {

////////////////////////////////////////////////////////////////////////////////
///                             Local Classes                                ///
////////////////////////////////////////////////////////////////////////////////

class DiagnosticInfoRipple : public DiagnosticInfoGeneric {
private:
  // Used to store the message when the input Twine is not a single StringRef
  SmallVector<char, 0> ErrStr;
  // The error message
  StringRef ErrStrRef;
  // Keep a Twine to ErrStrRef for DiagnosticInfoGeneric
  Twine ErrTwine;

public:
  DiagnosticInfoRipple(DiagnosticSeverity Severity, const Twine &Msg)
      : DiagnosticInfoGeneric(nullptr, ErrTwine, Severity),
        ErrStrRef(Msg.toStringRef(ErrStr)), ErrTwine(ErrStrRef) {}
};

class DiagnosticInfoRippleWithLoc : public DiagnosticInfoGenericWithLoc {
private:
  // Used to store the message when the input Twine is not a single StringRef
  SmallVector<char, 0> ErrStr;
  // The error message
  StringRef ErrStrRef;
  // Keep a Twine to ErrStrRef for DiagnosticInfoGenericWithLoc
  Twine ErrTwine;

public:
  DiagnosticInfoRippleWithLoc(enum DiagnosticSeverity Severity,
                              const Function &Fn, const DiagnosticLocation &Loc,
                              const Twine &Msg)
      : DiagnosticInfoGenericWithLoc(ErrTwine, Fn, Loc, Severity),
        ErrStrRef(Msg.toStringRef(ErrStr)), ErrTwine(ErrStrRef) {}
};

////////////////////////////////////////////////////////////////////////////////
///                        Local Helper Functions                            ///
////////////////////////////////////////////////////////////////////////////////

#define RETURN_UNEXPECTED(expr)                                                \
  if (!expr) {                                                                 \
    return (expr).takeError();                                                 \
  }

/// @brief: Sets the insert points in `IRB` after the instruction "I".
void setInsertPointAfter(llvm::IRBuilder<> &IRB, llvm::Instruction *I) {
  if (isa<PHINode>(I) || isa<LandingPadInst>(I))
    IRB.SetInsertPoint(I->getParent()->getFirstInsertionPt());
  else
    IRB.SetInsertPoint(std::next(I->getIterator()));
}

std::optional<uint64_t> getConstantOperandValue(const Instruction *I,
                                                unsigned index) {
  if (index >= I->getNumOperands())
    return std::nullopt;
  auto *op = I->getOperand(index);
  auto *cst = dyn_cast<ConstantInt>(op);
  if (!cst)
    return std::nullopt;
  if (cst->getValue().getActiveBits() > 64)
    return std::nullopt;
  return cst->getValue().getZExtValue();
}

/// @brief Helper function that returns a pointer if the instruction *I* is an
/// IntrinsicInst w/ any of the IDs provided in *ids*, nullptr otherwise.
const IntrinsicInst *intrinsicWithId(const Instruction *I,
                                     ArrayRef<Intrinsic::ID> ids) {
  if (auto II = dyn_cast_if_present<IntrinsicInst>(I)) {
    if (is_contained(ids, II->getIntrinsicID()))
      return II;
  }
  return nullptr;
}

IntrinsicInst *intrinsicWithId(Instruction *I,
                               std::initializer_list<Intrinsic::ID> ids) {
  return const_cast<IntrinsicInst *>(
      intrinsicWithId(const_cast<const Instruction *>(I), ids));
}

/**
 * @brief Returns a SetVector, S, of basic blocks on the path from \p From to \p
 * To. The insertion order in S corresponds to the BFS traversal from \p From to
 * \p To. 'S' is inclusive of \p From and \p To.
 *
 * @param From Starting point of traversal.
 * @param To End point of traversal.
 * @return SetVector<BasicBlock *>
 *
 * \note 'To' must postdominate 'From'.
 */
SetVector<BasicBlock *>
allBasicBlocksFromToBFS(BasicBlock *From, BasicBlock *To,
                        PostDominatorTreeAnalysis::Result &PDomTree) {
  assert(From != To);
  assert(PDomTree.dominates(To, From));

  std::queue<BasicBlock *> ToProcess;
  SetVector<BasicBlock *> Visited;
  ToProcess.push(From);
  while (!ToProcess.empty()) {
    auto *BB = ToProcess.front();
    ToProcess.pop();
    Visited.insert(BB);
    if (BB == To)
      continue;
    for (auto *SI : successors(BB)) {
      if (!Visited.contains(SI)) {
        ToProcess.push(SI);
      }
    }
  }
  assert((Visited.front() == From) && (Visited.back() == To));
  return Visited;
}

Constant *getRippleNeutralReductionElement(Intrinsic::ID ID, Type *EltTy,
                                           FastMathFlags FMF) {
  assert(VPReductionIntrinsic::isVPReduction(ID) &&
         "Expecting a vp reduction intrinsic ID");
  if (auto OptIntinsic = VPIntrinsic::getFunctionalIntrinsicIDForVP(ID))
    return dyn_cast<Constant>(getReductionIdentity(*OptIntinsic, EltTy, FMF));
  else
    return nullptr;
}

/// If the debug location is the result of inlining some code, returns the
/// location where it has been inlined from, else returns the debug location
/// unchanged.
///
/// This is useful for Ripple's error reporting because we'd rather want point
/// to a location inside the function we are vectorizing instead of the header
/// where implementation details of the API would be implemented (e.g.,
/// overloading of ripple_shuffle in C++).
DebugLoc stripInliningFromDebugLoc(DebugLoc DL) {
  while (DL && DL->getInlinedAt())
    DL = DL->getInlinedAt();
  return DL;
}

/// @brief Creates a SelectInst (not attached to any BasicBlock) taking a vector
/// of @p Count i1 booleans and outputing a vector of integer type @p OutType
/// with values 1 when true and 0 when false.
SelectInst *createMaskSelectToTrueFalse(IntegerType *OutType,
                                        ElementCount Count,
                                        const Twine &Name = "") {
  auto &Context = OutType->getContext();
  return SelectInst::Create(
      ConstantVector::getSplat(Count, ConstantInt::getTrue(Context)),
      ConstantVector::getSplat(Count, ConstantInt::get(OutType, 1)),
      ConstantVector::getSplat(Count, ConstantInt::get(OutType, 0)), Name);
}

/** @brief Returns the set, S, of basic blocks such that for every X in S, there
 * is a path, P, BB .. -(P)-> .. S, such that ExecludedBB is not a part of a P.
 */
DenseSet<const BasicBlock *>
getAllReachableBBsExcluding(const BasicBlock *BB,
                            const BasicBlock *ExcludedBB) {
  std::queue<const BasicBlock *> Worklist;
  Worklist.push(BB);

  DenseSet<const BasicBlock *> ReachableBBs;

  while (!Worklist.empty()) {
    auto *FrontBB = Worklist.front();
    Worklist.pop();
    for (auto *SuccBB : successors(FrontBB)) {
      if (SuccBB == ExcludedBB)
        continue;
      if (ReachableBBs.contains(SuccBB))
        continue;
      ReachableBBs.insert(SuccBB);
      Worklist.push(SuccBB);
    }
  }
  return ReachableBBs;
}

std::pair<BasicBlock *, BasicBlock *>
getIncrementAndCleanupTargets(BasicBlock *BranchingBB, BasicBlock *PDom,
                              DominatorTreeAnalysis::Result &DT) {
  BasicBlock *BB1 = *succ_begin(BranchingBB);
  BasicBlock *BB2 = *(succ_begin(BranchingBB) + 1);

  const SmallPtrSet<BasicBlock *, 1> ExclusionSet({PDom});

  if (!isPotentiallyReachable(BB2, BranchingBB, &ExclusionSet, &DT)) {
    return {BB1, BB2};
  }
  if (!isPotentiallyReachable(BB1, BranchingBB, &ExclusionSet)) {
    return {BB2, BB1};
  }
  llvm_unreachable(
      "getIncrementAndCleanupTargets must be only called in the case of"
      " hasTrivialLoopLikeBackEdge is true.");
}

} // namespace

bool llvm::hasTrivialLoopLikeBackEdge(BasicBlock *BranchingBB, BasicBlock *PDom,
                                      DominatorTreeAnalysis::Result &DT) {
  if (succ_size(BranchingBB) != 2) {
    return false;
  }
  const BasicBlock *BB1 = *succ_begin(BranchingBB);
  const BasicBlock *BB2 = *(succ_begin(BranchingBB) + 1);

  const SmallPtrSet<BasicBlock *, 1> PDomSet({PDom});
  const SmallPtrSet<BasicBlock *, 1> BranchBBSet({BranchingBB});

  if (isPotentiallyReachable(BB1, BranchingBB, &PDomSet, &DT) &&
      !isPotentiallyReachable(BB2, BranchingBB, &PDomSet, &DT) &&
      !isPotentiallyReachable(BB1, BB2, &BranchBBSet, &DT)) {
    return getAllReachableBBsExcluding(BB1, PDom).contains(BranchingBB);
  }
  if (isPotentiallyReachable(BB2, BranchingBB, &PDomSet) &&
      !isPotentiallyReachable(BB1, BranchingBB, &PDomSet) &&
      !isPotentiallyReachable(BB2, BB1, &BranchBBSet, &DT)) {
    return getAllReachableBBsExcluding(BB2, PDom).contains(BranchingBB);
  }
  return false;
}

////////////////////////////////////////////////////////////////////////////////
///                            Helper Functions                              ///
////////////////////////////////////////////////////////////////////////////////

namespace llvm {
void removeIncomingBlockFromPhis(const BasicBlock *RefToRemove,
                                 BasicBlock *BBToProcess) {
  auto Phis = BBToProcess->phis();
  // No Phis
  if (Phis.empty())
    return;
  bool ContainsRef =
      is_contained(cast<PHINode>(*Phis.begin()).blocks(), RefToRemove);
  if (!ContainsRef)
    return;
  for (PHINode &Phi : make_early_inc_range(BBToProcess->phis())) {
    Phi.removeIncomingValue(RefToRemove);
  }
}
} // namespace llvm

////////////////////////////////////////////////////////////////////////////////
///                             TensorShapeAny                               ///
////////////////////////////////////////////////////////////////////////////////

template <typename SizeTy>
bool TensorShapeAny<SizeTy>::operator==(
    const TensorShapeAny<SizeTy> &other) const {
  if (isScalar() && other.isScalar())
    return true;
  for (unsigned idx = 0, end = std::max(rank(), other.rank()); idx < end;
       ++idx) {
    if ((*this)[idx] != other[idx])
      return false;
  }
  return true;
}

template <typename SizeTy>
bool TensorShapeAny<SizeTy>::operator<(
    const TensorShapeAny<SizeTy> &other) const {
  unsigned MaximumRank = std::max(rank(), other.rank());
  std::less<SizeTy> lessThan;
  for (unsigned Index = MaximumRank - 1; Index < MaximumRank; --Index) {
    if (lessThan((*this)[Index], other[Index]))
      return true;
    else if (lessThan(other[Index], (*this)[Index]))
      return false;
  }
  return false;
}

template <typename SizeTy>
bool TensorShapeAny<SizeTy>::operator>(
    const TensorShapeAny<SizeTy> &other) const {
  return other < *this;
}

template <typename SizeTy>
bool TensorShapeAny<SizeTy>::operator>=(
    const TensorShapeAny<SizeTy> &other) const {
  return other < *this || *this == other;
}

template <typename SizeTy>
bool TensorShapeAny<SizeTy>::operator<=(
    const TensorShapeAny<SizeTy> &other) const {
  return *this < other || *this == other;
}

template <typename SizeTy>
Error TensorShapeAny<SizeTy>::checkDims(
    const TensorShapeAny<DimSize> &other,
    std::function<Error(unsigned idx, DimSize, DimSize)> f) const {
  for (unsigned i = 0, e = std::min(rank(), other.rank()); i < e; ++i) {
    if (Error e = f(i, (*this)[i], other[i])) {
      return e;
    }
  }
  return Error::success();
}

template <typename SizeTy>
Error TensorShapeAny<SizeTy>::combineShapeBcast(
    const TensorShapeAny<DimSize> &other) {
  if (Error e = canCombineWith(other))
    return e;
  for (unsigned i = 0, e = std::min(rank(), other.rank()); i < e; ++i) {
    shape[i] = std::max(shape[i], other[i]);
  }
  // Copy what's coming from the other shape
  for (unsigned i = rank(); i < other.rank(); ++i)
    shape.push_back(other[i]);
  return Error::success();
}

template <typename SizeTy>
Error TensorShapeAny<SizeTy>::isBroadcastError(
    const TensorShapeAny<DimSize> &other) const {
  auto check = [&](unsigned Idx, DimSize thisSize, DimSize otherSize) -> Error {
    if ((thisSize > 1 && otherSize > 1 && thisSize != otherSize) ||
        (thisSize > 1 && otherSize <= 1)) {
      // The shapes are not compatible and cannot be broadcasted
      std::string errorStr;
      raw_string_ostream os(errorStr);
      os << "dimension index " << Idx << " cannot be broadcasted from "
         << thisSize << " to " << otherSize;
      os.flush();
      return createStringError(inconvertibleErrorCode(), errorStr);
    }
    return Error::success();
  };
  return checkDims(other, check);
}

template <typename SizeTy>
Error TensorShapeAny<SizeTy>::canCombineWith(
    const TensorShapeAny<DimSize> &other) const {
  auto check = [&](unsigned Idx, DimSize thisSize, DimSize otherSize) -> Error {
    if (thisSize > 1 && otherSize > 1 && thisSize != otherSize) {
      // The shapes are not compatible and cannot be broadcasted
      std::string errorStr;
      raw_string_ostream os(errorStr);
      os << "cannot apply the broadcast rule between dimension size "
         << thisSize << " and " << otherSize << " at dimension index " << Idx;
      os.flush();
      return createStringError(inconvertibleErrorCode(), errorStr);
    }
    return Error::success();
  };
  return checkDims(other, check);
}

template <typename SizeTy>
bool TensorShapeAny<SizeTy>::reduceDimensions(const BitVector &bv) {
  bool Changed = false;
  for (auto idx : bv.set_bits()) {
    if (idx < shape.size()) {
      Changed = Changed || shape[idx] != DimSize(1);
      shape[idx] = DimSize(1);
    }
  }
  return Changed;
}

template <typename SizeTy>
bool TensorShapeAny<SizeTy>::keepDimensions(const BitVector &BV) {
  auto BVFlipped = BV;
  BVFlipped.flip();
  return reduceDimensions(BVFlipped);
}

template <typename SizeTy>
bool TensorShapeAny<SizeTy>::reducedToScalarBy(
    const BitVector &reduction) const {
  TensorShapeAny<SizeTy> t = *this;
  t.reduceDimensions(reduction);
  return t.isScalar();
}

template <typename SizeTy>
size_t TensorShapeAny<SizeTy>::getOffsetAt(ArrayRef<size_t> coordinate) const {
  size_t currentStride{1};
  size_t offset{0};
  for (size_t peIdx = 0; peIdx < coordinate.size(); ++peIdx) {
    DimSize peSize = (*this)[peIdx];
    if (peSize > DimSize(1)) {
      assert(coordinate[peIdx] < peSize && "Out of bound access");
      offset += coordinate[peIdx] * currentStride;
    }
    currentStride *= peSize;
  }
  return offset;
}

template <typename SizeTy>
void TensorShapeAny<SizeTy>::foreachIndex(
    std::function<void(const ArrayRef<size_t>)> f) const {
  if (isScalar())
    return;
  std::vector<size_t> indices(rank(), 0);
  for (DimSize flatIdx = 0; flatIdx < flatShape(); ++flatIdx) {
    f(indices);
    indices[0] += 1;
    for (unsigned dim = 0; dim < rank() - 1; ++dim) {
      if (indices[dim] == shape[dim]) {
        indices[dim] = 0;
        indices[dim + 1] += 1;
      }
    }
  }
}

template <typename SizeTy>
BitVector TensorShapeAny<SizeTy>::nonEmptyDims() const {
  unsigned rk = rank();
  BitVector NonEmpty(rk);
  for (unsigned i = 0; i < rk; ++i) {
    if ((*this)[i] > 1)
      NonEmpty.set(i);
  }
  return NonEmpty;
}

template <typename SizeTy>
BitVector TensorShapeAny<SizeTy>::testBothDims(
    const TensorShapeAny<SizeTy> &other,
    const std::function<bool(SizeTy, SizeTy)> &test) const {
  unsigned MaxRank = std::max(rank(), other.rank());
  BitVector testedDims(MaxRank);
  for (unsigned i = 0; i < MaxRank; i++) {
    if (test((*this)[i], other[i]))
      testedDims.set(i);
  }
  return testedDims;
}

template <typename SizeTy>
BitVector TensorShapeAny<SizeTy>::bothNonEmptyDims(
    const TensorShapeAny<SizeTy> &other) const {
  auto bothNonEmpty = [](SizeTy a, SizeTy b) { return a > 1 && b > 1; };
  return testBothDims(other, bothNonEmpty);
}

template <typename SizeTy>
BitVector TensorShapeAny<SizeTy>::reductionDimensionsBeforeBroadcast(
    const TensorShapeAny<SizeTy> &other) const {
  auto toReduce = [](SizeTy me, SizeTy them) { return me > 1 && them < 2; };
  return testBothDims(other, toReduce);
}

template <typename SizeTy>
BitVector TensorShapeAny<SizeTy>::requiredSplat(
    const TensorShapeAny<SizeTy> &other) const {
  auto needsSplat = [](SizeTy source, SizeTy destination) {
    return source == 1 && destination > 1;
  };
  return testBothDims(other, needsSplat);
}

template <typename SizeTy>
void TensorShapeAny<SizeTy>::print(raw_ostream &O) const {
  if (isScalar()) {
    O << "Scalar";
    return;
  }

  O << "Tensor";
  // Print a canonical tensor shape (all the trailing 1s are removed)
  size_t OmittedOnes = 0;
  for (auto &len : make_range(shape.begin(), shape.end())) {
    if (len > 1) {
      for (size_t I = 0; I < OmittedOnes; ++I) {
        O << "[1]";
      }
      OmittedOnes = 0;
      O << "[" << len << "]";
    } else
      OmittedOnes++;
  }
}

template <typename SizeTy>
MDNode *TensorShapeAny<SizeTy>::toConstMetadata(IntegerType *Ty) const {
  SmallVector<Metadata *, BaseTensorSize> DimsArray;
  for (auto Dim : *this) {
    auto *DimAsMetadata = ConstantAsMetadata::get(
        ConstantInt::get(Ty, APInt(Ty->getBitWidth(), Dim, false)));
    DimsArray.push_back(DimAsMetadata);
  }
  return MDNode::get(Ty->getContext(), DimsArray);
}

template <typename SizeTy>
std::unique_ptr<TensorShapeAny<SizeTy>>
TensorShapeAny<SizeTy>::fromConstMetadata(unsigned Rank, const MDNode *Node) {
  Shape S;
  // Don't fail too early, when Idx >= rank and all the upper dimensions are 1
  // (empty), we can construct the Tensor for this rank too
  for (unsigned Idx = 0, E = Node->getNumOperands(); Idx < E; Idx++) {
    if (auto *ConstantMeta =
            dyn_cast<ConstantAsMetadata>(Node->getOperand(Idx).get())) {
      if (auto *CI = dyn_cast<ConstantInt>(ConstantMeta->getValue())) {
        uint64_t Val = CI->getZExtValue();
        if (Val > 1 && Idx >= Rank)
          return nullptr;
        S.push_back(Val);
      }
    }
  }
  S.resize(Rank, DimSize(1));
  return std::make_unique<TensorShapeAny<SizeTy>>(std::move(S));
}

template <typename SizeTy>
Expected<TensorShapeAny<SizeTy>> TensorShapeAny<SizeTy>::broadcastShapeFromAll(
    ArrayRef<const TensorShapeAny<SizeTy> *> AllToBcast) {
  auto ptrToRef = [](const TensorShapeAny<SizeTy> *ShapePtr)
      -> const TensorShapeAny<SizeTy> & { return *ShapePtr; };
  return broadcastShapeFromAll(
      make_range(map_iterator(AllToBcast.begin(), ptrToRef),
                 map_iterator(AllToBcast.end(), ptrToRef)));
}

template <typename SizeTy>
template <typename IteratorT>
Expected<TensorShapeAny<SizeTy>> TensorShapeAny<SizeTy>::broadcastShapeFromAll(
    llvm::iterator_range<IteratorT> AllToBcast) {
  if (AllToBcast.begin() == AllToBcast.end())
    return createStringError(inconvertibleErrorCode(),
                             "Cannot broadcast without shapes");
  TensorShapeAny<SizeTy> BcastShape = *AllToBcast.begin();
  Error Err = Error::success();
  // Mark Err as checked
  for_each(make_range(std::next(AllToBcast.begin()), AllToBcast.end()),
           [&BcastShape, &Err](auto &TShape) {
             if (!Err)
               if (Error E = BcastShape.combineShapeBcast(TShape)) {
                 std::swap(E, Err);
                 consumeError(std::move(E));
               }
           });
  if (!Err)
    return BcastShape;
  else
    return Err;
}

////////////////////////////////////////////////////////////////////////////////
///                           NDLoadStoreAttr                                ///
////////////////////////////////////////////////////////////////////////////////

void NDLoadStoreAttr::print(raw_ostream &O) const {
  // NIY
}

NDLoadStoreAttr NDLoadStoreAttr::fromString(StringRef AttrName) {
  // NIY
  return NDLoadStoreAttr();
}

////////////////////////////////////////////////////////////////////////////////
///                         NDLoadStoreFactory                               ///
////////////////////////////////////////////////////////////////////////////////

bool NDLoadStoreFactory::analyzeCoalescing(Type *ElementTy,
                                           LinearSeries &AddressSeries,
                                           BitVector &StrideDims,
                                           Instruction *I) {

  assert(PointerType::isValidElementType(ElementTy) &&
         "Internal type inference error");
  DataLayout const &DL = Mod.getDataLayout();

  uint64_t ElemByteSize = DL.getTypeAllocSize(ElementTy);

  OptimizationRemarkEmitter &ORE = MyRipple.getORE();

  // Gather address tensor slice sizes, so we can compare them with slope sizes
  // 0-dimensional slice is one element, then one rod, plane, etc.
  const TensorShape &SlopeShape = AddressSeries.getSlopeShape();
  SmallVector<uint64_t, 3> SliceSize(SlopeShape.rank());
  uint64_t CurSliceSize = ElemByteSize;
  for (int i = 0, n = SlopeShape.rank(); i < n; ++i) {
    SliceSize[i] = CurSliceSize;
    CurSliceSize *= AddressSeries.getShape(i);
  }
  LLVM_DEBUG(dbgs() << "slice sizes for " << AddressSeries << ": ";
             for_each(SliceSize, [](auto size) { dbgs() << size << ' '; });
             dbgs() << '\n');

  const TensorShape &BaseShape = AddressSeries.getBaseShape();
  bool Contiguous = true;
  // We're contiguous up to dimension i of the Address series
  // if the address tensor is made of slopes up to i and these slopes
  // correspond to a slice of the address tensor.
  int k = 0;
  for (int i = 0, n = SlopeShape.rank(); i < n; ++i) {
    // There aren't any strides as long as the slopes match the slice of
    // the load's tensor shape.
    // If the stride comes from a collection of bases,
    // we assume that this match isn't happening.
    if (BaseShape[i] > 1) {
      std::string BaseShapeStr;
      {
        raw_string_ostream RSOS(BaseShapeStr);
        RSOS << BaseShape[i];
      }
      ORE.emit([&] {
        OptimizationRemarkMissed R(DEBUG_TYPE, "AnalyzeCoalescing", I);
        if (dyn_cast<LoadInst>(I)) {
          R << "tensor access requires gather: ";
          R << "base is a tensor with shape " << BaseShapeStr << "; ";
          R << "cannot form contiguous loads"
            << "\n";
        } else {
          R << "tensor access requires scatter: ";
          R << "base is a tensor with shape " << BaseShapeStr << "; ";
          R << "cannot form contiguous stores"
            << "\n";
        }
        return R;
      });
      Contiguous = false;
      continue;
    }
    if (SlopeShape[i] == 1)
      // stride is unchanged.
      // keep trying to match the same slice shape with the next stride
      continue;
    if (Constant *ConstSlope = dyn_cast<Constant>(AddressSeries.getSlope(i))) {
      // Skip trivial and broadcast values
      assert(!ConstSlope->isZeroValue() &&
             "Unexpected splat/broadcast in linear series");
      const APInt &ApSlope = ConstSlope->getUniqueInteger();
      const APInt ApSliceSize(ApSlope.getBitWidth(), SliceSize[k],
                              /*signed=*/false);
      LLVM_DEBUG(dbgs() << "slope " << ApSlope << " compares to slice size "
                        << ApSliceSize << '\n');
      // Slopes become load/store strides from the moment that contiguity
      // is broken
      if (!Contiguous || ApSlope != ApSliceSize) {
        Contiguous = false;
        ORE.emit([&] {
          OptimizationRemarkMissed R(DEBUG_TYPE, "AnalyzeCoalescing", I);
          if (dyn_cast<LoadInst>(I)) {
            R << "tensor access requires gather: ";
            R << "strided memory access along dimension " << std::to_string(i);
            R << " prevents coalesced vector load"
              << "\n";
          } else {
            R << "tensor access requires scatter: ";
            R << "strided memory access along dimension " << std::to_string(i);
            R << " prevents coalesced vector store"
              << "\n";
          }
          return R;
        });
        StrideDims.set(i);
      }
    } else { // non-constant slope
      ORE.emit([&] {
        // Instruction *I = dyn_cast<Instruction>(AddressSeries.getSlope(i));
        OptimizationRemarkMissed R(DEBUG_TYPE, "AnalyzeCoalescing", I);

        if (dyn_cast<LoadInst>(I)) {
          R << "tensor access requires gather: ";
          R << "non-constant stride along dimension " << std::to_string(i);
          R << " prevents coalesced vector load"
            << "\n";
        } else {
          R << "tensor access requires scatter: ";
          R << "non-constant stride along dimension " << std::to_string(i);
          R << " prevents coalesced vector store"
            << "\n";
        }
        return R;
      });
      Contiguous = false;
      StrideDims.set(i);
    }
    k++;
  }
  return Contiguous;
}

std::string NDLoadStoreFactory::ndFunctionName(StringRef LoadOrStore, int nDims,
                                               NDLoadStoreAttr &Attr) {
  std::string FunName;
  raw_string_ostream OS(FunName);
  OS << LoadOrStore << "." << nDims << "d" << LoadOrStore;
  if (!Attr.empty()) {
    OS << "." << Attr;
  }
  OS.flush();
  return FunName;
}

Value *NDLoadStoreFactory::genUnstructuredLoad(LoadInst *Load, Value *Address,
                                               const TensorShape &ToShape) {
  OptimizationRemarkEmitter &ORE = MyRipple.getORE();

  IrBuilder.SetInsertPoint(Load);
  if (ToShape.isScalar()) {
    // Base case, load
    Value *LoadVal =
        IrBuilder.CreateLoad(Load->getType(), Address, Load->getName());
    MyRipple.setRippleShape(LoadVal, ToShape);
    return LoadVal;
  } else if (AllocaInst *Alloca = dyn_cast<AllocaInst>(Address)) {
    assert(MyRipple.getRippleShape(static_cast<Value *>(Alloca)).flatShape() >=
           ToShape.flatShape());
    // Allocas that have been promoted don't require a gather
    Type *LoadVectorTy =
        VectorType::get(Alloca->getAllocatedType()->getArrayElementType(),
                        ToShape.flatShape(), /*Scalable*/ false);
    Value *LoadVal = IrBuilder.CreateLoad(LoadVectorTy, Alloca,
                                          Twine(Load->getName()) + ".ripple");
    MyRipple.setRippleShape(LoadVal, ToShape);
    return LoadVal;
  }

  if (ORE.enabled()) {
    ORE.emit([&] {
      OptimizationRemark R(DEBUG_TYPE, "UnstructuredLoadGen", Load);
      R << "generating gather: ";
      R << "gather instructions are typically expensive and may degrade "
           "performance. ";
      R << "improved data layout, access patterns, and loop structures ";
      R << "can help reduce this cost."
        << "\n";
      return R;
    });
  }

  Type *ElementTy = Load->getType();
  int NElements = ToShape.flatShape();
  assert(NElements > 1);
  Type *VecTy = VectorType::get(ElementTy, NElements, /*scalable=*/false);
  Value *GatheredVal =
      IrBuilder.CreateMaskedGather(VecTy, Address,
                                   // Aligned on element boundaries
                                   Load->getAlign());
  MyRipple.setRippleShape(GatheredVal, ToShape);
  return GatheredVal;
}

Value *NDLoadStoreFactory::genLoadNoSplat(LoadInst *Load,
                                          LinearSeries &AddressSeries,
                                          Value *DefaultAddress,
                                          const TensorShape &ToShape) {
  // TODO: check that AddressSeries is splat-free
  // TODO: With opaque types, I'm not sure this will always work. Check.
  Type *ElementTy = Load->getType();
  BitVector StrideDims(AddressSeries.rank());
  bool Contiguous =
      analyzeCoalescing(ElementTy, AddressSeries, StrideDims, Load);
  int NElements = ToShape.flatShape();
  Type *LoadTy = ToShape.isScalar() ? ElementTy
                                    : VectorType::get(ElementTy, NElements,
                                                      /*scalable=*/false);
  IrBuilder.SetInsertPoint(Load);
  assert(ToShape.isVector() || ToShape.isScalar() && Contiguous);
  if (Contiguous) {
    LoadInst *VectorLoad =
        IrBuilder.CreateLoad(LoadTy, AddressSeries.getBase());
    MyRipple.setRippleShape(static_cast<Value *>(VectorLoad), ToShape);
    VectorLoad->takeName(Load);
    // TODO: Take ripple_aligned() into account.
    VectorLoad->setAlignment(Load->getAlign());
    return VectorLoad;
  } else {
    return genUnstructuredLoad(Load, DefaultAddress, ToShape);
  }
}

Value *NDLoadStoreFactory::genLoad(LoadInst *Load, LinearSeries &AddressSeries,
                                   Value *DefaultAddress,
                                   const TensorShape &ToShape) {
  // Deal with broadcasts: load(a + 0*k along d) = broadcast(load(a), d)
  // So we first build the broadcast-free load, and broadcast it.
  // The reason why we expose broadcasts at this moment is that
  // it can simplify the application of masks during if-conversion.
  // A smaller-dimensional mask can apply before broadcast,
  // as opposed to having to broadcast the mask.
  BitVector SeriesSplat = AddressSeries.getSplatDims();
  // There may also be an implicit broadcast
  const TensorShape &AddressShape = AddressSeries.getShape();
  BitVector SplatDims = AddressShape.requiredSplat(ToShape);
  SplatDims |= SeriesSplat;
  Value *VectorLoad;
  if (!SplatDims.empty()) {
    // LLVM_DEBUG(dbgs() << "Splat dims: " << SplatDims << '\n');
    // Create a load without the splatting/broadcasting
    LinearSeries SplatFreeSeries = AddressSeries.removeSlopes(SeriesSplat);
    // Temporary local LS used for load optimization needs to be instantiated
    // w/o the cache
    IrBuilder.SetInsertPoint(Load);
    auto SplatFreeAddress =
        MyRipple.instantiateLinearSeriesNoCache(SplatFreeSeries);
    const TensorShape &LoadShape = SplatFreeSeries.getShape();
    LLVM_DEBUG(dbgs() << "Loaded series shape " << LoadShape << '\n');
    Value *CoreGather =
        genLoadNoSplat(Load, SplatFreeSeries, SplatFreeAddress, LoadShape);
    auto Splatted = MyRipple.tensorBcast(CoreGather, LoadShape, ToShape);
    // Checked during shape propagation
    if (!Splatted)
      report_fatal_error("Broadcast failure during codegen");
    VectorLoad = *Splatted;
  } else {
    VectorLoad = genLoadNoSplat(Load, AddressSeries, DefaultAddress, ToShape);
  }
  return VectorLoad;
}

Value *NDLoadStoreFactory::genUnstructuredStore(StoreInst *Store, Value *Val,
                                                Value *Address,
                                                const TensorShape &ToShape) {
  int NElements = ToShape.flatShape();
  // No implicit broadcasts are expected here
  assertNumElements(Val, NElements);
  assertNumElements(Address, NElements);

  MyRipple.analyzeTensorShape(ToShape, Store, Val, Address);
  OptimizationRemarkEmitter &ORE = MyRipple.getORE();

  if (ORE.enabled()) {
    ORE.emit([&] {
      OptimizationRemark R(DEBUG_TYPE, "UnstructuredStoreGen", Store);
      R << "generating scatter: ";
      R << "scatter instructions are typically expensive and may degrade "
           "performance. ";
      R << "improved data layout, access patterns, and loop structures ";
      R << "can help reduce this cost."
        << "\n";
      return R;
    });
  }

  IrBuilder.SetInsertPoint(Store);
  Value *StoredVal =
      IrBuilder.CreateMaskedScatter(Val, Address, Store->getAlign());
  MyRipple.setRippleShape(StoredVal, ToShape);
  return StoredVal;
}

Value *NDLoadStoreFactory::genStore(StoreInst *Store, Value *Val,
                                    LinearSeries &AddressSeries,
                                    Value *DefaultAddress,
                                    const TensorShape &ToShape) {
  Type *ElementTy = Store->getValueOperand()->getType();
  BitVector StrideDims(AddressSeries.rank());
  bool Contiguous =
      analyzeCoalescing(ElementTy, AddressSeries, StrideDims, Store);
  IrBuilder.SetInsertPoint(Store);
  Value *VectorStore;
  if (Contiguous) {
    StoreInst *VecStore = IrBuilder.CreateStore(Val, AddressSeries.getBase());
    MyRipple.setRippleShape(static_cast<Value *>(VecStore), ToShape);
    VecStore->setAlignment(Store->getAlign());
    VectorStore = VecStore;
  } else {
    return genUnstructuredStore(Store, Val, DefaultAddress, ToShape);
  }
  VectorStore->takeName(Store);
  MyRipple.setRippleShape(VectorStore, ToShape);
  return VectorStore;
}

////////////////////////////////////////////////////////////////////////////////
///                               Ripple                                     ///
////////////////////////////////////////////////////////////////////////////////

// This is a copy paste of updated UnifyFunctionExitNodes pass methods.
// It needs to be properly upstreamed, but for now we keep a copy
// to minimize upstream conflict.

static bool
unifyUnreachableBlocks(Function &F,
                       SmallVectorImpl<DominatorTree::UpdateType> *DTU) {
  std::vector<BasicBlock *> UnreachableBlocks;

  for (BasicBlock &I : F)
    if (isa<UnreachableInst>(I.getTerminator()))
      UnreachableBlocks.push_back(&I);

  if (UnreachableBlocks.size() <= 1)
    return false;

  BasicBlock *UnreachableBlock =
      BasicBlock::Create(F.getContext(), "UnifiedUnreachableBlock", &F);
  new UnreachableInst(F.getContext(), UnreachableBlock);

  for (BasicBlock *BB : UnreachableBlocks) {
    BB->back().eraseFromParent(); // Remove the unreachable inst.
    BranchInst::Create(UnreachableBlock, BB);
    if (DTU)
      DTU->push_back({DominatorTree::Insert, BB, UnreachableBlock});
  }

  return true;
}

static bool unifyReturnBlocks(Function &F,
                              SmallVectorImpl<DominatorTree::UpdateType> *DTU) {
  std::vector<BasicBlock *> ReturningBlocks;

  for (BasicBlock &I : F)
    if (isa<ReturnInst>(I.getTerminator()))
      ReturningBlocks.push_back(&I);

  if (ReturningBlocks.size() <= 1)
    return false;

  // Insert a new basic block into the function, add PHI nodes (if the function
  // returns values), and convert all of the return instructions into
  // unconditional branches.
  BasicBlock *NewRetBlock =
      BasicBlock::Create(F.getContext(), "UnifiedReturnBlock", &F);

  PHINode *PN = nullptr;
  if (F.getReturnType()->isVoidTy()) {
    ReturnInst::Create(F.getContext(), nullptr, NewRetBlock);
  } else {
    // If the function doesn't return void... add a PHI node to the block...
    PN = PHINode::Create(F.getReturnType(), ReturningBlocks.size(),
                         "UnifiedRetVal");
    PN->insertInto(NewRetBlock, NewRetBlock->end());
    ReturnInst::Create(F.getContext(), PN, NewRetBlock);
  }

  // Loop over all of the blocks, replacing the return instruction with an
  // unconditional branch.
  for (BasicBlock *BB : ReturningBlocks) {
    // Add an incoming element to the PHI node for every return instruction that
    // is merging into this new block...
    if (PN)
      PN->addIncoming(BB->getTerminator()->getOperand(0), BB);

    BB->back().eraseFromParent(); // Remove the return insn
    BranchInst::Create(NewRetBlock, BB);
    if (DTU)
      DTU->push_back({DominatorTree::Insert, BB, NewRetBlock});
  }
  return true;
}

void Ripple::setReplacementFor(Instruction *I, Value *V,
                               const TensorShape &Shape) {
  if (InstructionReplacementMapping.contains(I)) {
    LLVM_DEBUG(dbgs() << "Re-defining the mapping between " << *I << " and "
                      << *InstructionReplacementMapping[I] << " to " << *V
                      << "\n");
  }
  InstructionReplacementMapping[I] = V;
  setRippleShape(V, Shape);
}

std::pair<Value *, const TensorShape *>
Ripple::replacementValueAndShape(Value *V) const {
  if (Instruction *I = dyn_cast<Instruction>(V)) {
    ConstructedSeries CS = getCachedSeries(I);
    if (Value *Replacement = InstructionReplacementMapping.lookup(I)) {
      LLVM_DEBUG(dbgs() << "Replacement tensor value for " << *V
                        << " found: " << *Replacement << "\n");
      // During vectorization (genVectorInstruction), linear series with a
      // tensor base get a "replacement" base.
      // This method always returns the "replacement" of the base, irrespective
      // of the context. It's crucial to use the method getTensorUse to obtain
      // the expected tensor for all kinds of users (LS or not). This is because
      // an LS instantiation doesn't "replace" a value, it "creates" a tensor
      // value using an existing base and slopes, which getTensorUse handles!
      if (CS)
        return {Replacement, &CS.LS->getBaseShape()};
      else
        return {Replacement, &getRippleShape(V)};
    } else if (CS) {
      assert(CS.LS->getBaseShape() == ScalarShape &&
             "Tensor LinearSeries base has not been generated");
      return {V, &ScalarShape};
    }
  } else if (Argument *Arg = dyn_cast<Argument>(V)) {
    // When we are specializing this function, the original arguments are
    // duplicated, with the ripple arguments following the original ones
    if (isPendingRippleSpecialization(F)) {
      Argument *ReplacementArg = F.getArg(Arg->getArgNo() + F.arg_size() / 2);
      return {ReplacementArg, &getRippleShape(ReplacementArg)};
    }
  }
  LLVM_DEBUG(dbgs() << "No replacement tensor value for " << *V << "\n");
  return {V, &getRippleShape(V)};
}

std::pair<Value *, const TensorShape *> Ripple::getTensorUse(const Use &U) {
  // Ripple generates tensors for Instructions only
  Instruction *User = cast<Instruction>(U.getUser());
  Value *UseVal = U.get();
  // If the Use is a LS, we may have to instantiate it
  if (Instruction *UseI = dyn_cast<Instruction>(UseVal)) {
    if (auto UseCS = getCachedSeries(UseI)) {
      auto UserCS = getCachedSeries(User);
      // When both UserCS and UseCS are valid linear series we have two cases:
      // 1) if UserCS and UseCS have the same base shape, we return the
      //    (tensorized) base. This is because the only case, other than
      //    reduction and slicing, where we are asking for such getTensorUse is
      //    when we are generating the "Replacement" of UserCS's base. Hence we
      //    skip instantiation and fall back to getting the replacement below.
      // 2) In other cases we want to generate the instance of UseCS for the
      //    user and thus enter this condition's block
      if (!UserCS || UseCS.LS->getBaseShape() != UserCS.LS->getBaseShape() ||
          rippleReduceIntrinsics(User) || rippleSliceIntrinsic(User)) {
        auto InstantiatedLS = instantiateCachedSeries(UseCS, UseI);
        return std::make_pair(InstantiatedLS, &UseCS.LS->getShape());
      }
      // else returns the replacement below
    }
  }
  auto RepAndShape = replacementValueAndShape(UseVal);
  auto &[RepValue, RepShape] = RepAndShape;
  // Handle Alloca vector expansion shape-shifting
  if (isa<AllocaInst>(RepValue) && RepShape->isVector()) {
    auto &UserShape = getRippleShape(User);
    // That's always true assuming we don't allow taking the address of scalar
    // Alloca (which is part of Ripple pre-requisite for now). Otherwise, to
    // allow disambiguation, the fix would be to set TensorShapes to the
    // Use of AllocaInst instead.
    assert(UserShape <= *RepShape);
    RepShape = &UserShape;
  }
  return RepAndShape;
}

const TensorShape &Ripple::getRippleShape(const Value *V,
                                          bool ShapePropagation) const {
  if (const Instruction *I = dyn_cast<Instruction>(V)) {
    return getRippleShape(I);
  } else if (const Constant *C = dyn_cast<Constant>(V)) {
    return getRippleShape(C, ShapePropagation);
  } else if (const Argument *A = dyn_cast<Argument>(V)) {
    return getRippleShape(A, ShapePropagation);
  } else if (isa<BasicBlock>(V) || isa<MetadataAsValue>(V)) {
    return ShapeIgnoredByRipple;
  } else {
    std::string ErrorStr;
    raw_string_ostream OS(ErrorStr);
    OS << "The following value does not have a Ripple shape: " << *V;
    OS.flush();
    report_fatal_error(ErrorStr.c_str());
  }
}

const TensorShape &Ripple::getRippleShape(const Constant *C,
                                          bool ShapePropagation) const {
  if (ShapePropagation && C->getType()->isVectorTy())
    return ShapeIgnoredByRipple;
  else if (!C->getType()->isVectorTy())
    return ScalarShape;
  report_fatal_error(
      "Taking the Ripple shape of a vector constant is ill-defined");
}

const TensorShape &Ripple::getRippleShape(const Argument *A,
                                          bool ShapePropagation) const {
  // When specializing, arguments can have tensor shapes!
  if (isPendingRippleSpecialization(F))
    return ArgumentShapes[A->getArgNo()];
  else if (ShapePropagation && A->getType()->isVectorTy())
    return ShapeIgnoredByRipple;
  else if (!A->getType()->isVectorTy())
    return ScalarShape;
  report_fatal_error(
      "Taking the Ripple shape of a vector function Argument is ill-defined");
}

const TensorShape &Ripple::getRippleShape(const Instruction *I) const {
  auto Shape = InstructionRippleShapes.find(I);
  if (Shape != InstructionRippleShapes.end()) {
    return Shape->second;
  } else {
    std::string ErrorStr;
    raw_string_ostream OS(ErrorStr);
    OS << "The following instruction does not have a Ripple shape: " << *I;
    OS.flush();
    report_fatal_error(ErrorStr.c_str());
  }
}

Error Ripple::replaceRippleGetSize() {
  Error AllErrors = Error::success();
  for (auto &I : make_early_inc_range(instructions(F))) {
    if (IntrinsicInst *BlockGetSize =
            intrinsicWithId(&I, {Intrinsic::ripple_block_getsize})) {
      if (Error E = checkRippleBlockIntrinsics(BlockGetSize)) {
        AllErrors = joinErrors(std::move(AllErrors), std::move(E));
        continue;
      }
      auto DimSize = getRippleGetSizeValue(BlockGetSize);
      Constant *C = ConstantInt::get(BlockGetSize->getType(), DimSize);
      auto BlockIterator = BlockGetSize->getIterator();
      invalidateRippleDataFor(BlockGetSize);
      ReplaceInstWithValue(BlockIterator, C);
    }
  }
  return AllErrors;
}

Error Ripple::propagateShapes(bool &WaitingForSpecialization) {
  LLVM_DEBUG(dbgs() << "Propagating shapes in function:\n"; F.print(dbgs()));

  if (Error E = checkBlockShapeUsage(F))
    return E;

  // Arguments have ripple shapes during specialization
  if (isPendingRippleSpecialization(F)) {
    bool DefaultToScalar = true;
    if (hasRippleShapeMetadata(F)) {
      SmallVector<std::unique_ptr<TensorShape>, 8> ArgShapes;
      std::unique_ptr<TensorShape> ReturnShape;
      if (getFunctionShapeMetadata(F, ArgShapes, ReturnShape)) {
        DefaultToScalar = false;
        assert(ArgShapes.size() == F.arg_size());
        llvm::transform(ArgShapes, std::back_inserter(ArgumentShapes),
                        [](auto &UniqPtr) { return std::move(*UniqPtr); });
      }
    }
    if (DefaultToScalar)
      ArgumentShapes.append(F.arg_size(), ScalarShape);
  }

  std::queue<Instruction *> WorkQueue;
  WaitingForSpecialization = false;

  std::function<void(Instruction *)> revisitUserInstructions =
      [&](Instruction *I) {
        for (auto *User : I->users())
          if (Instruction *UserInst = dyn_cast<Instruction>(User))
            if (domTree.isReachableFromEntry(UserInst->getParent()))
              WorkQueue.push(UserInst);

        if (auto MemLoc = MemoryLocation::getOrNone(I))
          if (AllocaInst *Alloca = aliasesWithPromotableAlloca(*MemLoc)) {
            if (auto *Store = dyn_cast<StoreInst>(I)) {
              visitAllInstructionsBeingClobberedBy(
                  cast<MemoryDef>(MemSSA.getMemoryAccess(Store)),
                  [&](Instruction *Clobbered) -> bool {
                    WorkQueue.push(Clobbered);
                    return true;
                  });
            } else if (auto *Load = dyn_cast<LoadInst>(I)) {
              visitAllClobberingInstructions(
                  cast<MemoryUse>(MemSSA.getMemoryAccess(Load)),
                  [&](Instruction *Clobber) -> bool {
                    WorkQueue.push(Clobber);
                    return true;
                  });
            } else {
              llvm_unreachable("Ripple doesn't support promotion of VAARG or "
                               "Atomic instructions");
            }

            // Alloca gets the largest shape of all its users
            auto &AllocaShape = getRippleShape(Alloca);
            auto &InstructionShape = getRippleShape(I);
            if (AllocaShape.flatShape() < InstructionShape.flatShape())
              setRippleShape(Alloca, InstructionShape);
          }
      };

  // Start processing the function in reverse post order
  for (auto *BB : getFuncRPOT())
    for (auto &I : *BB) {
      WorkQueue.push(&I);
      if (AllocaInst *Alloca = dyn_cast<AllocaInst>(&I)) {
        Type *AllocatedType = Alloca->getAllocatedType();
        if (AllocatedType->isFloatingPointTy() || AllocatedType->isIntOrPtrTy())
          PromotableAlloca.insert(Alloca);
        else
          NonPromotableAlloca.insert(Alloca);
      }
    }

  while (!WorkQueue.empty()) {
    Instruction *I = WorkQueue.front();
    WorkQueue.pop();

    LLVM_DEBUG(dbgs() << "Shape inference of: " << *I << "\n");
    auto NewShape = inferShapeFromOperands(I, /*AllowPartialPhi*/ true,
                                           WaitingForSpecialization);
    if (!NewShape) {
      DiagnosticInfoRippleWithLoc Diag(
          DS_Error, F, sanitizeRippleLocation(I),
          "Ripple cannot infer the shape of this instruction");
      F.getContext().diagnose(Diag);
      return NewShape.takeError();
    }
    if (WaitingForSpecialization)
      return Error::success();

    LLVM_DEBUG(dbgs() << "Setting shape: " << *NewShape << "\n");
    // Process the users if we updates the shape of the instruction
    if (setRippleShape(I, *NewShape)) {
      // Revisit users of an instruction that changed shape
      revisitUserInstructions(I);

      if (isa<BranchInst>(I) || isa<SwitchInst>(I)) {
        // Mark masked calls
        if (NewShape->isVector()) {
          BasicBlock *VectorBlock = I->getParent();
          BasicBlock *PostDomBlock =
              postdomTree.getNode(VectorBlock)->getIDom()->getBlock();
          for (BasicBlock *BB : allBasicBlocksFromTo(VectorBlock, PostDomBlock))
            for (Instruction &I : *BB)
              if (CallInst *Call = dyn_cast<CallInst>(&I))
                MaskedCalls.insert(Call);
        }
      }
    }
    assert(getRippleShape(I) == *NewShape);
  }

  // All unreachable instructions get a scalar shape!
  for (auto &BB : F)
    if (!domTree.isReachableFromEntry(&BB))
      for (auto &I : BB)
        setRippleShape(&I, ScalarShape);

  assert(allInstructionsHaveRippleShapes() &&
         "Some instruction has no Ripple shape");

  if (isPendingRippleSpecialization(F)) {
    // All the return must have the same shape, broadcast to a common shape!
    TensorShape BcastReturnShape = ScalarShape;
    for (const auto &I : instructions(F)) {
      if (const auto *Return = dyn_cast<ReturnInst>(&I)) {
        auto &RetShape = getRippleShape(Return);
        auto NewShape = combineShapeBcastWithErrorReporting(
            BcastReturnShape, RetShape,
            "Ripple cannot combine this return instruction's tensor shape with "
            "other return instructions' shape using the broadcast rule",
            sanitizeRippleLocation(Return));
        RETURN_UNEXPECTED(NewShape);
        std::swap(BcastReturnShape, *NewShape);
      }
    }
    for (const auto &I : instructions(F))
      if (const auto *Return = dyn_cast<ReturnInst>(&I))
        setRippleShape(Return, BcastReturnShape);
  }

  // Now that all shapes have settled, create linear series
  for (auto *BB : getFuncRPOT()) {
    for (auto &I : *BB) {
      WorkQueue.push(&I);
    }
  }
  while (!WorkQueue.empty()) {
    Instruction *I = WorkQueue.front();
    WorkQueue.pop();
    LLVM_DEBUG(dbgs() << "Linear series processing of: " << *I << "\n");
    auto CSBefore = getCachedSeries(I);
    auto CSAfter = getLinearSeriesFor(I);
    LLVM_DEBUG(dbgs() << "Linear series is: " << CSAfter << "\n");

    if (CSBefore.getState() != CSAfter.getState())
      revisitUserInstructions(I);
  }

  // Remove non-valid series
  clearPotentialSeries();

  // Simplify and cleanup slopes
  simplifySlopes();

  // Replace ripple_get_size by their values before simplification
  if (Error E = replaceRippleGetSize())
    return E;

  // Iterate while simplification occurs
  while (simplifyFunction())
    ;

  assert(allInstructionsHaveRippleShapes() &&
         "Some instruction has no Ripple shape");
  return Error::success();
}

Ripple::PEIndex
Ripple::rippleToTensor(const RippleIntrinsicIndex &idAndIdx) const {
  auto firstFound =
      std::find_if(TensorDimIDMap.begin(), TensorDimIDMap.end(),
                   [&](auto &idx) { return idx == idAndIdx.first; });
  assert(firstFound != TensorDimIDMap.end());
  assert(PERank(idAndIdx.first) > idAndIdx.second);
  return std::distance(TensorDimIDMap.begin(), firstFound) + idAndIdx.second;
}

Ripple::RippleIntrinsicIndex Ripple::tensorToRipple(PEIndex tensorIdx) const {
  PEIdentifier id = TensorDimIDMap[tensorIdx];
  PEIndex firstTensorIndex = rippleToTensor(std::make_pair(id, 0));
  return std::make_pair(id, tensorIdx - firstTensorIndex);
}

Ripple::DimSize Ripple::PERank(PEIdentifier PEId) const {
  return PERanks.lookup_or(PEId, 0);
}

DenseMap<Ripple::PEIdentifier, Ripple::DimSize>
Ripple::gatherRippleFunctionUsage(const Function &F) {
  DenseMap<PEIdentifier, DimSize> PERanks;
  for (const auto &I : instructions(F)) {
    if (const IntrinsicInst *BSI =
            intrinsicWithId(&I, {Intrinsic::ripple_block_setshape})) {
      PEIdentifier ProcElem = *getConstantOperandValue(BSI, 0);
      DimSize Rank = 0;
      for (unsigned ArgIdx = 1, E = BSI->arg_size(); ArgIdx < E; ++ArgIdx) {
        DimSize DS = *getConstantOperandValue(BSI, ArgIdx);
        if (DS > 1)
          Rank = ArgIdx;
      }
      auto Res = PERanks.insert({ProcElem, Rank});
      // Key already present; take the maximum rank seen in this function
      if (!Res.second)
        Res.first->second = std::max(Res.first->second, Rank);
    }
  }
  return PERanks;
}

SmallVector<Ripple::PEIdentifier, 8> Ripple::buildPEIdMap() {
  SmallVector<Ripple::PEIdentifier, 8> IdMap;
  for (const auto &[DimIdentifier, DimRank] : PERanks) {
    LLVM_DEBUG(dbgs() << "PE id " << DimIdentifier << " has rank " << DimRank
                      << "\n");
    // Check that the ID is known by the compiler
    for (PEIndex rank = 0; rank < DimRank; ++rank) {
      IdMap.push_back(DimIdentifier);
    }
  }
  return IdMap;
}

IntrinsicInst *Ripple::rippleBlockIntrinsics(Instruction *I) {
  return intrinsicWithId(I, {Intrinsic::ripple_block_index,
                             Intrinsic::ripple_block_getsize,
                             Intrinsic::ripple_block_setshape});
}

IntrinsicInst *Ripple::rippleReduceIntrinsics(Instruction *I) {
  return intrinsicWithId(
      I, {Intrinsic::ripple_reduce_add, Intrinsic::ripple_reduce_mul,
          Intrinsic::ripple_reduce_and, Intrinsic::ripple_reduce_or,
          Intrinsic::ripple_reduce_xor, Intrinsic::ripple_reduce_smax,
          Intrinsic::ripple_reduce_smin, Intrinsic::ripple_reduce_umax,
          Intrinsic::ripple_reduce_umin, Intrinsic::ripple_reduce_fadd,
          Intrinsic::ripple_reduce_fmul, Intrinsic::ripple_reduce_fmin,
          Intrinsic::ripple_reduce_fmax, Intrinsic::ripple_reduce_fminimum,
          Intrinsic::ripple_reduce_fmaximum});
}

IntrinsicInst *Ripple::rippleShuffleIntrinsics(Instruction *I) {
  return intrinsicWithId(I, {Intrinsic::ripple_shuffle});
}

IntrinsicInst *Ripple::rippleBroadcastIntrinsic(Instruction *I) {
  return intrinsicWithId(I, {Intrinsic::ripple_broadcast});
}

IntrinsicInst *Ripple::rippleSliceIntrinsic(Instruction *I) {
  return intrinsicWithId(I, {Intrinsic::ripple_slice});
}

IntrinsicInst *Ripple::rippleIntrinsicsWithBlockShapeOperand(Instruction *I) {
  return intrinsicWithId(I, {Intrinsic::ripple_broadcast,
                             Intrinsic::ripple_block_getsize,
                             Intrinsic::ripple_block_index});
}

PreservedAnalyses Ripple::cleanupRippleSingleLane(ProcessingStatus Status) {
  for (auto &I : make_early_inc_range(instructions(F))) {
    if (IntrinsicInst *BlockInst = rippleBlockIntrinsics(&I)) {
      auto BlockIterator = BlockInst->getIterator();
      switch (BlockInst->getIntrinsicID()) {
      case Intrinsic::ripple_block_index: {
        invalidateRippleDataFor(&I);
        ReplaceInstWithValue(BlockIterator,
                             ConstantInt::get(BlockInst->getType(), 0));
      } break;
      case Intrinsic::ripple_block_getsize: {
        invalidateRippleDataFor(&I);
        ReplaceInstWithValue(BlockIterator,
                             ConstantInt::get(BlockInst->getType(), 1));
      } break;
      case Intrinsic::ripple_block_setshape: {
        invalidateRippleDataFor(&I);
        ReplaceInstWithValue(BlockIterator,
                             PoisonValue::get(BlockInst->getType()));
      } break;
      }
    } else if (IntrinsicInst *BroadcastInst = rippleBroadcastIntrinsic(&I)) {
      auto BroadcastIterator = BroadcastInst->getIterator();
      invalidateRippleDataFor(&I);
      ReplaceInstWithValue(BroadcastIterator, BroadcastInst->getOperand(2));
    } else if (IntrinsicInst *SliceInst = rippleSliceIntrinsic(&I)) {
      auto SliceIterator = SliceInst->getIterator();
      invalidateRippleDataFor(&I);
      ReplaceInstWithValue(SliceIterator, SliceInst->getOperand(0));
    } else if (IntrinsicInst *ReductionInst = rippleReduceIntrinsics(&I)) {
      auto ReductionIterator = ReductionInst->getIterator();
      invalidateRippleDataFor(&I);
      ReplaceInstWithValue(ReductionIterator, ReductionInst->getOperand(1));
    } else if (IntrinsicInst *ShuffleInst = rippleShuffleIntrinsics(&I)) {
      auto ShuffleIterator = ShuffleInst->getIterator();
      invalidateRippleDataFor(&I);
      ReplaceInstWithValue(ShuffleIterator, ShuffleInst->getOperand(0));
    } else {
      assert(!isRippleIntrinsics(&I));
    }
  }

  PS = Status;
  if (Status != ProcessingStatus::Success)
    return PreservedAnalyses::none();

  // All we did is replace Ripple intrinsics by constants and remove some
  // It may affect scalar evolution and loop analysis
  PreservedAnalyses PA;
  PA.preserve<AAManager>();
  PA.preserve<DominatorTreeAnalysis>();
  PA.preserve<PostDominatorTreeAnalysis>();
  PA.preserve<TargetLibraryAnalysis>();
  PA.preserve<GlobalsAA>();
  return PA;
}

void Ripple::initFuncRPOT() {
  // Change the control flow to have only one ReturnInst and UnreachableInst in
  // the Function
  SmallVector<DominatorTree::UpdateType, 8> DomTreeUpdates;
  unifyUnreachableBlocks(F, &DomTreeUpdates);
  unifyReturnBlocks(F, &DomTreeUpdates);
  if (!DomTreeUpdates.empty()) {
    DTU.applyUpdates(DomTreeUpdates);
    DTU.flush();
  }

  FuncRPOT = new ReversePostOrderTraversal<Function *>(&F);
}

PreservedAnalyses Ripple::run() {

  LLVM_DEBUG(dbgs() << "[Ripple] Processing function " << F.getName() << "\n");
  // There are no Ripple dimensions, nothing to be done!
  if (tensorRank() == 0)
    return cleanupRippleSingleLane(ProcessingStatus::Success);

  // Expensive, only do once!
  initFuncRPOT();

  auto abortDiagnosticAndRippleCleanup =
      [&](ProcessingStatus Status) -> PreservedAnalyses {
    DiagnosticInfoRippleWithLoc Diag(
        DS_Error, F, F.getSubprogram(),
        "Ripple failed to vectorize this function");
    F.getContext().diagnose(Diag);
    return cleanupRippleSingleLane(Status);
  };

  // Look for external Ripple declarations
  // This is required by propagateShapes and checkRippleSemantics
  loadRippleLibDeclarations();

  // Propagate vector shapes from instructions to users
  bool WaitingForSpecialization = false;
  if (Error e = propagateShapes(WaitingForSpecialization)) {
    llvm::handleAllErrors(std::move(e), [&](const StringError &SE) {
      LLVM_DEBUG(dbgs() << "Shape propagation error: " << SE.getMessage()
                        << "\n");
    });
    return abortDiagnosticAndRippleCleanup(
        ProcessingStatus::ShapePropagationFailure);
  }
  if (WaitingForSpecialization) {
    LLVM_DEBUG(dbgs() << "Aborting waiting for specialization!\n");
    PS = ProcessingStatus::WaitingForSpecialization;
    return PreservedAnalyses::all();
  }
  LLVM_DEBUG(dbgs() << "Tensor shapes after propagation:\n";
             printTensorInstructions(dbgs()));
  LLVM_DEBUG(dbgs() << "\nValid linear series after propagation:\n";
             printValidSeries(dbgs()); dbgs() << "\n");

  if (Error e = checkRippleSemantics()) {
    llvm::handleAllErrors(std::move(e), [&](const StringError &SE) {
      LLVM_DEBUG(dbgs() << "Ripple semantics error: " << SE.getMessage()
                        << "\n");
    });
    return abortDiagnosticAndRippleCleanup(
        ProcessingStatus::SemanticsCheckFailure);
  }

  emitRippleRemarks();

  LLVM_DEBUG(dbgs() << "Function before vector instruction generation:\n";
             F.print(dbgs()));
  // Generate vector instructions for the instructions w/ vector shapes
  genVectorInstructions();

  assert(allInstructionsHaveRippleShapes() &&
         "Some instruction has no Ripple shape");

  vectorGenerationPostProcess();
  assert(allInstructionsHaveRippleShapes() &&
         "Some instruction has no Ripple shape");

  LLVM_DEBUG(dbgs() << "Function after vector instruction generation:\n";
             F.print(dbgs()));

  // Flatten the control flow for branches that take a vector condition
  ifConvert();
  assert(allInstructionsHaveRippleShapes() &&
         "Some instruction has no Ripple shape");

  LLVM_DEBUG(dbgs() << "\nFunction after Ripple pass:\n\n"; F.print(dbgs());
             dbgs() << "\n");

  if (PadToTargetSIMDLength) {
    LLVM_DEBUG(
        dbgs() << "PadToTargetSIMDLength = true."
               << " Padding ripple generated vector types to align with the "
                  "target's SIMD width.\n";);

    padToTargetSIMDWidth();
  }

  PS = ProcessingStatus::Success;

  // Now that ripple is done with vectorizing the specialization, we can move
  // all the basic block to a function with the correct prototype
  if (isPendingRippleSpecialization(F)) {
    finishSpecialization();
    return PreservedAnalyses::none();
  }

  PreservedAnalyses PA;
  PA.preserve<AAManager>();
  PA.preserve<DominatorTreeAnalysis>();
  PA.preserve<PostDominatorTreeAnalysis>();
  PA.preserve<TargetLibraryAnalysis>();
  return PA;
}

void Ripple::padToTargetSIMDWidth() {
  LLVM_DEBUG(dbgs() << "[padToTargetSIMD] Padding ripple vectors in "
                    << F.getName() << ".\n";);

  const auto *TLI = TM->getSubtargetImpl(F)->getTargetLowering();

  /// GetEvtLegalTypes: Returns a pair of types (T1, T2), where T1 is the
  /// extended vector type corresponding to \p V, and, T2 is the extended vector
  /// type with the least number of scalar elements padded to T1 so that T2 is a
  /// legal vector type in the target. E.g. If V is passed as a 7xfloat on an
  /// AVX-512 target, then GetEvtLegalTypes returns {v7f32, v16f32}.
  /// Note: V must be a vector value.
  auto GetEvtLegalTypes = [this, TLI](const Value *V) -> std::pair<EVT, EVT> {
    auto *TypeV = V->getType();
    EVT EVTType = TLI->getValueType(DL, TypeV, true);
    assert(EVTType.isVector());

    auto VectorFactor =
        targetTransformInfo.getMinimumVF(EVTType.getScalarSizeInBits(), false)
            .getFixedValue();
    if (VectorFactor == 0) {
      // Some targets do not implement .getMinimumVF() method. Try a different
      // way to obtain VectorFactor.
      VectorFactor = targetTransformInfo.getRegisterBitWidth(
                         TargetTransformInfo::RGK_FixedWidthVector) /
                     EVTType.getScalarSizeInBits();
    }

    unsigned MaxPaddedLength =
        VectorFactor * divideCeil(EVTType.getVectorNumElements(), VectorFactor);

    EVT LegalVT;
    if (EVTType.getScalarSizeInBits() != 1) {
      LegalVT = EVT::getVectorVT(F.getContext(), EVTType.getScalarType(),
                                 MaxPaddedLength);
    } else {
      // Special handling for i1-based vector types. First ask TLI for the type
      // to be transformed, if not a padded type then go with MaxPaddedLength
      // vector-type. Why? Architectures like Hexagon have special SIMD width
      // that targetTransformInfo does not account for.
      LegalVT = TLI->getTypeToTransformTo(F.getContext(), EVTType);
      if (LegalVT.getVectorNumElements() < EVTType.getVectorNumElements()) {
        LegalVT = EVT::getVectorVT(F.getContext(), EVTType.getScalarType(),
                                   MaxPaddedLength);
      }
    }

    assert(LegalVT.getVectorNumElements() >= EVTType.getVectorNumElements());

    LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] For type = " << EVTType
                      << ", we pad to " << LegalVT << "\n";);
    return std::make_pair(EVTType, LegalVT);
  };

  /// GetPaddedLength: Returns the expected padded length for the target
  /// associated with \p F.
  auto GetPaddedLength = [&GetEvtLegalTypes](const Value *V) {
    return GetEvtLegalTypes(V).second.getVectorNumElements();
  };

  /// GetMaxPaddedLength: For an array of values, returns the maximum padding
  /// length as dictaed by \p GetPaddedLength for all the values in \p Values.
  auto GetMaxPaddedLength = [&GetPaddedLength](ArrayRef<Value *> Values) {
    unsigned MaxPaddedLength = 0;
    for (auto *V : Values) {
      if (!V->getType()->isVectorTy())
        continue;

      MaxPaddedLength = std::max(MaxPaddedLength, GetPaddedLength(V));
    }
    assert(MaxPaddedLength > 0);
    return MaxPaddedLength;
  };

  /// GetPaddedV: For a value V, returns a value V', such that V' is
  /// V padded with undef values so that it is a vector values with \p
  /// PaddedLength number of elements.
  /// Note: The following things must be true about \p V:
  /// - V should be a constant, OR,
  /// - V must be an instruction previously padded and must have its record in
  /// InstToPadded, InstToPaddedLength.
  auto GetPaddedV =
      [this, &GetPaddedLength](
          Value *V, const unsigned PaddedLength,
          DenseMap<std::pair<Instruction *, unsigned>, Value *> &InstToPadded)
      -> Value * {
    if (!V->getType()->isVectorTy()) {
      LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] Value to be padded (" << *V
                        << ") is scalar => no need to map.\n";);
      return V;
    }
    auto *VType = cast<VectorType>(V->getType());

    if (auto *I = dyn_cast<Instruction>(V)) {
      if (getRippleShape(I).isScalar()) {
        LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] Instruction " << *I
                          << " is a scalar. No replacement needed.\n";);
        return V;
      }
    }
    if (isa<Constant>(V)) {
      if (VType->getElementCount().isFixed()) {
        if (VType->getElementCount().getFixedValue() == PaddedLength) {
          return V;
        }
      }
    }

    LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] Value " << *V
                      << " not present in the mapped values. Applying padding "
                         "transformation to it.\n";);
    if (auto *C = dyn_cast<ConstantDataVector>(V)) {
      std::vector<Constant *> ElVals;
      for (uint64_t IEl = 0; IEl < C->getType()->getNumElements(); ++IEl)
        ElVals.push_back(C->getElementAsConstant(IEl));
      for (uint64_t IEl = C->getType()->getNumElements(); IEl < PaddedLength;
           ++IEl)
        ElVals.push_back(UndefValue::get(V->getType()->getScalarType()));
      auto *NewC = ConstantVector::get(ElVals);
      LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] C = " << *C << " will become "
                        << *NewC << "\n";);
      return NewC;
    }
    if (isa<PoisonValue>(V)) {
      auto *NewVType =
          VectorType::get(VType->getScalarType(), PaddedLength, false);
      return PoisonValue::get(NewVType);
    }
    if (isa<UndefValue>(V)) {
      auto *NewVType =
          VectorType::get(VType->getScalarType(), PaddedLength, false);
      return UndefValue::get(NewVType);
    }
    if (auto *C = dyn_cast<ConstantVector>(V)) {
      std::vector<Constant *> ElVals;
      for (uint64_t IEl = 0; IEl < C->getType()->getNumElements(); ++IEl)
        ElVals.push_back(C->getOperand(IEl));
      for (uint64_t IEl = C->getType()->getNumElements(); IEl < PaddedLength;
           ++IEl)
        ElVals.push_back(UndefValue::get(V->getType()->getScalarType()));
      auto *NewC = ConstantVector::get(ElVals);
      return NewC;
    }
    if (isa<ConstantAggregateZero>(V)) {
      auto *NewVType =
          VectorType::get(VType->getScalarType(), PaddedLength, false);
      return ConstantAggregateZero::get(NewVType);
    }
    if (auto *I = dyn_cast<Instruction>(V)) {
      if (auto It = InstToPadded.find({I, PaddedLength});
          It != InstToPadded.end()) {
        return It->second;
      }
      auto NaturalPaddedLngthI = GetPaddedLength(I);
      auto *VWithDifferentPad =
          dyn_cast<Instruction>(InstToPadded.at({I, NaturalPaddedLngthI}));

      LLVM_DEBUG(dbgs() << "[padToTargetSIMD] Expanding " << *VWithDifferentPad
                        << " to " << PaddedLength << ".\n";);

      // step 1. save old insertion point.
      auto OldIP = irBuilder.saveIP();
      // step 2. insert the concatenation.
      setInsertPointAfter(irBuilder, VWithDifferentPad);

      std::vector<int> MaskEls(PaddedLength, -1);
      for (unsigned IEl = 0; IEl < std::min(PaddedLength, NaturalPaddedLngthI);
           ++IEl)
        MaskEls[IEl] = IEl;

      auto *PaddedV = irBuilder.CreateShuffleVector(
          VWithDifferentPad, MaskEls, VWithDifferentPad->getName() + ".pad");
      // step 3. restore insertion point.
      irBuilder.restoreIP(OldIP);
      InstToPadded.insert_or_assign({I, PaddedLength}, PaddedV);
      return PaddedV;
    }
    dbgs() << "Unimplemented for -- " << *V << "\n";
    llvm_unreachable("Padding not implemented for this value.");
  };

  /// ExtractStartingSubVec: Returns the subvector value S, where S is value
  /// corresponding to a vector holding the first \p SubvecLength elements of
  /// \p Vec.
  /// Note that we do not change the insertion point of \p IRB and we
  /// insert a single instruction at the current insertion point.
  auto ExtractStartingSubVec = [](IRBuilder<> &IRB, Value *Vec,
                                  unsigned SubvecLength) {
    auto *VecTy = dyn_cast<VectorType>(Vec->getType());
    auto *SubvecTy =
        FixedVectorType::get(VecTy->getElementType(), SubvecLength);
    auto *Subvec = IRB.CreateExtractVector(
        SubvecTy, Vec, static_cast<uint64_t>(0), Vec->getName() + ".subvec");
    return Subvec;
  };

  /// GetInstructionsThatMustBePadded: Returns either:
  /// - A set of instructions that must be padded to align well
  ///    with the hardware's SIMD width, OR,
  /// - An error if the function `F` contains an instruction that is
  ///   unsupported.
  auto GetInstructionsThatMustBePadded =
      [this, &GetPaddedLength,
       &GetMaxPaddedLength]() -> Expected<std::set<Instruction *>> {
    std::set<Instruction *> MustBePadded;
    std::queue<Instruction *> Worklist;

    auto AnyValuesKnownToBeMustPadded =
        [&MustBePadded](ArrayRef<Value *> Values,
                        unsigned PaddedLength) -> bool {
      for (Value *V : Values) {
        if (auto *VType = dyn_cast<FixedVectorType>(V->getType())) {
          bool TypeAlignsWellWithPaddedLength =
              (VType->getElementCount().getFixedValue() == PaddedLength);
          if (isa<Constant>(V)) {
            if (!TypeAlignsWellWithPaddedLength)
              return true;
            continue;
          }
          if (auto *I = dyn_cast<Instruction>(V)) {
            if (!TypeAlignsWellWithPaddedLength)
              return true;
            if (MustBePadded.find(I) != MustBePadded.end())
              return true;
            continue;
          }
          llvm_unreachable("Control should not reach here.");
        } else {
          // scalar value => do not pad.
          continue;
        }
      }

      return false;
    };

    auto AddUserInstsToWorklist = [](Instruction *I,
                                     std::queue<Instruction *> &Worklist) {
      for (auto *U : I->users()) {
        if (auto *UserI = dyn_cast<Instruction>(U)) {
          Worklist.push(UserI);
        }
      }
    };

    // Initialize the worklist.
    for (auto *BB : getFuncRPOT()) {
      for (auto &I : *BB) {
        auto ShapeI = getRippleShape(&I);
        if (ShapeI.isScalar()) {
          if (isa<ExtractElementInst>(I)) {
            Worklist.push(&I);
          }
        } else {
          Worklist.push(&I);
        }
      }
    }

    while (!Worklist.empty()) {
      auto *I = Worklist.front();
      auto ShapeI = getRippleShape(I);
      Worklist.pop();

      if (MustBePadded.find(I) != MustBePadded.end())
        continue; // already marked as padded.

      if (ShapeI.isScalar()) {
        if (auto *ExtractElementI = dyn_cast<ExtractElementInst>(I)) {
          auto *VecOperand = ExtractElementI->getVectorOperand();
          auto PaddedLength = GetPaddedLength(VecOperand);

          if (getRippleShape(VecOperand).isVector() &&
              AnyValuesKnownToBeMustPadded({VecOperand}, PaddedLength))
            MustBePadded.insert(I);
        }
        continue;
      }

      if (auto *BinOpI = dyn_cast<BinaryOperator>(I)) {
        auto *Lhs = BinOpI->getOperand(0);
        auto *Rhs = BinOpI->getOperand(1);
        auto PaddedLength = GetPaddedLength(BinOpI);
        if (AnyValuesKnownToBeMustPadded({Lhs, Rhs}, PaddedLength)) {
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *CastI = dyn_cast<CastInst>(I)) {
        auto *SrcVal = CastI->getOperand(0);
        unsigned PaddedLength = GetMaxPaddedLength({CastI, SrcVal});
        if (AnyValuesKnownToBeMustPadded({SrcVal, CastI}, PaddedLength)) {
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *GEPI = dyn_cast<GetElementPtrInst>(I)) {
        SmallVector<Value *> Operands(GEPI->operand_values().begin(),
                                      GEPI->operand_values().end());

        unsigned PaddedLength =
            std::max(GetPaddedLength(GEPI), GetMaxPaddedLength(Operands));

        if (AnyValuesKnownToBeMustPadded(Operands, PaddedLength) ||
            AnyValuesKnownToBeMustPadded({GEPI}, PaddedLength)) {
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *LoadI = dyn_cast<LoadInst>(I)) {
        unsigned PaddedLength = GetPaddedLength(LoadI);
        if (AnyValuesKnownToBeMustPadded({LoadI}, PaddedLength)) {
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *StoreI = dyn_cast<StoreInst>(I)) {
        auto PaddedLength = GetPaddedLength(StoreI->getValueOperand());
        if (AnyValuesKnownToBeMustPadded({StoreI->getValueOperand()},
                                         PaddedLength)) {
          MustBePadded.insert(I);
        }
      } else if (auto *IntrnscInst = dyn_cast<IntrinsicInst>(I)) {

        SmallVector<Value *> Operands(IntrnscInst->arg_begin(),
                                      IntrnscInst->arg_end());
        unsigned PaddedLength = GetMaxPaddedLength(Operands);
        if (IntrnscInst->getType()->isVectorTy()) {
          PaddedLength = std::max(PaddedLength, GetPaddedLength(IntrnscInst));
        }

        if (AnyValuesKnownToBeMustPadded(Operands, PaddedLength) ||
            ((IntrnscInst->getType()->isVectorTy()) &&
             AnyValuesKnownToBeMustPadded({IntrnscInst}, PaddedLength))

        ) {
          MustBePadded.insert(I);
          if (IntrnscInst->getType()->isVectorTy())
            AddUserInstsToWorklist(I, Worklist);
        }

        switch (IntrnscInst->getIntrinsicID()) {
        case Intrinsic::masked_load:
        case Intrinsic::masked_store:
        case Intrinsic::masked_scatter:
        case Intrinsic::masked_gather:
          break;
        default:
          if (auto VectorIntrnscId =
                  getVectorIntrinsicIDForCall(IntrnscInst, &targetLibraryInfo);
              VectorIntrnscId == Intrinsic::not_intrinsic) {
            std::string ErrStr;
            raw_string_ostream OSS(ErrStr);
            OSS << "Unimplemented intrisic -- " << *I;
            OSS.flush();

            return createStringError(inconvertibleErrorCode(), ErrStr);
          }
        }

      } else if (auto *InsrtElmntI = dyn_cast<InsertElementInst>(I)) {
        auto *VecOperand = InsrtElmntI->getOperand(0);
        unsigned PaddedLength = GetPaddedLength(VecOperand);

        if (AnyValuesKnownToBeMustPadded({VecOperand, InsrtElmntI},
                                         PaddedLength)) {
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *CmpI = dyn_cast<CmpInst>(I)) {
        auto *Op1 = CmpI->getOperand(0);
        auto *Op2 = CmpI->getOperand(1);
        auto PaddedLength = GetMaxPaddedLength({Op1, Op2, CmpI});
        if (AnyValuesKnownToBeMustPadded({Op1, Op2, CmpI}, PaddedLength)) {
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *SelectI = dyn_cast<SelectInst>(I)) {
        auto *Cond = SelectI->getCondition();
        auto *True = SelectI->getTrueValue();
        auto *False = SelectI->getFalseValue();
        unsigned PaddedLength = GetMaxPaddedLength({Cond, True, False});

        if (AnyValuesKnownToBeMustPadded({Cond, True, False, SelectI},
                                         PaddedLength)) {
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *UnOpI = dyn_cast<UnaryOperator>(I)) {
        auto PaddedLength = GetMaxPaddedLength({UnOpI, UnOpI->getOperand(0)});

        if (AnyValuesKnownToBeMustPadded({UnOpI, UnOpI->getOperand(0)},
                                         PaddedLength)) {
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *ShflVctrI = dyn_cast<ShuffleVectorInst>(I)) {

        auto *V1 = ShflVctrI->getOperand(0);
        auto *V2 = ShflVctrI->getOperand(1);

        auto PaddedLength = GetPaddedLength(ShflVctrI);
        auto PaddedOperandLength = GetPaddedLength(V1);

        if (AnyValuesKnownToBeMustPadded({V1, V2}, PaddedOperandLength) ||
            AnyValuesKnownToBeMustPadded({ShflVctrI}, PaddedLength)) {
          // already maps well to the device -> do nothing.
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *Phi = dyn_cast<PHINode>(I)) {
        SmallVector<Value *> IncomingValues(Phi->incoming_values().begin(),
                                            Phi->incoming_values().end());
        unsigned PaddedLength = GetPaddedLength(Phi);

        if (AnyValuesKnownToBeMustPadded(IncomingValues, PaddedLength)) {
          // already maps well to the device -> do nothing.
          MustBePadded.insert(I);
          AddUserInstsToWorklist(I, Worklist);
        }
      } else if (auto *CallI = dyn_cast<CallInst>(I)) {
        std::string ErrStr;
        raw_string_ostream OSS(ErrStr);
        OSS << "Only call instructions that accept legalizable types suported. "
               "Unsupported call: "
            << *I;
        OSS.flush();

        if (auto *CallIVtype = dyn_cast<FixedVectorType>(CallI->getType())) {
          if (CallIVtype->getNumElements() != GetPaddedLength(CallI)) {
            return createStringError(inconvertibleErrorCode(), ErrStr);
          }
        }

        for (auto &Arg : CallI->args()) {
          auto *ArgV = Arg.get();
          if (auto *ArgVtype = dyn_cast<FixedVectorType>(ArgV->getType())) {
            if (ArgVtype->getNumElements() != GetPaddedLength(ArgV)) {
              return createStringError(inconvertibleErrorCode(), ErrStr);
            }
          }
        }
      } else {
        std::string ErrStr;
        raw_string_ostream OSS(ErrStr);
        OSS << "Unimplemented instruction -- " << *I;
        OSS.flush();

        return createStringError(inconvertibleErrorCode(), ErrStr);
      }
    }

    return std::move(MustBePadded);
  };

  auto ExpectedInstsThatMustBePadded = GetInstructionsThatMustBePadded();
  if (!ExpectedInstsThatMustBePadded) {
    auto Err = ExpectedInstsThatMustBePadded.takeError();
    std::string ErrStr;
    raw_string_ostream OSS(ErrStr);
    OSS << "Not padding Ripple generated vectors to target's SIMD length due "
           "to '"
        << toString(std::move(Err)) << "'.";
    OSS.flush();
    SMDiagnostic Diag("", SourceMgr::DiagKind::DK_Warning, ErrStr);
    Diag.print("PadToTargetSIMD.", errs());
    return;
  }
  auto InstsThatMustBePadded = *ExpectedInstsThatMustBePadded;

  LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] #Instructions that must be padded = "
                    << InstsThatMustBePadded.size() << ".\n";);

  if (InstsThatMustBePadded.size() == 0) {
    // Already aligns well with target SIMD -> nothing to do.
    return;
  }

  /// InstToPadded: A mapping from the pairs of the form (I, P) to (I'), where
  /// I' is the vector value V padded to length "P".
  DenseMap<std::pair<Instruction *, unsigned>, Value *> InstToPadded;
  /// InstructionsToRemove: Since we are replacing instructions with their
  /// padded versions, the unpadded instructions must be removed.
  /// InstructionsToRemove records those instructions.
  SmallVector<Instruction *> InstructionsToRemove;

  DenseMap<PHINode *, PHINode *> OldPhiToNewPhi;
  SmallVector<Instruction *> InstsThatMustBePaddedRPOT;

  for (auto *BB : getFuncRPOT()) {
    for (auto &I : *BB) {
      if (InstsThatMustBePadded.find(&I) != InstsThatMustBePadded.end()) {
        InstsThatMustBePaddedRPOT.push_back(&I);
      } else {
        if (getRippleShape(&I).isVector()) {
          auto PaddedLength = GetPaddedLength(&I);
          InstToPadded.insert_or_assign({&I, PaddedLength}, &I);
        }
      }
    }
  }

  for (auto *I : InstsThatMustBePaddedRPOT) {
    if (auto *ExtractElementI = dyn_cast<ExtractElementInst>(I)) {
      auto *VecOperand = ExtractElementI->getVectorOperand();
      auto PaddedLength = GetPaddedLength(VecOperand);

      auto *NewVecOperand = GetPaddedV(VecOperand, PaddedLength, InstToPadded);
      irBuilder.SetInsertPoint(ExtractElementI);
      auto *NewExtractElementI = irBuilder.CreateExtractElement(
          NewVecOperand, ExtractElementI->getIndexOperand(),
          ExtractElementI->getName() + ".pad");
      ExtractElementI->replaceAllUsesWith(NewExtractElementI);
      InstructionsToRemove.push_back(ExtractElementI);
    } else if (auto *BinOpI = dyn_cast<BinaryOperator>(I)) {
      auto *Lhs = BinOpI->getOperand(0);
      auto *Rhs = BinOpI->getOperand(1);

      auto PaddedLength = GetPaddedLength(BinOpI);

      irBuilder.SetInsertPoint(BinOpI);

      IRBuilder<>::FastMathFlagGuard FMFGuard(irBuilder);
      FastMathFlags FMF = isa<FPMathOperator>(BinOpI)
                              ? BinOpI->getFastMathFlags()
                              : FastMathFlags{};
      irBuilder.setFastMathFlags(FMF);

      auto *NewLHS = GetPaddedV(Lhs, PaddedLength, InstToPadded);
      auto *NewRHS = GetPaddedV(Rhs, PaddedLength, InstToPadded);
      auto *NewBinOp = irBuilder.CreateBinOp(
          BinOpI->getOpcode(), NewLHS, NewRHS, BinOpI->getName() + ".pad",
          BinOpI->getMetadata(LLVMContext::MD_fpmath));
      InstToPadded.insert_or_assign({BinOpI, PaddedLength}, NewBinOp);
      InstructionsToRemove.push_back(BinOpI);
    } else if (auto *CastI = dyn_cast<CastInst>(I)) {
      auto *SrcVal = CastI->getOperand(0);
      unsigned PaddedLength = GetMaxPaddedLength({CastI, SrcVal});

      irBuilder.SetInsertPoint(CastI);

      auto *NewDestTy = VectorType::get(CastI->getType()->getScalarType(),
                                        PaddedLength, false);
      auto *NewSrcVal = GetPaddedV(SrcVal, PaddedLength, InstToPadded);

      auto *NewCast =
          irBuilder.CreateCast(CastI->getOpcode(), NewSrcVal, NewDestTy);
      InstToPadded.insert_or_assign({CastI, PaddedLength}, NewCast);
      auto NaturalPaddedLength = GetPaddedLength(CastI);
      if (NaturalPaddedLength != PaddedLength) {
        auto *NewCastPaddedNaturally =
            ExtractStartingSubVec(irBuilder, NewCast, NaturalPaddedLength);
        InstToPadded.insert_or_assign({CastI, NaturalPaddedLength},
                                      NewCastPaddedNaturally);
      }

      InstructionsToRemove.push_back(CastI);
    } else if (auto *GEPI = dyn_cast<GetElementPtrInst>(I)) {
      auto [EVTType, LegalVT] = GetEvtLegalTypes(GEPI);
      SmallVector<Value *> Operands(GEPI->operand_values().begin(),
                                    GEPI->operand_values().end());

      unsigned PaddedLength = std::max(LegalVT.getVectorNumElements(),
                                       GetMaxPaddedLength(Operands));

      // GEP is an elementwise operation => pad every operand
      Value *NewPointer =
          GetPaddedV(GEPI->getPointerOperand(), PaddedLength, InstToPadded);
      SmallVector<Value *, 4> NewIndices;

      for (auto &Idx : GEPI->indices()) {
        NewIndices.push_back(GetPaddedV(Idx.get(), PaddedLength, InstToPadded));
      }
      irBuilder.SetInsertPoint(GEPI);
      auto *NewGEP =
          irBuilder.CreateGEP(VectorType::get(GEPI->getType()->getScalarType(),
                                              PaddedLength, false),
                              NewPointer, NewIndices, GEPI->getName() + ".pad");

      auto NaturallyPaddedLength = LegalVT.getVectorNumElements();
      if (NaturallyPaddedLength != PaddedLength) {
        auto *NewNaturallyPaddedGEP =
            ExtractStartingSubVec(irBuilder, NewGEP, NaturallyPaddedLength);
        InstToPadded.insert_or_assign({GEPI, NaturallyPaddedLength},
                                      NewNaturallyPaddedGEP);
      }

      InstToPadded.insert_or_assign({GEPI, PaddedLength}, NewGEP);
      InstructionsToRemove.push_back(GEPI);
    } else if (auto *LoadI = dyn_cast<LoadInst>(I)) {
      auto [EVTType, LegalVT] = GetEvtLegalTypes(LoadI);
      auto PaddedLength = LegalVT.getVectorNumElements();

      irBuilder.SetInsertPoint(LoadI);
      auto *NewLoadTy = VectorType::get(LoadI->getType()->getScalarType(),
                                        PaddedLength, false);
      std::vector<Constant *> MaskEls(PaddedLength, nullptr);
      for (unsigned IEl = 0; IEl < PaddedLength; ++IEl)
        MaskEls[IEl] = irBuilder.getInt1(IEl < EVTType.getVectorNumElements());

      auto *NewLoad = irBuilder.CreateMaskedLoad(
          NewLoadTy, LoadI->getPointerOperand(), LoadI->getAlign(),
          ConstantVector::get(MaskEls), UndefValue::get(NewLoadTy),
          LoadI->getName() + ".pad");
      LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] Load, " << *LoadI
                        << " was mapped to " << *NewLoad << "\n";);
      InstToPadded.insert_or_assign({LoadI, PaddedLength}, NewLoad);
      InstructionsToRemove.push_back(LoadI);
    } else if (auto *StoreI = dyn_cast<StoreInst>(I)) {
      auto [EVTType, LegalVT] = GetEvtLegalTypes(StoreI->getValueOperand());
      unsigned PaddedLength = LegalVT.getVectorNumElements();
      auto *NewValueOperand =
          GetPaddedV(StoreI->getValueOperand(), PaddedLength, InstToPadded);

      irBuilder.SetInsertPoint(StoreI);
      std::vector<Constant *> MaskEls(PaddedLength, nullptr);
      for (unsigned IEl = 0; IEl < PaddedLength; ++IEl)
        MaskEls[IEl] = irBuilder.getInt1(IEl < EVTType.getVectorNumElements());

      [[maybe_unused]] auto *NewStore = irBuilder.CreateMaskedStore(
          NewValueOperand, StoreI->getPointerOperand(), StoreI->getAlign(),
          ConstantVector::get(MaskEls));
      LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] Store, " << *StoreI
                        << " was mapped to " << *NewStore << "\n";);
      InstructionsToRemove.push_back(StoreI);
    } else if (auto *IntrnscInst = dyn_cast<IntrinsicInst>(I)) {

      SmallVector<Value *> Operands(IntrnscInst->arg_begin(),
                                    IntrnscInst->arg_end());
      unsigned PaddedLength = GetMaxPaddedLength(Operands);
      if (IntrnscInst->getType()->isVectorTy()) {
        auto [EVTType, LegalVT] = GetEvtLegalTypes(IntrnscInst);
        PaddedLength = std::max(PaddedLength, LegalVT.getVectorNumElements());
      }

      if (IntrnscInst->getIntrinsicID() == Intrinsic::masked_load) {
        unsigned UnpaddedLength =
            GetEvtLegalTypes(IntrnscInst).first.getVectorNumElements();
        irBuilder.SetInsertPoint(IntrnscInst);
        auto *NewLoadTy = VectorType::get(
            IntrnscInst->getType()->getScalarType(), PaddedLength, false);

        std::vector<Constant *> MaskEls(PaddedLength, nullptr);
        for (unsigned IEl = 0; IEl < PaddedLength; ++IEl)
          MaskEls[IEl] = irBuilder.getInt1(IEl < UnpaddedLength);

        Value *PaddedMask = GetPaddedV(IntrnscInst->getArgOperand(1),
                                       PaddedLength, InstToPadded);
        Value *NewMask =
            irBuilder.CreateAnd(ConstantVector::get(MaskEls), PaddedMask,
                                PaddedMask->getName() + ".pad");
        Value *NewPassthru = GetPaddedV(IntrnscInst->getArgOperand(2),
                                        PaddedLength, InstToPadded);
        Align AlignVal(IntrnscInst->getParamAlign(0).valueOrOne());

        auto *NewMaskedLoad =
            irBuilder.CreateMaskedLoad(NewLoadTy, IntrnscInst->getArgOperand(0),
                                       AlignVal, NewMask, NewPassthru);

        LLVM_DEBUG(dbgs() << "'" << *IntrnscInst << "' got converted to '"
                          << *NewMaskedLoad << "', with mask = '" << *NewMask
                          << "'.\n";);

        InstToPadded.insert_or_assign({IntrnscInst, PaddedLength},
                                      NewMaskedLoad);
        InstructionsToRemove.push_back(IntrnscInst);
      } else if (IntrnscInst->getIntrinsicID() == Intrinsic::masked_store) {
        irBuilder.SetInsertPoint(IntrnscInst);
        auto UnpaddedLength = GetEvtLegalTypes(IntrnscInst->getArgOperand(0))
                                  .first.getVectorNumElements();

        Value *NewVal = GetPaddedV(IntrnscInst->getArgOperand(0), PaddedLength,
                                   InstToPadded);

        std::vector<Constant *> MaskEls(PaddedLength, nullptr);
        for (unsigned IEl = 0; IEl < PaddedLength; ++IEl)
          MaskEls[IEl] = irBuilder.getInt1(IEl < UnpaddedLength);

        Value *PaddedMask = GetPaddedV(IntrnscInst->getArgOperand(2),
                                       PaddedLength, InstToPadded);
        Value *NewMask =
            irBuilder.CreateAnd(ConstantVector::get(MaskEls), PaddedMask,
                                PaddedMask->getName() + ".pad");
        Align AlignVal(IntrnscInst->getParamAlign(1).valueOrOne());

        irBuilder.CreateMaskedStore(NewVal, IntrnscInst->getArgOperand(1),
                                    AlignVal, NewMask);
        InstructionsToRemove.push_back(IntrnscInst);
      } else if (IntrnscInst->getIntrinsicID() == Intrinsic::masked_gather) {
        unsigned UnpaddedLength =
            GetEvtLegalTypes(IntrnscInst).first.getVectorNumElements();

        irBuilder.SetInsertPoint(IntrnscInst);
        auto *NewGatherTy = VectorType::get(
            IntrnscInst->getType()->getScalarType(), PaddedLength, false);
        Value *NewPtrs = GetPaddedV(IntrnscInst->getArgOperand(0), PaddedLength,
                                    InstToPadded);
        Align AlignVal(IntrnscInst->getParamAlign(0).valueOrOne());

        std::vector<Constant *> MaskEls(PaddedLength, nullptr);
        for (unsigned IEl = 0; IEl < PaddedLength; ++IEl)
          MaskEls[IEl] = irBuilder.getInt1(IEl < UnpaddedLength);

        Value *PaddedMask = GetPaddedV(IntrnscInst->getArgOperand(1),
                                       PaddedLength, InstToPadded);
        Value *NewMask =
            irBuilder.CreateAnd(ConstantVector::get(MaskEls), PaddedMask,
                                PaddedMask->getName() + ".pad");
        Value *NewPassthru = GetPaddedV(IntrnscInst->getArgOperand(2),
                                        PaddedLength, InstToPadded);

        auto *NewMaskedGather = irBuilder.CreateMaskedGather(
            NewGatherTy, NewPtrs, AlignVal, NewMask, NewPassthru,
            IntrnscInst->getName() + ".pad");

        InstToPadded.insert_or_assign({IntrnscInst, PaddedLength},
                                      NewMaskedGather);

        LLVM_DEBUG(dbgs() << "Masked gather '" << *IntrnscInst << "' became '"
                          << *NewMaskedGather << "' with the mask '" << *NewMask
                          << "'.\n";);

        auto NaturalPaddedLength = GetPaddedLength(IntrnscInst);
        if (NaturalPaddedLength != PaddedLength) {
          auto *NewNaturallyPaddedMaskedGather = ExtractStartingSubVec(
              irBuilder, NewMaskedGather, NaturalPaddedLength);
          InstToPadded.insert_or_assign({IntrnscInst, NaturalPaddedLength},
                                        NewNaturallyPaddedMaskedGather);
        }

        InstructionsToRemove.push_back(IntrnscInst);
      } else if (IntrnscInst->getIntrinsicID() == Intrinsic::masked_scatter) {
        auto UnpaddedLength = GetEvtLegalTypes(IntrnscInst->getArgOperand(0))
                                  .first.getVectorNumElements();
        irBuilder.SetInsertPoint(IntrnscInst);

        Value *NewVals = GetPaddedV(IntrnscInst->getArgOperand(0), PaddedLength,
                                    InstToPadded);
        Value *NewPtrs = GetPaddedV(IntrnscInst->getArgOperand(1), PaddedLength,
                                    InstToPadded);
        Align AlignVal(IntrnscInst->getParamAlign(1).valueOrOne());

        std::vector<Constant *> MaskEls(PaddedLength, nullptr);
        for (unsigned IEl = 0; IEl < PaddedLength; ++IEl)
          MaskEls[IEl] = irBuilder.getInt1(IEl < UnpaddedLength);

        Value *PaddedMask = GetPaddedV(IntrnscInst->getArgOperand(2),
                                       PaddedLength, InstToPadded);
        Value *NewMask =
            irBuilder.CreateAnd(ConstantVector::get(MaskEls), PaddedMask,
                                PaddedMask->getName() + ".pad");

        irBuilder.CreateMaskedScatter(NewVals, NewPtrs, AlignVal, NewMask);
        InstructionsToRemove.push_back(IntrnscInst);
      } else if (auto VectorIntrnscId = getVectorIntrinsicIDForCall(
                     IntrnscInst, &targetLibraryInfo);
                 VectorIntrnscId != Intrinsic::not_intrinsic) {

        auto *NewTy = VectorType::get(IntrnscInst->getType()->getScalarType(),
                                      PaddedLength, false);
        SmallVector<Value *, 2> NewArgs;
        for (Value *Arg : IntrnscInst->args())
          NewArgs.push_back(GetPaddedV(Arg, PaddedLength, InstToPadded));

        irBuilder.SetInsertPoint(IntrnscInst);
        auto *NewIntrnscInst = irBuilder.CreateIntrinsic(
            NewTy, VectorIntrnscId, NewArgs,
            isa<FPMathOperator>(IntrnscInst) ? IntrnscInst : nullptr,
            IntrnscInst->getName() + ".pad");

        InstToPadded.insert_or_assign({IntrnscInst, PaddedLength},
                                      NewIntrnscInst);

        auto NaturalPaddedLength = GetPaddedLength(IntrnscInst);
        if (NaturalPaddedLength != PaddedLength) {
          auto *NewNaturallyPaddedIntrnscInst = ExtractStartingSubVec(
              irBuilder, NewIntrnscInst, NaturalPaddedLength);
          InstToPadded.insert_or_assign({IntrnscInst, NaturalPaddedLength},
                                        NewNaturallyPaddedIntrnscInst);
        }

        InstructionsToRemove.push_back(IntrnscInst);
      } else {
        llvm_unreachable("Not sure how to handle this intrinsic inst.\n");
      }
    } else if (auto *InsrtElmntI = dyn_cast<InsertElementInst>(I)) {
      auto [EVTType, LegalVT] = GetEvtLegalTypes(InsrtElmntI);
      auto *VecOperand = InsrtElmntI->getOperand(0);
      unsigned PaddedLength = LegalVT.getVectorNumElements();
      auto *NewVecOperand = GetPaddedV(VecOperand, PaddedLength, InstToPadded);
      irBuilder.SetInsertPoint(InsrtElmntI);
      auto *NewInsrtElmntI = irBuilder.CreateInsertElement(
          NewVecOperand, InsrtElmntI->getOperand(1), InsrtElmntI->getOperand(2),
          InsrtElmntI->getName() + ".pad");
      InstToPadded.insert_or_assign({InsrtElmntI, PaddedLength},
                                    NewInsrtElmntI);
      InstructionsToRemove.push_back(InsrtElmntI);
    } else if (auto *CmpI = dyn_cast<CmpInst>(I)) {
      auto [EVTType, LegalVT] = GetEvtLegalTypes(CmpI);
      auto *Op1 = CmpI->getOperand(0);
      auto *Op2 = CmpI->getOperand(1);
      auto PaddedLength = std::max(LegalVT.getVectorNumElements(),
                                   GetMaxPaddedLength({Op1, Op2}));

      irBuilder.SetInsertPoint(CmpI);

      auto *NewOp1 = GetPaddedV(Op1, PaddedLength, InstToPadded);
      auto *NewOp2 = GetPaddedV(Op2, PaddedLength, InstToPadded);
      auto *NewCmpI = irBuilder.CreateCmp(CmpI->getPredicate(), NewOp1, NewOp2);

      auto NaturalPaddedLength = GetPaddedLength(CmpI);
      if (NaturalPaddedLength != PaddedLength) {
        auto *NewNaturallyPaddedCmpI =
            ExtractStartingSubVec(irBuilder, NewCmpI, NaturalPaddedLength);
        InstToPadded.insert_or_assign({CmpI, NaturalPaddedLength},
                                      NewNaturallyPaddedCmpI);
      }

      InstToPadded.insert_or_assign({CmpI, PaddedLength}, NewCmpI);
      InstructionsToRemove.push_back(CmpI);
    } else if (auto *SelectI = dyn_cast<SelectInst>(I)) {
      auto [EVTType, LegalVT] = GetEvtLegalTypes(SelectI);
      auto *Cond = SelectI->getCondition();
      auto *True = SelectI->getTrueValue();
      auto *False = SelectI->getFalseValue();

      unsigned PaddedLength = std::max(LegalVT.getVectorNumElements(),
                                       GetMaxPaddedLength({Cond, True, False}));

      irBuilder.SetInsertPoint(SelectI);

      auto *NewCond = GetPaddedV(Cond, PaddedLength, InstToPadded);
      auto *NewTrue = GetPaddedV(True, PaddedLength, InstToPadded);
      auto *NewFalse = GetPaddedV(False, PaddedLength, InstToPadded);

      auto *NewSelectI = irBuilder.CreateSelect(NewCond, NewTrue, NewFalse,
                                                SelectI->getName() + ".pad");

      auto NaturalPaddedLength = GetPaddedLength(SelectI);
      if (NaturalPaddedLength != PaddedLength) {
        auto *NewNaturallyPaddedSelectI =
            ExtractStartingSubVec(irBuilder, NewSelectI, NaturalPaddedLength);
        InstToPadded.insert_or_assign({SelectI, NaturalPaddedLength},
                                      NewNaturallyPaddedSelectI);
      }

      InstToPadded.insert_or_assign({SelectI, PaddedLength}, NewSelectI);
      InstructionsToRemove.push_back(SelectI);
    } else if (auto *UnOpI = dyn_cast<UnaryOperator>(I)) {
      auto [EVTType, LegalVT] = GetEvtLegalTypes(UnOpI);
      auto PaddedLength = std::max(LegalVT.getVectorNumElements(),
                                   GetPaddedLength(UnOpI->getOperand(0)));

      auto *NewOperand =
          GetPaddedV(UnOpI->getOperand(0), PaddedLength, InstToPadded);
      auto *NewUnOpI = irBuilder.CreateUnOp(UnOpI->getOpcode(), NewOperand);
      InstToPadded.insert_or_assign({UnOpI, PaddedLength}, NewUnOpI);

      auto NaturalPaddedLength = GetPaddedLength(UnOpI);
      if (NaturalPaddedLength != PaddedLength) {
        auto *NewNaturallyPaddedUnOpI =
            ExtractStartingSubVec(irBuilder, NewUnOpI, NaturalPaddedLength);
        InstToPadded.insert_or_assign({UnOpI, NaturalPaddedLength},
                                      NewNaturallyPaddedUnOpI);
      }

      InstructionsToRemove.push_back(UnOpI);
    } else if (auto *ShflVctrI = dyn_cast<ShuffleVectorInst>(I)) {

      auto *V1 = ShflVctrI->getOperand(0);
      auto *V2 = ShflVctrI->getOperand(1);
      auto Mask = ShflVctrI->getShuffleMask();

      auto [EVTType, LegalVT] = GetEvtLegalTypes(ShflVctrI);
      auto PaddedLength = LegalVT.getVectorNumElements();
      auto UnpaddedOperandLength =
          GetEvtLegalTypes(V1).first.getVectorNumElements();
      auto PaddedOperandLength = GetPaddedLength(V1);

      auto *NewV1 = GetPaddedV(V1, PaddedOperandLength, InstToPadded);
      auto *NewV2 = GetPaddedV(V2, PaddedOperandLength, InstToPadded);

      assert(EVTType.getVectorNumElements() == Mask.size());

      std::vector<int> NewMask(PaddedLength, -1);

      for (unsigned IVecLane = 0; IVecLane < Mask.size(); ++IVecLane) {
        if (Mask[IVecLane] == -1)
          NewMask[IVecLane] = -1; // Poison mask should be passthrough.
        else if (Mask[IVecLane] < static_cast<int>(UnpaddedOperandLength))
          NewMask[IVecLane] = Mask[IVecLane];
        else
          NewMask[IVecLane] =
              Mask[IVecLane] + (PaddedOperandLength - UnpaddedOperandLength);
      }

      irBuilder.SetInsertPoint(ShflVctrI);
      auto *NewShflVctrI = irBuilder.CreateShuffleVector(
          NewV1, NewV2, NewMask, ShflVctrI->getName() + ".pad");
      InstToPadded.insert_or_assign({ShflVctrI, PaddedLength}, NewShflVctrI);
      InstructionsToRemove.push_back(ShflVctrI);
    } else if (auto *Phi = dyn_cast<PHINode>(I)) {
      // NB: Phis need special treatment. In this loop over the instructions
      // of the function, the implementation assumes that the padded version
      // of operands of an instruction are available in `InstToPadded`. This
      // is generally not true for Phis, e.g. in the presence of a back edge.
      // We address this by recording the newly created Phi nodes in
      // OldPhiToNewPhi and the operands of the new phi nodes and updated
      // after the completion of this loop.

      SmallVector<Value *> IncomingValues(Phi->incoming_values().begin(),
                                          Phi->incoming_values().end());
      auto [EVType, LegalVT] = GetEvtLegalTypes(Phi);
      unsigned PaddedLength = LegalVT.getVectorNumElements();

      irBuilder.SetInsertPoint(Phi);
      auto *NewPhi = irBuilder.CreatePHI(
          FixedVectorType::get(
              dyn_cast<FixedVectorType>(Phi->getType())->getElementType(),
              PaddedLength),
          0, Phi->getName() + ".pad");

      // Record the new-Phi node to be updated later.
      OldPhiToNewPhi.insert({Phi, NewPhi});

      InstToPadded.insert_or_assign({Phi, PaddedLength}, NewPhi);
      InstructionsToRemove.push_back(Phi);
    } else {
      llvm_unreachable("Not yet Implemented error.");
    }
  }

  for (auto &[OldPhi, NewPhi] : OldPhiToNewPhi) {
    auto PaddedLength = GetPaddedLength(OldPhi);
    auto NumIncomingValues = OldPhi->getNumIncomingValues();
    for (unsigned IIncoming = 0; IIncoming < NumIncomingValues; ++IIncoming) {
      auto *IncomingV = OldPhi->getIncomingValue(IIncoming);
      auto *IncomingBB = OldPhi->getIncomingBlock(IIncoming);
      NewPhi->addIncoming(GetPaddedV(IncomingV, PaddedLength, InstToPadded),
                          IncomingBB);
    }
    // Remove dangling references to unpadded values in the Phi.
    OldPhi->removeIncomingValueIf([](unsigned X) { return true; }, false);
  }

  OldPhiToNewPhi.clear();
  InstToPadded.clear();
  InstsThatMustBePadded.clear();
  InstsThatMustBePaddedRPOT.clear();

  for (auto *I : llvm::reverse(InstructionsToRemove)) {
    invalidateRippleDataFor(I);
    LLVM_DEBUG(dbgs() << "[PadToTargetSIMD] Deleting " << *I << "\n";);
    if (!I->use_empty()) {
      dbgs() << *I << " has non empty uses.\n";
      for (auto *U : I->users()) {
        dbgs() << *U << "\n";
      }
    }
    assert(I->use_empty());
    I->eraseFromParent();
  }
  return;
}

BitVector Ripple::reductionTensorDimensions(const IntrinsicInst *I) const {
  // All we need are the dimensions of Input that are not in the result of the
  // reduction
  auto InputSet = getRippleShape(I->getArgOperand(1)).nonEmptyDims();
  auto ResultSet = getRippleShape(I).nonEmptyDims();
  for (auto SetOutput : ResultSet.set_bits()) {
    InputSet.reset(SetOutput);
  }
  return InputSet;
}

unsigned Ripple::lastVectorIdx(const IntrinsicInst *I,
                               const TensorShape &IShape,
                               const unsigned SpecialArgIdx,
                               const char *OpKind) const {
  assert(rippleReduceIntrinsics(I) || rippleSliceIntrinsic(I));

  // The API only allows reducing/broadcasting vector dimensions for now; find
  // the one being reduced/bcast automatically (only one vector PE can be used
  // for a given Instruction, this is checked by the ripple semantics checker)
  unsigned SetVectorId = tensorRank();
  for (unsigned Idx = 0, EndIdx = tensorRank(); Idx < EndIdx; ++Idx) {
    if (IShape[Idx] > 1 && isVectorId(tensorToRipple(Idx).first))
      SetVectorId = Idx;
  }

  if (SetVectorId == tensorRank()) {
    // No vector dimensions being reduced/broadcasted
    std::string ErrMsg;
    llvm::raw_string_ostream RSO(ErrMsg);
    RSO << "Ripple " << OpKind
        << " used on a tensor with empty vector dimensions: "
           "input is (";
    Value *TensorArg = I->getArgOperand(SpecialArgIdx);
    if (TensorArg->getName().empty())
      RSO << *TensorArg;
    else
      RSO << TensorArg->getName();
    RSO << ") with shape " << IShape;
    RSO.flush();
    DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(I),
                                   ErrMsg);
    F.getContext().diagnose(DI);
  }
  return SetVectorId;
}

Expected<TensorShape>
Ripple::computeRippleShapeForBitsetIntrinsic(const IntrinsicInst *I,
                                             const TensorShape &IShape) {
  assert(rippleReduceIntrinsics(I) || rippleBroadcastIntrinsic(I));

  bool IsReduction = rippleReduceIntrinsics(I);

  // When the machine model does not specify vector dimensions, the reduction or
  // broadcast is a no-op
  if ((IsReduction && IShape.isScalar()) || hasNoVectorDimension())
    return IShape;

  // Initialize an all false BitVector
  TensorShape OutputShape = IShape;

  unsigned BitSetArgIdx = IsReduction
                              ? &I->getArgOperandUse(0) - I->arg_begin()
                              : &I->getArgOperandUse(1) - I->arg_begin();

  // Guaranteed to be a constant because the intrinsics require an immediate
  auto BitsetArgValue = *getConstantOperandValue(I, BitSetArgIdx);

  IntrinsicInst *BcastBlockShape = nullptr;
  // Only vector dimensions are allowed
  unsigned SetVectorId = tensorRank();
  if (IsReduction)
    SetVectorId = lastVectorIdx(I, IShape, 1, "reduction");
  else {
    BcastBlockShape = getBlockShapeIntrinsic(I->getArgOperandUse(0));
    // Checked by checkBlockShapeUsage
    assert(BcastBlockShape);
    PEIdentifier PEId = *getConstantOperandValue(BcastBlockShape, 0);
    SetVectorId = rippleToTensor({PEId, 0});
  }
  if (SetVectorId >= tensorRank())
    return OutputShape;

  auto [PEBeingAffected, _] = tensorToRipple(SetVectorId);
  unsigned NumPEDimensions = PERank(PEBeingAffected);
  BitVector AffectedDimensions(tensorRank());
  for (uint64_t BitMask = 1, DimIdx = 0; BitMask != 0;
       BitMask = BitMask << 1, ++DimIdx) {
    // dimIdx ripple dimension is set
    if (BitsetArgValue & BitMask) {
      if (DimIdx < NumPEDimensions) {
        auto AffectedDim = rippleToTensor({PEBeingAffected, DimIdx});
        AffectedDimensions.set(AffectedDim);
      } else {
        auto printBinary = [](raw_ostream &OS, uint64_t value) {
          std::bitset<64> bits(value);
          std::string binaryString = bits.to_string();
          // Remove leading zeros
          binaryString.erase(0, binaryString.find_first_not_of('0'));
          OS << "0b" << binaryString;
        };

        // warn about an out of bounds reduction/broadcast dimension index
        std::string ErrMsg;
        llvm::raw_string_ostream RSO(ErrMsg);
        RSO << "Ripple " << (IsReduction ? "reduction" : "broadcast")
            << " applied on the dimension index (" << DimIdx
            << ") of processing element (PE number " << PEBeingAffected
            << ") that is out of bounds [0, " << NumPEDimensions
            << "[ (all set mask is ";
        uint64_t AllOneMaskForPE = (uint64_t(1) << NumPEDimensions) - 1;
        printBinary(RSO, AllOneMaskForPE);
        RSO << "); did you mean to use the mask (";
        printBinary(RSO, BitsetArgValue & AllOneMaskForPE);
        RSO << ")?";
        RSO.flush();
        DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(I),
                                       ErrMsg);
        F.getContext().diagnose(DI);
      }
    }
  }
  if (IsReduction)
    OutputShape.reduceDimensions(AffectedDimensions);
  else {
    // Broadcast
    TensorShape BlockShape = setShapeToTensorShape(BcastBlockShape);
    LLVM_DEBUG(dbgs() << "ripple.broadcast block is " << *BcastBlockShape
                      << " with " << BlockShape << "\n");
    BlockShape.keepDimensions(AffectedDimensions);
    LLVM_DEBUG(dbgs() << "Max after reduce " << BlockShape << "\n");
    // And then broadcast InputShape by what's left
    if (auto Err = OutputShape.combineShapeBcast(BlockShape))
      return std::move(Err);
  }

  return OutputShape;
}

Error Ripple::validate() const {
  const std::string ErrorPrefix = "Ripple validation error: ";
  for (auto &[DimId, DimRank] : PERanks) {
    // Check that the ID is known by the compiler
    if (idType(DimId) == UnknownDimType) {
      std::string errStr;
      raw_string_ostream oss(errStr);
      oss << ErrorPrefix << "the dimension identifier (" << DimId
          << ") is used in function " << F.getName()
          << " but has not been registered; it is either missing from the "
             "machine model or the input program is malformed.";
      oss.flush();
      return createStringError(inconvertibleErrorCode(), errStr);
    }
  }
  return Error::success();
}

void Ripple::printValidSeries(raw_ostream &OSS) const {
  for (auto &[_, LS] : LsCache.Valid) {
    OSS << *LS << "\n";
  }
}

void Ripple::printTensorInstructions(raw_ostream &oss) const {
  auto dimTypeStr = [&](DimType type) {
    switch (type) {
    case VectorDimension:
      return "Vector";
    case ThreadDimension:
      return "Thread";
    case CoreDimension:
      return "Core";
    case DeviceDimension:
      return "Device";
    default:
      return "Unknown";
    }
  };
  auto printDimTypes = [&]() {
    oss << "DimTypes{";
    for (unsigned i = 0; i < tensorRank(); i++) {
      if (i != 0)
        oss << ", ";
      oss << dimTypeStr(tensorDimType(i));
    }
    oss << "}\n";
  };
  for (auto &shapeIt : InstructionRippleShapes) {
    if (shapeIt.second.isVector()) {
      oss << "\tInstruction " << *shapeIt.first << "\n\t\t" << shapeIt.second
          << " ";
      printDimTypes();
    }
  }
}

void Ripple::genVectorInstructions() {
  /// State to store if any changes were done to the CFG that made
  /// this->FuncRPOT to be invalidated *after* generating vector instructions.
  bool isFuncRPOTValid = true;

  auto vectorizedName = [](Instruction *I) -> Twine {
    return Twine(I->getName()) + ".ripple.vectorized";
  };

  auto processFreeze = [&](FreezeInst *Freeze,
                           const TensorShape &ToShape) -> void {
    irBuilder.SetInsertPoint(Freeze);
    auto [FreezeVal, FreezeShape] = getTensorUse(Freeze->getOperandUse(0));
    assert(*FreezeShape == ToShape);
    auto *Freezed = irBuilder.CreateFreeze(
        FreezeVal, tensorizedName(Freeze->getName(), ToShape));
    setReplacementFor(Freeze, Freezed, ToShape);
  };

  // TODO: We should probably implement ValueMaterializer and use a ValueMapper
  // to remap the values instead of this function followed by a cleanup.
  auto processRippleBlock = [&](IntrinsicInst *rippleDim,
                                const TensorShape &toShape) -> void {
    switch (rippleDim->getIntrinsicID()) {
    case Intrinsic::ripple_block_index: {
      // The propagation is handled by Linear Series, here we replace the
      // instruction by the value zero.
      assert(getCachedSeries(rippleDim));
      Constant *c = ConstantInt::get(rippleDim->getType(), 0);
      setReplacementFor(rippleDim, c, toShape);
    } break;
    case Intrinsic::ripple_block_getsize: {
      // Replace ripple.block.getsize calls by a constant
      auto size = getRippleGetSizeValue(rippleDim);
      Constant *c = ConstantInt::get(rippleDim->getType(), size);
      setReplacementFor(rippleDim, c, toShape);
    } break;
    case Intrinsic::ripple_block_setshape: {
      // Erase this intrinsic
      setReplacementFor(rippleDim, nullptr, toShape);
    } break;
    }
  };

  auto processRippleBroadcast = [&](IntrinsicInst *RippleBcast,
                                    const TensorShape &ToShape) -> void {
    // Broadcast the first argument to RippleBcast's shape
    irBuilder.SetInsertPoint(RippleBcast);
    auto VectorizedArg =
        getTensorUseAndBcast(RippleBcast->getArgOperandUse(2), ToShape);
    setReplacementFor(RippleBcast, VectorizedArg, ToShape);
  };

  auto processRippleSlice = [&](IntrinsicInst *RippleSlice,
                                const TensorShape &ToShape) -> void {
    irBuilder.SetInsertPoint(RippleSlice);
    // Expected signature: T ripple_slice(T x, int64_t idx0, int64_t idx1, ...,
    // int64_t idx9); where idx0 ... idx9 are constants
    auto [Slicee, FromShape] = getTensorUse(RippleSlice->getArgOperandUse(0));
    // No slicing -> ripple_slice is the identity
    if (ToShape == *FromShape) {
      setReplacementFor(RippleSlice, Slicee, ToShape);
      return;
    }
    SmallVector<int64_t, RippleIntrinsicsMaxDims> SliceArgs;
    llvm::transform(
        make_range(&RippleSlice->getArgOperandUse(1),
                   &RippleSlice->getArgOperandUse(1 + FromShape->rank())),
        std::back_inserter(SliceArgs), [](Use &U) -> int64_t {
          ConstantInt *Arg = cast<ConstantInt>(U.get());
          return Arg->getSExtValue();
        });
    std::vector<int> ShuffleIndices;
    Value *SliceInst;
    if (ToShape.isScalar()) {
      SmallVector<size_t, RippleIntrinsicsMaxDims> SliceIndex;
      for (unsigned Idx = 0, n = SliceArgs.size(); Idx < n; ++Idx) {
        assert(SliceArgs[Idx] != -1);
        SliceIndex.push_back((size_t)SliceArgs[Idx]);
      }
      ShuffleIndices.push_back(FromShape->getOffsetAt(SliceIndex));
      SliceInst = irBuilder.CreateExtractElement(Slicee, ShuffleIndices[0],
                                                 "ripple.slice");
    } else {
      // Capture of struct binding (FromShape) is c++20, use a copy
      auto *FromShapePtr = FromShape;
      auto computeShuffleIndices =
          [&, FromShapePtr](ArrayRef<size_t> index) -> void {
        SmallVector<size_t, RippleIntrinsicsMaxDims> SliceIndex;
        for (unsigned Idx = 0, n = FromShapePtr->rank(); Idx < n; ++Idx) {
          int64_t SliceArg = SliceArgs[Idx];
          SliceIndex.push_back(SliceArg < 0 ? (size_t)index[Idx]
                                            : (size_t)SliceArg);
        }
        ShuffleIndices.push_back(FromShapePtr->getOffsetAt(SliceIndex));
      };
      ToShape.foreachIndex(computeShuffleIndices);
      SliceInst =
          irBuilder.CreateShuffleVector(Slicee, ShuffleIndices, "ripple.slice");
    }
    setReplacementFor(RippleSlice, SliceInst, ToShape);
  };

  auto processRippleReductions = [&](IntrinsicInst *rippleReduction,
                                     const TensorShape &toShape) -> void {
    auto rippleToVPReduce = [](Intrinsic::ID rippleReduction) -> Intrinsic::ID {
      switch (rippleReduction) {
      default:
        llvm_unreachable("Not a Ripple reduction instruction");
      case Intrinsic::ripple_reduce_add:
        return Intrinsic::vp_reduce_add;
      case Intrinsic::ripple_reduce_mul:
        return Intrinsic::vp_reduce_mul;
      case Intrinsic::ripple_reduce_and:
        return Intrinsic::vp_reduce_and;
      case Intrinsic::ripple_reduce_or:
        return Intrinsic::vp_reduce_or;
      case Intrinsic::ripple_reduce_xor:
        return Intrinsic::vp_reduce_xor;
      case Intrinsic::ripple_reduce_smax:
        return Intrinsic::vp_reduce_smax;
      case Intrinsic::ripple_reduce_smin:
        return Intrinsic::vp_reduce_smin;
      case Intrinsic::ripple_reduce_umax:
        return Intrinsic::vp_reduce_umax;
      case Intrinsic::ripple_reduce_umin:
        return Intrinsic::vp_reduce_umin;
      case Intrinsic::ripple_reduce_fadd:
        return Intrinsic::vp_reduce_fadd;
      case Intrinsic::ripple_reduce_fmul:
        return Intrinsic::vp_reduce_fmul;
      case Intrinsic::ripple_reduce_fmin:
        return Intrinsic::vp_reduce_fmin;
      case Intrinsic::ripple_reduce_fmax:
        return Intrinsic::vp_reduce_fmax;
      case Intrinsic::ripple_reduce_fminimum:
        return Intrinsic::vp_reduce_fminimum;
      case Intrinsic::ripple_reduce_fmaximum:
        return Intrinsic::vp_reduce_fmaximum;
      }
    };
    irBuilder.SetInsertPoint(rippleReduction);

    auto [RedValue, RedValueShape] =
        getTensorUse(rippleReduction->getArgOperandUse(1));
    auto ReductionShape = getRippleShape(rippleReduction);

    if (*RedValueShape == ReductionShape) {
      // The reduction has the same shape as the input value; nothing to reduce!
      setReplacementFor(rippleReduction, RedValue, *RedValueShape);
    } else {
      auto VPReductionID = rippleToVPReduce(rippleReduction->getIntrinsicID());
      BitVector ReductionDims = reductionTensorDimensions(rippleReduction);
      FastMathFlags FMFRed = isa<FPMathOperator>(rippleReduction)
                                 ? rippleReduction->getFastMathFlags()
                                 : FastMathFlags();

      // Ripple reductions allow reassoc
      FMFRed.setAllowReassoc();
      Value *ReductionResult = genMultiDimReduction(
          VPReductionID, RedValue, *RedValueShape, ReductionDims, FMFRed);
      setReplacementFor(rippleReduction, ReductionResult, ReductionShape);
    }
  };

  auto createVectorPHI = [&](PHINode *oldPhi,
                             const TensorShape &toShape) -> void {
    // We create a dummy vector PHI and will fix its arguments later, once we
    // all the vector values have been generated
    Type *newPhiType = VectorType::get(oldPhi->getType()->getScalarType(),
                                       toShape.flatShape(), /*scalable*/ false);
    irBuilder.SetInsertPoint(oldPhi);
    PHINode *newPhi = irBuilder.CreatePHI(
        newPhiType, oldPhi->getNumIncomingValues(), vectorizedName(oldPhi));
    setReplacementFor(oldPhi, newPhi, toShape);
  };

  auto processComparisons = [&](CmpInst *cmp,
                                const TensorShape &toShape) -> void {
    auto bcastOps = tensorizedOperandsAndBroadcast(cmp, toShape);
    assert(bcastOps.size() == 2);
    irBuilder.SetInsertPoint(cmp);
    Value *vectorCompare = irBuilder.CreateCmp(
        cmp->getPredicate(), bcastOps[0], bcastOps[1], vectorizedName(cmp));
    setReplacementFor(cmp, vectorCompare, toShape);
  };

  auto processSelects = [&](SelectInst *Select,
                            const TensorShape &toShape) -> void {
    auto BcastOps = tensorizedOperandsAndBroadcast(Select, toShape);
    irBuilder.SetInsertPoint(Select);
    Value *VecSelect = irBuilder.CreateSelect(
        BcastOps[0], BcastOps[1], BcastOps[2], vectorizedName(Select));
    setReplacementFor(Select, VecSelect, toShape);
  };

  auto processBinaryOps = [&](BinaryOperator *binOp,
                              const TensorShape &toShape) -> void {
    auto bcastedOperands = tensorizedOperandsAndBroadcast(binOp, toShape);
    assert(bcastedOperands.size() == 2);
    irBuilder.SetInsertPoint(binOp);
    IRBuilder<>::FastMathFlagGuard FMFGuard(irBuilder);
    FastMathFlags FMF = isa<FPMathOperator>(binOp) ? binOp->getFastMathFlags()
                                                   : FastMathFlags{};
    irBuilder.setFastMathFlags(FMF);
    Value *newBinop = irBuilder.CreateBinOp(
        binOp->getOpcode(), bcastedOperands[0], bcastedOperands[1],
        vectorizedName(binOp), binOp->getMetadata(LLVMContext::MD_fpmath));
    setReplacementFor(binOp, newBinop, toShape);
  };

  auto processCasts = [&](CastInst *castInst,
                          const TensorShape &toShape) -> void {
    Type *toType = VectorType::get(castInst->getType()->getScalarType(),
                                   toShape.flatShape(), /*scalable*/ false);
    auto [cachedVal, _] = getTensorUse(castInst->getOperandUse(0));

    assert(cachedVal && "Did not visit the predecessor of a vector cast");
    assert(isa<VectorType>(cachedVal->getType()) &&
           cast<VectorType>(cachedVal->getType())
                   ->getElementCount()
                   .getKnownMinValue() == toShape.flatShape());
    irBuilder.SetInsertPoint(castInst);
    Value *vectorCast = irBuilder.CreateCast(castInst->getOpcode(), cachedVal,
                                             toType, vectorizedName(castInst));
    setReplacementFor(castInst, vectorCast, toShape);
  };

  auto processGEP = [&](GetElementPtrInst *Gep,
                        const TensorShape &toShape) -> void {
    auto BcastedPtr = getTensorUseAndBcast(
        Gep->getOperandUse(Gep->getPointerOperandIndex()), toShape);

    unsigned FirstIndex = std::distance(Gep->op_begin(), Gep->idx_begin());
    auto BcastedIndices = tensorizedOperandsAndBroadcast(
        Gep, toShape, FirstIndex, FirstIndex + Gep->getNumIndices());

    irBuilder.SetInsertPoint(Gep);
    Value *vecGEP = irBuilder.CreateGEP(Gep->getSourceElementType(), BcastedPtr,
                                        BcastedIndices, vectorizedName(Gep),
                                        Gep->isInBounds());
    LLVM_DEBUG(dbgs() << "Created GEP " << *vecGEP << " of type "
                      << *vecGEP->getType() << " source type "
                      << *Gep->getSourceElementType() << " Ptr " << *BcastedPtr
                      << " indices: ";
               std::for_each(BcastedIndices.begin(), BcastedIndices.end(),
                             [](auto *V) { dbgs() << V << " "; });
               dbgs() << "\n");
    setReplacementFor(Gep, vecGEP, toShape);
  };

  auto processLoad = [&](LoadInst *Load, const TensorShape &ToShape) -> void {
    auto [LoadPtr, LoadShape] =
        getTensorUse(Load->getOperandUse(Load->getPointerOperandIndex()));

    ConstructedSeries PointerOpSeries;

    if (AllocaInst *ThisAlloca =
            aliasesWithPromotableAlloca(MemoryLocation::get(Load))) {
      // Loads of promotable Alloca have shape-shifting capabilities

      // getRippleShape(LoadPtr) returns the maximum size of the alloca
      // used in the function, however here we want this load's shape instead
      // since we broadcasted before the store(s) that clobbers this load
      PointerOpSeries = getSplatSeries(ThisAlloca, ToShape, ToShape);
    } else {
      PointerOpSeries =
          getCachedSeries(dyn_cast<Instruction>(Load->getPointerOperand()));
      if (!PointerOpSeries) {
        PointerOpSeries = getSplatSeries(LoadPtr, *LoadShape, ToShape);
        assert(PointerOpSeries && "A load pointer can always become a series");
      }
    }

    // Here we Load from a set of addresses defined by a Linear Series.
    // The result is a tensor whose shape is that of the Linear Series.
    LinearSeries *AddressSeries = PointerOpSeries.LS;
    LLVM_DEBUG(dbgs() << "Generating series load for " << *Load << ':'
                      << *AddressSeries << '\n');
    auto NewLoad =
        NdLoadStoreFac.genLoad(Load, *AddressSeries, LoadPtr, ToShape);

    setReplacementFor(Load, NewLoad, ToShape);
  };

  auto processStore = [&](StoreInst *Store,
                          const TensorShape &ToShape) -> void {
    auto [VectorPtr, VectorShape] =
        getTensorUse(Store->getOperandUse(Store->getPointerOperandIndex()));
    if (VectorPtr == Store->getPointerOperand())
      report_fatal_error("Missing vector replacement for vectorized store");

    LLVM_DEBUG(dbgs() << "Storing to " << *VectorPtr << "\n");

    // TODO: broadcasting and storing the broadcasted value is not the most
    //       efficient way to do a broadcast store.
    //       It's best to not duplicate the regs and just store multiple times.
    //       Incorporate the broadcast dimensions into the store attributes.
    auto *VectorValue = getTensorUseAndBcast(Store->getOperandUse(0), ToShape);

    AllocaInst *AllocaPtr = dyn_cast<AllocaInst>(VectorPtr);
    // Could be an alloca promotion!
    if (AllocaPtr && VectorShape) {
      // We can issue an aligned store to the alloca!
      irBuilder.SetInsertPoint(Store);
      Value *AllocaStore = irBuilder.CreateStore(VectorValue, AllocaPtr);
      setReplacementFor(Store, AllocaStore, ToShape);
      return;
    }

    if (!(VectorPtr->getType()->isVectorTy() &&
          VectorPtr->getType()->getScalarType()->isPointerTy()))
      report_fatal_error("Expected a vector of pointers for vector store");

    // Here we load from a set of addresses defined by a Linear Series.
    // The result is a tensor whose shape is that of the Linear Series.
    auto Series =
        getCachedSeries(cast<Instruction>(Store->getPointerOperand()));
    Value *NewStore;

    if (*VectorShape != ToShape) {
      auto Bcasted = tensorBcast(VectorPtr, *VectorShape, ToShape);
      if (!Bcasted)
        report_fatal_error("Broadcast failure during codegen");
      VectorPtr = *Bcasted;
    }

    if (!Series || !Series.isValid()) {
      NewStore = NdLoadStoreFac.genUnstructuredStore(Store, VectorValue,
                                                     VectorPtr, ToShape);
    } else {
      LinearSeries *AddressSeries = Series.LS;
      NewStore = NdLoadStoreFac.genStore(Store, VectorValue, *AddressSeries,
                                         VectorPtr, ToShape);
    }

    setReplacementFor(Store, NewStore, ToShape);
  };

  auto processRippleShuffles = [&](IntrinsicInst *rippleShuffle,
                                   const TensorShape &toShape) -> void {
    auto [VecToShuffle, ShuffleShape] =
        getTensorUse(rippleShuffle->getArgOperandUse(0));
    auto VecToShuffleWith =
        getTensorUseAndBcast(rippleShuffle->getArgOperandUse(1), toShape);

    bool IsPairShuffle =
        !cast<ConstantInt>(rippleShuffle->getArgOperand(2))->isZero();

    if (IsPairShuffle) {
      // Pair may require an extra broadcast
      auto Bcasted = tensorBcast(VecToShuffle, *ShuffleShape, toShape);
      // Checked during shape propagation
      if (!Bcasted)
        report_fatal_error("Broadcast failure during codegen");
      VecToShuffle = *Bcasted;
    }

    // For PairShuffle we need to "select" the LHS or RHS depending on the index
    // value (0 or 1)
    if (!IsPairShuffle) {
      if (toShape.isScalar() && !IsPairShuffle) {
        assert(ShuffleShape->isScalar() &&
               VecToShuffle == rippleShuffle->getArgOperand(0));

        setReplacementFor(rippleShuffle, VecToShuffle, toShape);
        return;
      }
      assert(VecToShuffle != rippleShuffle->getArgOperand(0));
    }

    // We made sure that this is a valid function earlier
    Function *ShuffleFunc = cast<Function>(rippleShuffle->getArgOperand(3));
    FunctionType *ShuffleFuncType = ShuffleFunc->getFunctionType();

    irBuilder.SetInsertPoint(rippleShuffle);
    DimSize BlockSize = toShape.flatShape();
    std::vector<int> PermIdxs(BlockSize, 0);

    Evaluator Evaler(DL, &targetLibraryInfo);

    for (DimSize IIdx = 0; IIdx < BlockSize; ++IIdx) {
      Constant *RetVal;
      Constant *IdxArg =
          ConstantInt::get(ShuffleFuncType->getParamType(0), IIdx);
      Constant *BlockSizeArg =
          ConstantInt::get(ShuffleFuncType->getParamType(1), BlockSize);
      SmallVector<Constant *, 2> Args = {IdxArg, BlockSizeArg};

      if (!Evaler.EvaluateFunction(ShuffleFunc, RetVal, Args)) {
        // This is checked by checkRippleShuffleIntrinsics
        report_fatal_error("Unexpected Evaler failure");
      }
      ConstantInt *RetIntVal = cast<ConstantInt>(RetVal);
      PermIdxs[IIdx] = RetIntVal->getSExtValue();
    }

    if (IsPairShuffle && toShape.isScalar()) {
      // This case is similar to a select
      assert(PermIdxs.size() == 1);
      setReplacementFor(rippleShuffle,
                        PermIdxs[0] == 0 ? VecToShuffle : VecToShuffleWith,
                        toShape);
      return;
    }

    Value *ShuffledVec = irBuilder.CreateShuffleVector(
        VecToShuffle,
        IsPairShuffle ? VecToShuffleWith
                      : PoisonValue::get(VecToShuffle->getType()),
        PermIdxs, vectorizedName(rippleShuffle));

    setReplacementFor(rippleShuffle, ShuffledVec, toShape);
  };

  auto ProcessIntrinsicCall = [&](CallInst *Call, const TensorShape &ToShape,
                                  Intrinsic::ID VectorIntrId) -> void {
    // Replace the Call with its corresponding vector intrinsic

    // Create a vector type with the target shape and broadcast the Call
    // arguments to match the shape
    auto [ArgumentShapes, ReturnType] =
        promotedIntrinsicArgShapesAndReturnTy(*Call, ToShape, VectorIntrId);

    SmallVector<Value *> BcastedArgs;
    BcastedArgs.reserve(Call->arg_size());
    for (size_t ArgNo = 0; ArgNo < Call->arg_size(); ++ArgNo) {
      if (ArgumentShapes[ArgNo]->isVector()) {
        auto ReplacementAndBcast =
            getTensorUseAndBcast(Call->getArgOperandUse(ArgNo), ToShape);
        BcastedArgs.push_back(ReplacementAndBcast);
      } else {
        BcastedArgs.push_back(Call->getArgOperand(ArgNo));
      }
    }

    irBuilder.SetInsertPoint(Call);
    // Create a vectorized Intrinsic with arguments that should be vectorized
    CallInst *VecCall = irBuilder.CreateIntrinsic(
        ReturnType, VectorIntrId, BcastedArgs,
        isa<FPMathOperator>(Call) ? Call : nullptr, vectorizedName(Call));

    setReplacementFor(Call, VecCall, ToShape);
  };

  auto replacebyExternalRippleFunctionCall =
      [&](ExternalRippleFunction &ExternF, CallInst *CallToReplace,
          Value *MaskForExternalFun, const TensorShape &ToShape) -> void {
    // We replace the call by a call to the external function
    SmallVector<Value *, 0> CallArguments;
    for (unsigned ArgIdx = 0, E = CallToReplace->arg_size(); ArgIdx < E;
         ++ArgIdx) {
      Use &OriginalArg = CallToReplace->getArgOperandUse(ArgIdx);
      auto [Replacement, ReplacementShape] = getTensorUse(OriginalArg);
      assert(*ReplacementShape == ExternF.getArgOperandShape(ArgIdx) &&
             "Expected matching Tensor shape to non-element wise "
             "external ripple functions");
      CallArguments.push_back(Replacement);
    }
    irBuilder.SetInsertPoint(CallToReplace);
    Value *ExternFunCall = genFunctionCallHandleArgAttributes(
        CallToReplace, ExternF, ToShape, CallArguments, MaskForExternalFun);
    setReplacementFor(CallToReplace, ExternFunCall, ToShape);
  };

  auto processElementWiseCall = [&](ExternalRippleFunction &ExternF,
                                    CallInst *Call, Value *MaskArgument,
                                    const TensorShape &ToShape) -> void {
    auto padWithZeros = [&](Value *Vector, uint64_t ToSize) -> Value * {
      VectorType *VecTy = cast<VectorType>(Vector->getType());
      uint64_t VecSize = VecTy->getElementCount().getKnownMinValue();
      SmallVector<int, 0> ShuffleMask;
      while (VecSize < ToSize) {
        // ShuffleVector can only double the size of a vector!
        uint64_t MaxShuffleSize = VecSize * 2;
        ShuffleMask.resize(std::min(MaxShuffleSize, ToSize));
        std::iota(ShuffleMask.begin(), ShuffleMask.end(), 0);
        Vector = irBuilder.CreateShuffleVector(
            Vector, ConstantAggregateZero::get(Vector->getType()), ShuffleMask);
        TensorShape::Shape Shape(tensorRank());
        Shape[0] = ShuffleMask.size();
        setRippleShape(Vector, TensorShape(std::move(Shape)));
        VecSize = ShuffleMask.size();
      }
      return Vector;
    };

    const auto &ExternElemWiseShape = ExternF.elementWiseShape();
    LLVM_DEBUG(dbgs() << "Extern call shape: " << ExternElemWiseShape << "\n");
    const unsigned ExternElemCount = ExternElemWiseShape.flatShape();
    auto TensorArgShapes = ExternF.returnTensorShape(Call, *this);
    // This is checked during the shape propagation phase, in
    // inferShapeFromOperands, CallInst case
    if (!TensorArgShapes)
      report_fatal_error("Broadcast failure during codegen");
    assert(ExternF.returnsVoid() ||
           (!ExternF.returnsVoid() && *TensorArgShapes == ToShape));
    irBuilder.SetInsertPoint(Call);
    // We broadcast all tensors to the largest tensor shape between the operands
    // and return
    SmallVector<Value *, 0> TensorArgs;
    for (unsigned ArgIdx = 0, E = Call->arg_size(); ArgIdx < E; ++ArgIdx) {
      auto &ArgUse = Call->getArgOperandUse(ArgIdx);
      if (ExternF.getArgOperandShape(ArgIdx).isScalar()) {
        assert(getRippleShape(ArgUse).isScalar());
        TensorArgs.push_back(ArgUse);
      } else {
        auto BcastOp = getTensorUseAndBcast(ArgUse, *TensorArgShapes);
        // We need to pad to ceil(ExternElemCount / 2) for shuffle in the slice
        // loop to succeed!
        auto *Padded = padWithZeros(BcastOp, (ExternElemCount + 1) / 2);
        TensorArgs.push_back(Padded);
      }
    }
    // We place the mask as last argument so we can slice it w/ the operand
    // tensors
    if (MaskArgument)
      TensorArgs.push_back(MaskArgument);

    // TODO: when Call calls isa<Function> having some byval arguments we need
    // to load the vectors before slicing (the shape of byval is scalar, it's a
    // pointer, although it's a tensor under disguise). This is a low priority
    // task since when compiling Ripple functions, these call have not yet been
    // modified to match the target ABI, hence we shouldn't see byval at this
    // compilation stage.
    // This is checked in checkRippleExternCall
    if (Function *CalledF = Call->getCalledFunction())
      if (any_of(CalledF->args(),
                 [](Argument &Arg) { return Arg.hasByValAttr(); }))
        report_fatal_error(
            "Unimplemented slicing of element-wise byval arguments");

    SmallVector<int, 0> ShuffleIndices(ExternElemCount);
    SmallVector<Value *, 0> SlicedCallArgs;
    SmallVector<Value *, 0> ExternCallReturnSlices;
    for (unsigned SliceIdx = 0,
                  E = divideCeil(TensorArgShapes->flatShape(), ExternElemCount);
         SliceIdx < E; ++SliceIdx) {
      SlicedCallArgs.clear();
      for (unsigned ArgIdx = 0, E = TensorArgs.size(); ArgIdx < E; ++ArgIdx) {
        if (ExternF.getArgOperandShape(ArgIdx).isScalar()) {
          SlicedCallArgs.push_back(TensorArgs[ArgIdx]);
        } else {
          std::iota(ShuffleIndices.begin(), ShuffleIndices.end(),
                    SliceIdx * ExternElemCount);
          assert(ShuffleIndices.size() == ExternElemCount);
          LLVM_DEBUG(dbgs() << "Shuffling " << *TensorArgs[ArgIdx]
                            << " Mask length: " << ExternElemCount << "\n");
          Value *Shuffle = irBuilder.CreateShuffleVector(
              TensorArgs[ArgIdx],
              ConstantAggregateZero::get(TensorArgs[ArgIdx]->getType()),
              ShuffleIndices);
          setRippleShape(Shuffle, ExternElemWiseShape);
          SlicedCallArgs.push_back(Shuffle);
        }
      }
      // Extract the sliced mask
      Value *SlicedMask = nullptr;
      if (MaskArgument) {
        SlicedMask = SlicedCallArgs.back();
        SlicedCallArgs.pop_back();
      }
      Value *ResultVal = genFunctionCallHandleArgAttributes(
          Call, ExternF, ToShape, SlicedCallArgs, SlicedMask);
      setRippleShape(ResultVal, ExternElemWiseShape);

      if (!ExternF.returnsVoid())
        ExternCallReturnSlices.push_back(ResultVal);
    }
    uint64_t NumSlices = ExternCallReturnSlices.size();
    if (NumSlices == 0) {
      assert(ExternF.returnsVoid());
      // We generated the replacement(s), mark this instruction for removal
      setReplacementFor(Call, nullptr, ToShape);
      return;
    }

    if (NumSlices >= 2) {
      // We merge consecutive slices until we are left with the fully
      // reconstructed tensor
      uint64_t Pow2CeilSlicesForMerge =
          isPowerOf2_64(NumSlices) ? NumSlices : NextPowerOf2(NumSlices);
      unsigned MergeSteps = Log2_64(Pow2CeilSlicesForMerge);
      LLVM_DEBUG(dbgs() << "Merging " << NumSlices << " slices using "
                        << MergeSteps << " steps\n");

      // Merge the Result slices in log2 shuffle steps
      for (uint64_t Step = 0, E = MergeSteps, ShuffleRhsDist = 1,
                    ShuffleSliceSize = ExternElemCount * 2;
           Step < E; ++Step, ShuffleRhsDist *= 2, ShuffleSliceSize *= 2) {
        TensorShape::Shape Shape(tensorRank());
        Shape[0] = ShuffleSliceSize;
        TensorShape SliceShape(std::move(Shape));
        ShuffleIndices.resize(ShuffleSliceSize);
        std::iota(ShuffleIndices.begin(), ShuffleIndices.end(), 0);
        // Iterate over the indices of vectors to be shuffled
        uint64_t NextPairStride = ShuffleRhsDist * 2;
        for (uint64_t LhsIdx = 0, RhsIdx = ShuffleRhsDist; LhsIdx < NumSlices;
             LhsIdx += NextPairStride, RhsIdx += NextPairStride) {
          Value *Lhs = ExternCallReturnSlices[LhsIdx];
          Value *Rhs = RhsIdx >= NumSlices ? PoisonValue::get(Lhs->getType())
                                           : ExternCallReturnSlices[RhsIdx];
          ExternCallReturnSlices[LhsIdx] =
              irBuilder.CreateShuffleVector(Lhs, Rhs, ShuffleIndices);
          setRippleShape(ExternCallReturnSlices[LhsIdx], SliceShape);
        }
      }
    }
    Value *ReturnVal = ExternCallReturnSlices[0];
    if (ToShape.flatShape() < cast<VectorType>(ReturnVal->getType())
                                  ->getElementCount()
                                  .getKnownMinValue()) {
      // We have to extract the padding values
      ShuffleIndices.resize(ToShape.flatShape());
      std::iota(ShuffleIndices.begin(), ShuffleIndices.end(), 0);
      ReturnVal = irBuilder.CreateShuffleVector(
          ReturnVal, ShuffleIndices, "ripple.extern_call.remove_padding");
    }
    setReplacementFor(Call, ReturnVal, ToShape);
  };

  auto ProcessGeneralFunctionCall = [&](CallInst *Call,
                                        const TensorShape &ToShape) -> void {
    LLVM_DEBUG(dbgs() << "\nFunction before ProcessGeneralFunctionCall:\n\n";
               F.print(dbgs(), nullptr); dbgs() << "\n");
    LLVM_DEBUG(dbgs() << "Sequentializing this Call " << *Call << "\n");
    // Sequentialize the Call by extracting each element of the vector operands,
    // running the scalar function on them, and insert the scalar result into
    // the output vector.
    // The Hexagon backend does not deal well with Extract/InsertElement
    bool UseBuffers = Triple(F.getParent()->getTargetTriple()).isHexagon();

    // The Index type for GEP of the Alloca buffers
    Type *AllocaIndexType =
        DL.getIndexType(F.getContext(), DL.getAllocaAddrSpace());

    // Allocate temporary buffers for each argument
    // Store broadcasted arguments into the buffer
    SmallVector<std::pair<Value *, bool>, 4> ScalarArgOrBuffers;
    for (auto &Arg : Call->args()) {
      if (getRippleShape(Arg).isScalar()) {
        // We don't need to broadcast
        ScalarArgOrBuffers.push_back({Arg, /*IsBufferized*/ false});
      } else {
        // Create a temporary buffer to store the vector
        auto BcastedVal = getTensorUseAndBcast(Arg, ToShape);
        if (!UseBuffers) {
          ScalarArgOrBuffers.push_back({BcastedVal, /*IsBufferized*/ false});
        } else {
          Type *ScalarTy = Arg->getType()->getScalarType();
          Type *BufferTy = ArrayType::get(ScalarTy, ToShape.flatShape());
          irBuilder.SetInsertPoint(F.getEntryBlock().getFirstInsertionPt());
          AllocaInst *ArgBuffer = irBuilder.CreateAlloca(BufferTy);
          // Get the alignment right so that we can do align vector load/store
          ArgBuffer->setAlignment(std::max(
              ArgBuffer->getAlign(),
              DL.getPrefTypeAlign(VectorType::get(ScalarTy, ToShape.flatShape(),
                                                  /*Scalable*/ false))));
          setRippleShape(ArgBuffer, ScalarShape);
          ScalarArgOrBuffers.push_back({ArgBuffer, /*IsBufferized*/ true});
          irBuilder.SetInsertPoint(Call);
          Value *StrPtr = irBuilder.CreateStore(BcastedVal, ArgBuffer);
          setRippleShape(StrPtr, ToShape);
        }
      }
    }

    // Setup loop blocks
    BasicBlock *BeforeLoop = Call->getParent();
    BasicBlock *LoopPreHeader =
        SplitBlock(Call->getParent(), Call->getIterator(), &DTU, nullptr,
                   nullptr, Twine(Call->getName()) + ".ripple.call.loop.pre");
    setRippleShape(BeforeLoop->getTerminator(), ScalarShape);
    assert(Call->getParent() == LoopPreHeader);
    BasicBlock *LoopHeader =
        SplitBlock(LoopPreHeader, Call->getIterator(), &DTU, nullptr, nullptr,
                   Twine(Call->getName()) + ".ripple.call.loop.header");
    setRippleShape(LoopPreHeader->getTerminator(), ScalarShape);
    assert(Call->getParent() == LoopHeader);
    BasicBlock *LoopBody =
        SplitBlock(LoopHeader, Call->getIterator(), &DTU, nullptr, nullptr,
                   Twine(Call->getName()) + ".ripple.call.loop.body");
    setRippleShape(LoopHeader->getTerminator(), ScalarShape);
    assert(Call->getParent() == LoopBody);
    BasicBlock *CallBlock = SplitBlock(
        LoopBody, Call->getIterator(), &DTU, nullptr, nullptr,
        Twine(Call->getName()) + ".ripple.call.loop.body.call.block");
    setRippleShape(LoopBody->getTerminator(), ScalarShape);
    assert(Call->getParent() == CallBlock);

    BasicBlock *ContinueBlock = SplitBlock(
        CallBlock, std::next(Call->getIterator()), &DTU, nullptr, nullptr,
        Twine(Call->getName()) + ".ripple.call.loop.body.continue.block");
    setRippleShape(CallBlock->getTerminator(), ScalarShape);

    BasicBlock *AfterLoop =
        SplitBlock(ContinueBlock, ContinueBlock->begin(), &DTU, nullptr,
                   nullptr, Twine(Call->getName()) + ".ripple.call.loop.end");
    setRippleShape(ContinueBlock->getTerminator(), ScalarShape);

    cast<BranchInst>(ContinueBlock->getTerminator())
        ->setSuccessor(0, LoopHeader);
    DTU.applyUpdates({{DominatorTree::Delete, ContinueBlock, AfterLoop},
                      {DominatorTree::Insert, ContinueBlock, LoopHeader}});

    // Create induction variable
    irBuilder.SetInsertPoint(LoopHeader->begin());
    irBuilder.SetInstDebugLocation(Call);
    PHINode *InductionVar =
        irBuilder.CreatePHI(AllocaIndexType, 2, "ripple.scalarcall.iterator");
    setRippleShape(InductionVar, ScalarShape);
    InductionVar->addIncoming(ConstantInt::get(AllocaIndexType, 0),
                              LoopPreHeader);
    VectorType *MaskVectorType = VectorType::get(
        irBuilder.getInt8Ty(), ElementCount::getFixed(ToShape.flatShape()));

    // Increment induction variable
    irBuilder.SetInsertPoint(ContinueBlock->getTerminator());
    irBuilder.SetInstDebugLocation(Call);
    Value *IncrementedInductionVar =
        irBuilder.CreateAdd(InductionVar, ConstantInt::get(AllocaIndexType, 1));
    setRippleShape(IncrementedInductionVar, ScalarShape);
    InductionVar->addIncoming(IncrementedInductionVar, ContinueBlock);

    // Initialize function call mask with as select(True, True, False)
    // which ensures getting a value of 1 for active vector
    // lanes and 0 for the rest after IfConvert is applied
    irBuilder.SetInsertPoint(LoopHeader->getTerminator());
    SelectInst *MaskSelect = irBuilder.Insert(
        createMaskSelectToTrueFalse(irBuilder.getInt8Ty(),
                                    MaskVectorType->getElementCount()),
        "zeroinit.select");
    setRippleShape(MaskSelect, ToShape);
    // Enable masking for this select
    SelectToMaskWhenIfConvert.insert(MaskSelect);

    Value *MaskVal;
    irBuilder.SetInsertPoint(LoopBody->getTerminator());
    MaskVal = irBuilder.CreateExtractElement(MaskSelect, InductionVar);
    setRippleShape(MaskVal, ScalarShape);

    // Add mask condition in LoopBody
    irBuilder.SetInsertPoint(LoopBody->getTerminator());
    irBuilder.SetInstDebugLocation(Call);
    Value *MaskCond = irBuilder.CreateICmpEQ(
        MaskVal, ConstantInt::get(irBuilder.getInt8Ty(), 1));
    setRippleShape(MaskCond, ScalarShape);
    Value *BranchInst =
        irBuilder.CreateCondBr(MaskCond, CallBlock, ContinueBlock);
    setRippleShape(BranchInst, ScalarShape);
    invalidateRippleDataFor(LoopBody->getTerminator());
    LoopBody->getTerminator()->eraseFromParent();

    // Create loop condition
    irBuilder.SetInsertPoint(LoopHeader->getTerminator());
    irBuilder.SetInstDebugLocation(Call);
    Value *LoopCond = irBuilder.CreateICmpULT(
        InductionVar, ConstantInt::get(AllocaIndexType, ToShape.flatShape()));
    setRippleShape(LoopCond, ScalarShape);

    // Create loop branch
    irBuilder.SetInsertPoint(LoopHeader);
    irBuilder.SetInstDebugLocation(Call);
    Instruction *OldTerminator = LoopHeader->getTerminator();
    Value *LoopBranchInst =
        irBuilder.CreateCondBr(LoopCond, LoopBody, AfterLoop);
    setRippleShape(LoopBranchInst, ScalarShape);
    invalidateRippleDataFor(OldTerminator);
    OldTerminator->eraseFromParent();
    DTU.applyUpdates({{DominatorTree::Insert, LoopHeader, AfterLoop}});
    DTU.flush();

    // nullptr if function returns void else contains a buffer (AllocaInst) when
    // UseBuffers or a PHINode when !UseBuffers
    Value *ResValue = nullptr;
    Type *ResBuffTy = nullptr;

    Type *VectorCallType = Call->getType();
    if (!VectorCallType->isVoidTy()) {
      assert(VectorType::isValidElementType(VectorCallType));
      VectorCallType = VectorType::get(Call->getType(), ToShape.flatShape(),
                                       /*Scalable*/ false);
      if (UseBuffers) {
        // Allocate a buffer for the return values
        ResBuffTy = ArrayType::get(Call->getType(), ToShape.flatShape());
        irBuilder.SetInsertPoint(F.getEntryBlock().getFirstInsertionPt());
        AllocaInst *AllocaRes = irBuilder.CreateAlloca(ResBuffTy);
        setRippleShape(AllocaRes, ScalarShape);
        AllocaRes->setAlignment(std::max(AllocaRes->getAlign(),
                                         DL.getPrefTypeAlign(VectorCallType)));
        ResValue = AllocaRes;
      } else {
        // When in register mode, generate a PHI init with Poison from the
        // LoopPreHeader and the InsertElement from LoopBody
        irBuilder.SetInsertPoint(LoopHeader->begin());
        irBuilder.SetInstDebugLocation(Call);
        PHINode *ResultValue = irBuilder.CreatePHI(VectorCallType, 2u);
        setRippleShape(ResultValue, ToShape);
        ResultValue->addIncoming(PoisonValue::get(VectorCallType),
                                 LoopPreHeader);
        ResValue = ResultValue;
      }
    }

    // Create the scalar call inside the loop body w/ the extracted/buffer
    // values
    irBuilder.SetInsertPoint(CallBlock->getFirstNonPHIIt());

    SmallVector<Value *, 4> ScalarArgs;
    for (auto &[Arg, IsBufferized] : ScalarArgOrBuffers) {
      if (IsBufferized) {
        AllocaInst *Alloca = cast<AllocaInst>(Arg);
        Type *BufferTy = Alloca->getAllocatedType();
        Type *ScalarTy = BufferTy->getArrayElementType();
        Value *ArgPtr = irBuilder.CreateGEP(
            BufferTy, Arg,
            {ConstantInt::get(AllocaIndexType, 0), InductionVar});
        setRippleShape(ArgPtr, ScalarShape);
        Value *LdPtr = irBuilder.CreateLoad(ScalarTy, ArgPtr);
        setRippleShape(LdPtr, ScalarShape);
        ScalarArgs.push_back(LdPtr);
      } else {
        if (Arg->getType()->isVectorTy()) {
          assert(!UseBuffers);
          Value *Extracted = irBuilder.CreateExtractElement(Arg, InductionVar);
          setRippleShape(Extracted, ScalarShape);
          ScalarArgs.push_back(Extracted);
        } else
          ScalarArgs.push_back(Arg);
      }
    }
    // Function call proper
    Value *ScalarCall = irBuilder.CreateCall(
        Call->getFunctionType(), Call->getCalledOperand(), ScalarArgs);
    setRippleShape(ScalarCall, ScalarShape);
    // If the function returns a value
    if (ResValue) {
      if (UseBuffers) {
        // Store the returned scalar value into the buffer
        Value *ResultPtr = irBuilder.CreateGEP(
            ResBuffTy, ResValue,
            {ConstantInt::get(AllocaIndexType, 0), InductionVar});
        setRippleShape(ResultPtr, ScalarShape);
        Value *StrPtr = irBuilder.CreateStore(ScalarCall, ResultPtr);
        setRippleShape(StrPtr, ScalarShape);
        // Create a vector load in AfterLoop
        irBuilder.SetInsertPoint(AfterLoop->getFirstInsertionPt());
        irBuilder.SetInstDebugLocation(Call);
        Value *ResultLoad = irBuilder.CreateLoad(VectorCallType, ResValue);
        setReplacementFor(Call, ResultLoad, ToShape);
      } else {
        PHINode *ResultPhi = cast<PHINode>(ResValue);
        // Insert the element into the PHINode
        Value *Inserted =
            irBuilder.CreateInsertElement(ResultPhi, ScalarCall, InductionVar);
        setRippleShape(Inserted, ToShape);

        irBuilder.SetInsertPoint(ContinueBlock->begin());
        irBuilder.SetInstDebugLocation(Call);
        PHINode *UpdatedResPhi = irBuilder.CreatePHI(VectorCallType, 2u);
        setRippleShape(UpdatedResPhi, ToShape);
        UpdatedResPhi->addIncoming(Inserted, CallBlock);
        UpdatedResPhi->addIncoming(ResultPhi, LoopBody);
        // Which is the value coming back from the LoopBody
        ResultPhi->addIncoming(UpdatedResPhi, ContinueBlock);
        setReplacementFor(Call, ResultPhi, ToShape);
      }
    } else {
      setReplacementFor(Call, nullptr, ToShape);
    }
    LLVM_DEBUG(dbgs() << "\nFunction after ProcessGeneralFunctionCall:\n\n";
               F.print(dbgs(), nullptr); dbgs() << "\n");

    // CFG has been changed => FuncRPOT is no longer valid.
    isFuncRPOTValid = false;
  };

  auto processSpecializedFunctionCall =
      [&](CallInst *Call, Function *SpecializedFunction,
          const TensorShape &ToShape, bool IsMaskedCall) {
        irBuilder.SetInsertPoint(Call);
        SmallVector<Value *, 8> Arguments;
        llvm::transform(Call->args(), std::back_inserter(Arguments),
                        [&](Use &U) { return getTensorUse(U).first; });
        if (IsMaskedCall) {
          Function *MaskedSpecialization =
              getMaskedSpecialization(*SpecializedFunction);
          if (!MaskedSpecialization)
            llvm_unreachable("We should always create a specialization and "
                             "its masked companion at the same time");

          // If the masked is not being leveraged by the specialization, we can
          // fallback to the non-masked version
          if (MaskedSpecialization->getArg(MaskedSpecialization->arg_size() - 1)
                  ->getNumUses()) {
            auto MaskShape = getSpecializationMaskShape(Call->args());
            // We already checked that the specialization can be created during
            // shape propagation. We must succeed here!
            if (!MaskShape)
              llvm_unreachable(
                  "Ripple specialization cannot broadcast mask in codegen!");
            SelectInst *MaskArgument = irBuilder.Insert(
                createMaskSelectToTrueFalse(
                    getSpecializationMaskElementType(),
                    ElementCount::getFixed(MaskShape->flatShape())),
                "ripple.specialization.mask");
            setRippleShape(MaskArgument, MaskShape.get());
            // Enable masking for this select
            SelectToMaskWhenIfConvert.insert(MaskArgument);

            // Call the masked specialization with the extra argument instead!
            SpecializedFunction = MaskedSpecialization;
            Arguments.push_back(MaskArgument);
          }
        }
        auto *SpecializedCall =
            irBuilder.CreateCall(SpecializedFunction, Arguments);
        SpecializedCall->setCallingConv(CallingConv::Fast);
        setReplacementFor(Call, SpecializedCall, ToShape);
      };

  auto processCallInst = [&](CallInst *call,
                             const TensorShape &toShape) -> void {
    // Check if the call matches any specific ripple intrinsic
    if (IntrinsicInst *rippleDim = rippleBlockIntrinsics(call)) {
      processRippleBlock(rippleDim, toShape);
    } else if (IntrinsicInst *RippleBroadcast =
                   rippleBroadcastIntrinsic(call)) {
      processRippleBroadcast(RippleBroadcast, toShape);
    } else if (IntrinsicInst *rippleReduction = rippleReduceIntrinsics(call)) {
      processRippleReductions(rippleReduction, toShape);
    } else if (IntrinsicInst *rippleSlice = rippleSliceIntrinsic(call)) {
      processRippleSlice(rippleSlice, toShape);
    } else if (IntrinsicInst *rippleShuffle = rippleShuffleIntrinsics(call)) {
      processRippleShuffles(rippleShuffle, toShape);
      // ToShape can be scalar because of reductions/slicing
    } else if (toShape.isVector() || rippleVectorizeCall(*call)) {
      Intrinsic::ID vectorIntrId =
          getVectorIntrinsicIDForCall(call, &targetLibraryInfo);
      bool IsMaskedCall = MaskedCalls.contains(call);
      if (auto *ExternFun = findExternalRippleFunctionFor(call, IsMaskedCall)) {
        SelectInst *MaskArgument = nullptr;
        if (IsMaskedCall) {
          assert(ExternFun->isMaskable() &&
                 "Cannot mask a non-maskable external ripple function");
          if (Argument *MaskArg = ExternFun->getTensorMaskArgument()) {
            // If the external ripple function has a mask argument, insert
            // a dummy select to be masked by the if-conversion process
            auto *MaskType = ExternalRippleFunction::getTrueMaskType(MaskArg);
            // Checked when ExternalRippleFunction is created
            assert(MaskType->isVectorTy() &&
                   MaskType->getScalarType()->isIntegerTy() &&
                   "Expecting a mask type as vector of integers");
            VectorType *MaskVecTy = cast<VectorType>(MaskType);
            IntegerType *MaskIntTy =
                cast<IntegerType>(MaskVecTy->getElementType());

            auto ExternFunMaskShape = ExternFun->maskShape(call, *this);
            // Checked by checkVectorBranch
            if (!ExternFunMaskShape)
              report_fatal_error("Operands and return tensor shapes cannot be "
                                 "broadcasted to mask shape");
            irBuilder.SetInsertPoint(call);
            MaskArgument = irBuilder.Insert(
                createMaskSelectToTrueFalse(
                    MaskIntTy,
                    ElementCount::getFixed(ExternFunMaskShape->flatShape())),
                "ripple.extern.mask");
            setRippleShape(MaskArgument, *ExternFunMaskShape);

            // Enable masking for this select
            SelectToMaskWhenIfConvert.insert(MaskArgument);
          }
        }
        if (!ExternFun->isElementWiseFunction()) {
          replacebyExternalRippleFunctionCall(*ExternFun, call, MaskArgument,
                                              toShape);
        } else {
          processElementWiseCall(*ExternFun, call, MaskArgument, toShape);
        }
      } else if (toShape.isVector() &&
                 vectorIntrId != Intrinsic::not_intrinsic) {
        // We assume vectorizing an intrinsic call does not
        // introduce approximations
        ProcessIntrinsicCall(call, toShape, vectorIntrId);
      } else {
        auto [SpecializedFunction, ReturnShape] =
            getRippleSpecializationFor(*call);
        if (SpecializedFunction) {
          assert(getRippleShape(call) == ReturnShape);
          assert(!isPendingRippleSpecialization(*SpecializedFunction));
          processSpecializedFunctionCall(call, SpecializedFunction, ReturnShape,
                                         IsMaskedCall);
        } else {
          assert(toShape.isVector() &&
                 "ripple general function call has broadcast "
                 "semantics and cannot return a scalar");
          // Sequential execution for non-intrinsic function calls or floating
          // point intrinsic-calls without fast math
          ProcessGeneralFunctionCall(call, toShape);
        }
      }
    }
  };

  auto processFneg = [&](UnaryInstruction *fneg,
                         const TensorShape &toShape) -> void {
    auto bcastOps = tensorizedOperandsAndBroadcast(fneg, toShape);
    irBuilder.SetInsertPoint(fneg);
    Value *newFneg =
        irBuilder.CreateFNegFMF(bcastOps[0], fneg, vectorizedName(fneg));
    setReplacementFor(fneg, newFneg, toShape);
  };

  auto processAlloca = [&](AllocaInst *Alloca,
                           const TensorShape &ToShape) -> void {
    irBuilder.SetInsertPoint(Alloca);
    Type *AllocatedTy = Alloca->getAllocatedType();
    assert(!AllocatedTy->isVectorTy() && "Ripple cannot promote vector types");
    AllocaInst *AllocaVector = irBuilder.CreateAlloca(
        ArrayType::get(AllocatedTy, ToShape.flatShape()),
        /*ArraySize*/ nullptr, tensorizedName(Alloca->getName(), ToShape));
    AllocaVector->setAlignment(std::max(
        AllocaVector->getAlign(),
        DL.getPrefTypeAlign(VectorType::get(AllocatedTy, ToShape.flatShape(),
                                            /*Scalable*/ false))));
    setReplacementFor(Alloca, AllocaVector, ToShape);
  };

  auto processReturn = [&](ReturnInst *Return,
                           const TensorShape &ToShape) -> void {
    // We only process return that have a value when we specialize
    assert(isPendingRippleSpecialization(F));
    assert(Return->getNumOperands() != 0);
    irBuilder.SetInsertPoint(Return);
    auto *ReturnOperandBcasted =
        getTensorUseAndBcast(Return->getOperandUse(0), ToShape);
    auto *VectorReturn = irBuilder.CreateRet(ReturnOperandBcasted);
    setReplacementFor(Return, VectorReturn, ToShape);
  };

  for (auto *BB : getFuncRPOT()) {
    // Gather the instructions in the BB to vectorize
    SmallVector<Instruction *, 16> toProcess;
    for (auto &I : *BB) {
      // For LinearSeries we need to process a vector base
      auto CS = getCachedSeries(&I);
      auto IShape = getRippleShape(&I);

      bool ProcessLinSeries = CS && CS.LS->getBaseShape().isVector();
      bool ProcessNonSeries = !CS && IShape.isVector();

      // Process instructions that should be vectors, but not linear series
      // Call instructions are special cases because they can be external ripple
      // functions/specializations/ripple intrinsic with a scalar shape!
      if (ProcessNonSeries || ProcessLinSeries || isa<CallInst>(&I)) {
        // For PHI nodes, we generate empty vector PHIs and fix them at the end
        if (PHINode *phi = dyn_cast<PHINode>(&I)) {
          createVectorPHI(phi, CS ? CS.LS->getBaseShape() : IShape);
        } else {
          toProcess.push_back(&I);
        }
      }
    }
    // Generate the vectorized instructions
    for (auto *I : toProcess) {
      auto CS = getCachedSeries(I);
      const TensorShape &toShape =
          CS ? CS.LS->getBaseShape() : getRippleShape(I);
      // TODO: Specialize for the different instruction kinds (InstrTypes.h +
      // Instructions.h), e.g., UnaryOp, BinaryOp, Phi, etc.
      LLVM_DEBUG(dbgs() << "Vectorizing instruction " << *I << " to shape "
                        << toShape << "\n");

      if (auto *binOp = dyn_cast<BinaryOperator>(I))
        processBinaryOps(binOp, toShape);
      else if (auto *castInst = dyn_cast<CastInst>(I))
        processCasts(castInst, toShape);
      else if (auto *gep = dyn_cast<GetElementPtrInst>(I))
        processGEP(gep, toShape);
      else if (auto *Load = dyn_cast<LoadInst>(I))
        processLoad(Load, toShape);
      else if (auto *Store = dyn_cast<StoreInst>(I))
        processStore(Store, toShape);
      else {
        switch (I->getOpcode()) {
        case Instruction::Call:
          processCallInst(cast<CallInst>(I), toShape);
          break;
        case Instruction::ICmp:
        case Instruction::FCmp:
          processComparisons(cast<CmpInst>(I), toShape);
          break;
        case Instruction::FNeg:
          processFneg(cast<UnaryInstruction>(I), toShape);
          break;
        case Instruction::Select:
          processSelects(cast<SelectInst>(I), toShape);
          break;
        case Instruction::Br: {
          // Branches are handled by the if-conversion function
          BranchInst *Branch = cast<BranchInst>(I);
          assert(Branch->isConditional());
          auto [VectorCondVal, ConditionShape] =
              getTensorUse(Branch->getOperandUse(0));
          assert(*ConditionShape == toShape);
          BranchAndSwitchVecCond[Branch] = VectorCondVal;
        } break;
        case Instruction::Switch: {
          SwitchInst *Switch = cast<SwitchInst>(I);
          auto [VectorCondVal, ConditionShape] =
              getTensorUse(Switch->getOperandUse(0));
          assert(*ConditionShape == toShape);
          BranchAndSwitchVecCond[Switch] = VectorCondVal;
        } break;
        case Instruction::Ret: {
          if (isPendingRippleSpecialization(F))
            processReturn(cast<ReturnInst>(I), toShape);
          else
            // Checked by checkRippleFunctionReturn
            report_fatal_error("Unsupported return instruction vectorization");
        } break;
        case Instruction::Alloca:
          processAlloca(cast<AllocaInst>(I), toShape);
          break;
        case Instruction::Freeze:
          processFreeze(cast<FreezeInst>(I), toShape);
          break;
        default: {
          std::string ErrMsg;
          llvm::raw_string_ostream RSO(ErrMsg);
          RSO << "instruction type not known by the ripple vectorizer; please "
                 "fill up a support request to the Ripple team: "
              << *I;
          RSO.flush();
          DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(I),
                                         ErrMsg);
          F.getContext().diagnose(DI);
          // Don't crash here to catch more potential issues; the error is
          // diagnosed already
          if (I->getType()->isVoidTy()) {
            setReplacementFor(I, nullptr, toShape);
          } else {
            auto Shape = getRippleShape(I);
            Value *Replacement = PoisonValue::get(
                VectorType::get(I->getType()->getScalarType(),
                                toShape.flatShape(), /*scalable*/ false));
            setReplacementFor(I, Replacement, toShape);
          }
        } break;
        }
      }
    }
  }
  // Fill the PHIs now that all the instructions are generated
  for (auto &[From, To] : InstructionReplacementMapping) {
    if (PHINode *OldPhi = dyn_cast<PHINode>(&*From)) {
      PHINode *NewPhi = cast<PHINode>(&*To);
      LLVM_DEBUG(dbgs() << "Fixing generated PHI: " << *NewPhi
                        << " with value generated from " << *OldPhi << "\n");
      auto PhiOperands =
          tensorizedOperandsAndBroadcast(OldPhi, getRippleShape(NewPhi));
      for (unsigned PhiIdx = 0; PhiIdx < OldPhi->getNumIncomingValues();
           ++PhiIdx) {
        NewPhi->addIncoming(PhiOperands[PhiIdx],
                            OldPhi->getIncomingBlock(PhiIdx));
      }
    }
  }

  if (!isFuncRPOTValid) {
    // We modified the control flow so FuncRPOT is not valid anymore.
    // Note that using the FuncRPOT during the replacement of scalar values
    // with vector values is sound as "isFuncRPOTValid" is supposed to store
    // whether the FuncRPOT is invalid *after* the replacement.
    invalidateFuncRPOT();
  }
}

SmallVector<Value *, 8>
Ripple::tensorizedOperandsAndBroadcast(Instruction *I,
                                       const TensorShape &ToShape,
                                       unsigned StartIdx, unsigned EndIdx) {
  SmallVector<Value *, 8> BcastedOperands;
  for (unsigned OpIdx = StartIdx, E = std::min(I->getNumOperands(), EndIdx);
       OpIdx < E; ++OpIdx) {
    auto ReplacementAndBcast =
        getTensorUseAndBcast(I->getOperandUse(OpIdx), ToShape);
    BcastedOperands.push_back(ReplacementAndBcast);
  }
  return BcastedOperands;
}

Value *Ripple::getTensorUseAndBcast(const Use &U, const TensorShape &ToShape) {
  auto [Replacement, ReplacementShape] = getTensorUse(U);
  auto Bcasted = tensorBcast(Replacement, *ReplacementShape, ToShape);
  // Checked during shape propagation
  if (!Bcasted)
    report_fatal_error("Broadcast failure during codegen");
  return *Bcasted;
}

Expected<Value *> Ripple::tensorBcast(Value *V, const TensorShape &FromShape,
                                      const TensorShape &ToShape) {

  auto broadcastConstantTensor =
      [&](Constant *C, const TensorShape &fromShape,
          const TensorShape &toShape) -> Expected<Value *> {
    if (!C->getType()->isVectorTy() && toShape.isScalar())
      return C;
    if (auto *cstInt = dyn_cast<ConstantInt>(C)) {
      auto &val = cstInt->getValue();
      Type *baseTy = cstInt->getType()->getScalarType();
      Type *vectorTy =
          VectorType::get(baseTy, toShape.flatShape(), /*scalable*/ false);
      return ConstantInt::get(vectorTy, val);
    } else if (auto *cstFP = dyn_cast<ConstantFP>(C)) {
      Type *baseTy = cstFP->getType()->getScalarType();
      Type *vectorTy =
          VectorType::get(baseTy, toShape.flatShape(), /*scalable*/ false);
      auto &val = cstFP->getValue();
      return ConstantFP::get(vectorTy, val);
    } else if (C->isNullValue()) {
      // isNullValue() handles ConstantPointerNull, ConstantAggregateZero,
      // ConstantTokenNone, ConstantTargetNone
      Type *baseTy = C->getType()->getScalarType();
      if (baseTy->isVectorTy())
        baseTy = cast<VectorType>(baseTy)->getElementType();
      Type *vectorTy =
          VectorType::get(baseTy, toShape.flatShape(), /*scalable*/ false);
      return ConstantAggregateZero::get(vectorTy);
    } else if (isa<UndefValue>(C)) {
      Type *BaseTy = C->getType()->getScalarType();
      if (BaseTy->isVectorTy())
        BaseTy = cast<VectorType>(BaseTy)->getElementType();
      Type *VectorTy =
          VectorType::get(BaseTy, toShape.flatShape(), /*scalable*/ false);
      return UndefValue::get(VectorTy);
    } else if (auto *SplatVal =
                   C->getType()->isVectorTy() ? C->getSplatValue() : nullptr) {
      return ConstantVector::getSplat(
          ElementCount::getFixed(toShape.flatShape()), SplatVal);
    } else if (auto *cstDataVector = dyn_cast<ConstantDataVector>(C)) {
      std::vector<int> shuffleMask;
      auto addOffsetToMask = [&](ArrayRef<size_t> index) {
        size_t offset = fromShape.getOffsetAt(index);
        shuffleMask.push_back(offset);
      };
      toShape.foreachIndex(addOffsetToMask);
      return ConstantExpr::getShuffleVector(
          cstDataVector, PoisonValue::get(cstDataVector->getType()),
          shuffleMask);
    } else if (auto *CstVector = dyn_cast<ConstantVector>(C)) {
      std::vector<Constant *> BcastedCstVectorVals(toShape.flatShape());

      auto BuildBcastedVector =
          [&](ArrayRef<TensorShape::DimSize> ToMultiIndex) {
            SmallVector<TensorShape::DimSize> FromMultiIndex(fromShape.rank(),
                                                             0);
            int IDim = 0;
            for (auto FromAxisLen : fromShape) {
              FromMultiIndex[IDim] =
                  (FromAxisLen == 1) ? 0 : ToMultiIndex[IDim];
              IDim++;
            }
            BcastedCstVectorVals[toShape.getOffsetAt(ToMultiIndex)] =
                CstVector->getOperand(fromShape.getOffsetAt(FromMultiIndex));
          };

      toShape.foreachIndex(BuildBcastedVector);
      return ConstantVector::get(BcastedCstVectorVals);
    } else if (isa<GlobalValue>(C)) {
      // That's a global constant address splat
      // TODO: there might be issues with global value's semantics (Linkage,
      // thread_local, etc) and Ripple's semantics
      return ConstantVector::getSplat(
          ElementCount::getFixed(toShape.flatShape()), C);
    } else {
      // We don't support vectorization of ConstantStruct, ConstantArray,
      // ConstantDataArray, ConstantTargetNone, ConstantTokenNone
      std::string ErrMsg;
      llvm::raw_string_ostream RSO(ErrMsg);
      RSO << "ripple does not know how to broadcast the value " << *C
          << " (only ConstantInt, ConstantFP, ConstantPointerNull, "
             "ConstantAggregateZero, ConstantTokenNone, ConstantTargetNone, "
             "Splats, ConstantDataVector and ConstantVector are supported)";
      RSO.flush();
      DiagnosticInfoRippleWithLoc DI(DS_Error, F, {}, ErrMsg);
      F.getContext().diagnose(DI);
      return createStringError(std::errc::invalid_argument,
                               "Unsupported Constant type being broadcasted");
    }
  };

  if (FromShape == ToShape) {
    LLVM_DEBUG(dbgs() << "Reusing " << *V << " w/ shape " << ToShape << "\n");
    return V;
  }

  if (Error e = FromShape.isBroadcastError(ToShape)) {
    std::string ErrMsg;
    llvm::raw_string_ostream RSO(ErrMsg);
    RSO << "ripple cannot broadcast the value " << *V << " from shape "
        << FromShape << " to shape " << ToShape;
    llvm::handleAllErrors(std::move(e), [&](const StringError &SE) {
      RSO << ": " << SE.getMessage();
    });
    RSO.flush();
    DebugLoc DL = isa<Instruction>(V)
                      ? sanitizeRippleLocation(cast<Instruction>(V))
                      : DebugLoc();
    DiagnosticInfoRippleWithLoc DI(DS_Error, F, DL, ErrMsg);
    F.getContext().diagnose(DI);
    return createStringError(std::errc::invalid_argument,
                             "Broadcast shapes non compatible");
  }

  // TODO: We can probably keep a cache of broadcasted values
  LLVM_DEBUG(dbgs() << "Broadcasting " << *V << " from shape " << FromShape
                    << " to shape " << ToShape << "\n");

  if (auto *C = dyn_cast<Constant>(V)) {
    return broadcastConstantTensor(C, FromShape, ToShape);
  } else if (isa<Instruction>(V) || isa<Argument>(V)) {
    auto SaveIP = irBuilder.saveIP();
    if (isa<Argument>(V)) {
      auto InsertAt = F.getEntryBlock().getFirstNonPHIOrDbgOrAlloca();
      irBuilder.SetInsertPoint(InsertAt);
    } else {
      auto *I = cast<Instruction>(V);
      if (auto *Invoke = dyn_cast<InvokeInst>(I)) {
        // Invokeinstruction is special because it is the only terminator that
        // can have users. So we must set the insertion point in a different
        // basic block.
        irBuilder.SetInsertPoint(
            Invoke->getNormalDest()->getFirstInsertionPt());
      } else {
        setInsertPointAfter(irBuilder, I);
      }
      if (I->getStableDebugLoc())
        irBuilder.SetCurrentDebugLocation(I->getStableDebugLoc());
    }

    Value *BcastedValue = nullptr;
    if (FromShape.isScalar()) {
      // Simple vector splat
      BcastedValue = irBuilder.CreateVectorSplat(
          ToShape.flatShape(), V, Twine(V->getName()) + ".ripple.bcast");
      // A Splat is an insert followed by a shuffle
      if (ShuffleVectorInst *ShuffleInst =
              dyn_cast<ShuffleVectorInst>(BcastedValue))
        setRippleShape(ShuffleInst->getOperand(0), ToShape);
    } else {
      // To construct the shuffle mask we map each index from toShape to the
      // offset in fromShape.
      std::vector<int> shuffleMask;
      auto addOffsetToMask = [&](ArrayRef<size_t> index) {
        size_t offset = FromShape.getOffsetAt(index);
        shuffleMask.push_back(offset);
      };
      ToShape.foreachIndex(addOffsetToMask);
      assert(shuffleMask.size() == ToShape.flatShape());
      BcastedValue = irBuilder.CreateShuffleVector(
          V, shuffleMask, Twine(V->getName()) + ".ripple.bcast");
    }
    setRippleShape(BcastedValue, ToShape);
    irBuilder.restoreIP(SaveIP);
    return BcastedValue;
  } else {
    std::string ErrMsg;
    llvm::raw_string_ostream RSO(ErrMsg);
    RSO << "ripple does not know how to broadcast the value " << *V
        << " (only Instructions, Arguments, and Constants are supported)";
    RSO.flush();
    DebugLoc DL = isa<Instruction>(V)
                      ? sanitizeRippleLocation(cast<Instruction>(V))
                      : DebugLoc();
    DiagnosticInfoRippleWithLoc DI(DS_Error, F, DL, ErrMsg);
    F.getContext().diagnose(DI);
    return createStringError(std::errc::invalid_argument,
                             "Unsupported Value type being broadcasted");
  }
}

DILocalVariable *Ripple::createVectorLocalFromScalarLocal(
    DIBuilder &DIB, DILocalVariable *LocalVariable, const TensorShape &Shape) {
  unsigned VectorLanes = Shape.flatShape();
  DIType *VariableType = LocalVariable->getType();
  if (!VariableType)
    return nullptr;

  // The subrange is used by the debugger to know the number of
  // elements of the vector
  DISubrange *Sr = DIB.getOrCreateSubrange(/*start*/ 0, /*size*/ VectorLanes);

  // We construct the vector type from the scalar debug type
  DICompositeType *VectorTy = DIB.createVectorType(
      VectorLanes * VariableType->getSizeInBits(),
      VariableType->getAlignInBits(), VariableType, DIB.getOrCreateArray({Sr}));

  LLVM_DEBUG(dbgs() << "Built composite vector type " << *VectorTy
                    << " with vector size " << VectorLanes << " subrange "
                    << *Sr << "\n");

  // Create a new LocalVariable with the correct type
  DILocalVariable *VectorLocal = DIB.createAutoVariable(
      LocalVariable->getScope(),
      tensorizedName(LocalVariable->getName(), Shape), LocalVariable->getFile(),
      LocalVariable->getLine(), VectorTy, true, LocalVariable->getFlags(),
      LocalVariable->getAlignInBits());

  LLVM_DEBUG(dbgs() << "Built local vector variable " << *VectorLocal << "\n");
  return VectorLocal;
}

void Ripple::vectorGenerationPostProcess() {
  // Fixup debug info
  DISubprogram *SP = F.getSubprogram();
  DICompileUnit *CU = SP ? SP->getUnit() : nullptr;
  if (CU) {
    DIBuilder DIB(*F.getParent(), /*AllowUnresolved*/ false, CU);
    for (auto &BB : F) {
      for (auto &I : make_early_inc_range(BB)) {
        // Process Debug records
        for (DbgRecord &DbgR : make_early_inc_range(I.getDbgRecordRange())) {
          if (DbgVariableRecord *DVR = dyn_cast<DbgVariableRecord>(&DbgR)) {
            processDebugIntrinsicOrRecord(DIB, *DVR);
          }
        }
        // Support old style Debug intrinsics
        if (DbgVariableIntrinsic *DVI = dyn_cast<DbgVariableIntrinsic>(&I)) {
          processDebugIntrinsicOrRecord(DIB, *DVI);
        }
      }
    }
    DIB.finalize();
  }

  // LS that have not been replaced have the shape of the base, and since we
  // replace all bases w/o a ScalarShape, they are scalar!
  for (auto &[I, LS] : LsCache.Valid) {
    if (!InstructionReplacementMapping.contains(
            const_cast<Instruction *>(&*I))) {
      assert(LS->getBaseShape().isScalar());
      setRippleShape(I, LS->getBaseShape());
    }
  }
  // We don't need Linear Series after vector codegen
  clearLinearSeriesCache();

  // Check that the users of a replaced instruction are also being vectorized
  // (modulo if-converted instruction)
  for (auto &[I, Replacement] : InstructionReplacementMapping) {
    if (Replacement && Replacement->getType() != I->getType()) {
      for (auto *User : I->users()) {
        if (Instruction *UserInst = dyn_cast<Instruction>(User))
          if (!InstructionReplacementMapping.contains(UserInst) &&
              !(isa<BranchInst>(UserInst) || isa<SwitchInst>(UserInst))) {
            LLVM_DEBUG(dbgs()
                       << "Instruction " << *I << " has a non-vectorized user "
                       << *User << "\n");
            llvm_unreachable(
                "A vectorized instruction has a non-vectorized user");
          }
      }
    }
  }

  // We replace all values that need replacement
  for (auto &[I, Replacement] : InstructionReplacementMapping) {
    invalidateRippleDataFor(I);
    // Some replacement can be in place
    auto It = I->getIterator();
    if (Replacement && Replacement->getType() == I->getType()) {
      ReplaceInstWithValue(It, Replacement);
    } else {
      ReplaceInstWithValue(It, PoisonValue::get(I->getType()));
    }
  }
  InstructionReplacementMapping.clear();

  if (isPendingRippleSpecialization(F)) {
    // Function pending ripple specialization have twice the number of arguments
    // to allow processing by the ripple pass. We replace all use of the
    // remaining scalar original arguments (first half) by the ripple arguments
    // (second half)

    for (unsigned ArgIdx = 0, E = F.arg_size() / 2; ArgIdx < E; ++ArgIdx) {
      auto &Arg = *F.getArg(ArgIdx);
      // If the arg is still used
      if (Arg.use_begin() != Arg.use_end()) {
        assert(getRippleShape(&Arg).isScalar());
        Arg.replaceAllUsesWith(F.getArg(E + Arg.getArgNo()));
      }
    }
  }
}

bool Ripple::maskInstructionWhenIfConvert(const Instruction *I) const {
  return isa<LoadInst>(I) || isa<StoreInst>(I) ||
         intrinsicWithId(I,
                         {Intrinsic::masked_load, Intrinsic::masked_store,
                          Intrinsic::masked_gather, Intrinsic::masked_scatter,
                          Intrinsic::masked_expandload,
                          Intrinsic::masked_compressstore}) ||
         isa<VPIntrinsic>(I) || isa<SelectInst>(I);
}

void Ripple::applyMaskToOps(ArrayRef<BasicBlock *> BBs, Value *VectorMask,
                            const TensorShape &MaskShape,
                            Instruction *MaskInsertionPoint) {
  return applyMaskToOps(make_range(BBs.begin(), BBs.end()), VectorMask,
                        MaskShape, MaskInsertionPoint);
}

template <typename IteratorT>
void Ripple::applyMaskToOps(iterator_range<IteratorT> BBs, Value *VectorMask,
                            const TensorShape &MaskShape,
                            Instruction *MaskInsertionPoint) {
  assert(VectorMask->getType()->isVectorTy());

  // We create a dummy (non-maskable) instruction that will not be processed by
  // this function if the current mask insertion point will be processed.
  bool RemoveInsertionPoint = false;
  if (maskInstructionWhenIfConvert(MaskInsertionPoint) &&
      any_of(BBs, [MaskInsertionPoint](BasicBlock *BB) {
        return BB == MaskInsertionPoint->getParent();
      })) {
    irBuilder.SetInsertPoint(MaskInsertionPoint);
    auto *NewMaskInsertPoint = irBuilder.Insert(BinaryOperator::Create(
        Instruction::Add, PoisonValue::get(irBuilder.getInt8Ty()),
        PoisonValue::get(irBuilder.getInt8Ty())));
    MaskInsertionPoint = NewMaskInsertPoint;
    RemoveInsertionPoint = true;
  }

  // A cache of the same mask, with different shapes
  std::map<TensorShape, Value *> MaskWithDifferentShapes = {
      {MaskShape, VectorMask}};
  auto getMaskWithShape =
      [&](const TensorShape &RequestedMaskShape) -> Value * {
    auto CachedMaskIt = MaskWithDifferentShapes.find(RequestedMaskShape);
    if (CachedMaskIt != MaskWithDifferentShapes.end())
      return CachedMaskIt->second;
    irBuilder.SetInsertPoint(MaskInsertionPoint);
    auto ReshapedMask =
        reduceBcastMaskToShape(VectorMask, MaskShape, RequestedMaskShape);
    MaskWithDifferentShapes.insert({RequestedMaskShape, ReshapedMask});
    return ReshapedMask;
  };

  // Apply the mask to the maskable operators
  LLVM_DEBUG(dbgs() << "Mask to apply: " << *VectorMask << " with shape "
                    << MaskShape << " insert at " << *MaskInsertionPoint
                    << "\n");
  std::queue<BasicBlock *> WorkList;
  for (BasicBlock *ClonedBB : BBs)
    WorkList.push(ClonedBB);

  while (!WorkList.empty()) {
    BasicBlock *BBToProcess = WorkList.front();
    WorkList.pop();
    for (auto &I : make_early_inc_range(*BBToProcess)) {

      if (!maskInstructionWhenIfConvert(&I))
        continue;
      if (ToSkipMaskingWhenIfConvert.contains(&I)) {
        LLVM_DEBUG(dbgs() << "Skip masking for instruction: " << I << "\n");
        continue;
      }

      const TensorShape &InstructionShape = getRippleShape(&I);
      auto MaskToApply = getMaskWithShape(InstructionShape);

      LLVM_DEBUG(dbgs() << "Applying mask " << *MaskToApply << " with shape "
                        << MaskShape << " to " << I << "\n");

      irBuilder.SetInsertPoint(&I);
      if (LoadInst *Load = dyn_cast<LoadInst>(&I)) {
        // Transform into a masked load

        AllocaInst *AllocaPtr = dyn_cast<AllocaInst>(Load->getPointerOperand());
        if (!Load->getType()->isVectorTy()) {
          // For scalars we have to introduce a branch to load only when the
          // mask is true.
          //
          // Interpret the following as: BasicBlock | Instructions
          //
          // BB | ...              BB | ...
          //    | x = load   ->       | br mask Y N
          //    | rest
          //                        Y | x = load
          //                          | br N
          //
          //                        N | PHI (Y -> x), (BB -> Poison)
          //                          | rest
          assert(!MaskToApply->getType()->isVectorTy());
          // We move everything after the load in another BB
          BasicBlock *AfterLoadBB = SplitBlock(
              BBToProcess, std::next(Load->getIterator()), &DTU, nullptr,
              nullptr,
              Twine(BBToProcess->getName()) + ".ripple.after.masked.load");
          // We move the load in it's own BB
          BasicBlock *LoadBB =
              SplitBlock(BBToProcess, I.getIterator(), &DTU, nullptr, nullptr,
                         Twine(I.getName()) + ".ripple.branch.mask.apply");
          setRippleShape(LoadBB->getTerminator(), ScalarShape);

          // We replace the unconditional branch by a conditional one
          Instruction *Term = BBToProcess->getTerminator();
          irBuilder.SetInsertPoint(BBToProcess);
          irBuilder.SetCurrentDebugLocation(Load->getDebugLoc());
          BranchInst *Br =
              irBuilder.CreateCondBr(MaskToApply, LoadBB, AfterLoadBB);
          setRippleShape(Br, ScalarShape);
          invalidateRippleDataFor(Term);
          Term->eraseFromParent();

          // And we insert a PHI in AfterLoadBB
          irBuilder.SetInsertPoint(AfterLoadBB->begin());
          irBuilder.SetCurrentDebugLocation(Load->getDebugLoc());
          PHINode *LoadPHI = irBuilder.CreatePHI(Load->getType(), 2,
                                                 Twine(Load->getName()) +
                                                     ".ripple.masked.load");
          setRippleShape(LoadPHI, ScalarShape);
          Load->replaceAllUsesWith(LoadPHI);
          LoadPHI->addIncoming(Load, LoadBB);
          LoadPHI->addIncoming(PoisonValue::get(Load->getType()), BBToProcess);

          // The current DTU state after the two SplitBlocks is "BBToProcess ->
          // LoadBB -> AfterLoadBB". Since we change the direct branch into a
          // conditional branch we insert the new link BBToProcess ->
          // AfterLoadBB.
          DTU.applyUpdates({DominatorTree::UpdateType(
              DominatorTree::Insert, BBToProcess, AfterLoadBB)});

          // We are done with this BasicBlock but still need to process what
          // comes after the load
          WorkList.push(AfterLoadBB);
          break;
        } else if (AllocaPtr && getRippleShape(AllocaPtr).isVector()) {
          // We generate a Select(Mask, Load, Poison) to match a MaskedLoad
          // semantics. This is mostly useful for debugging (and possibly an
          // optimization hint).
          // TODO: Check if this helps or hinders optimizations
          setInsertPointAfter(irBuilder, Load);
          irBuilder.SetCurrentDebugLocation(Load->getDebugLoc());
          Value *SelectedLoad = irBuilder.CreateSelect(
              MaskToApply, Load, PoisonValue::get(Load->getType()));
          setRippleShape(SelectedLoad, InstructionShape);
          Load->replaceUsesWithIf(SelectedLoad, [=](Use &U) {
            return U.getUser() != SelectedLoad;
          });
        } else {
          Value *MaskedLoad = irBuilder.CreateMaskedLoad(
              Load->getType(), Load->getPointerOperand(), Load->getAlign(),
              MaskToApply, nullptr, Twine(I.getName()) + ".ripple.masked.load");
          setRippleShape(MaskedLoad, InstructionShape);
          Load->replaceAllUsesWith(MaskedLoad);
          invalidateRippleDataFor(Load);
          Load->eraseFromParent();
        }
      } else if (IntrinsicInst *MaskedLoad =
                     intrinsicWithId(&I, {Intrinsic::masked_load})) {
        // Add the branch mask to the existing one
        Value *OldMask = MaskedLoad->getArgOperand(1);
        Value *NewMask = irBuilder.CreateAnd(OldMask, MaskToApply,
                                             Twine(OldMask->getName()) +
                                                 ".ripple.branch.mask.apply");
        setRippleShape(NewMask, InstructionShape);
        MaskedLoad->setArgOperand(1, NewMask);
      } else if (IntrinsicInst *MaskedGather =
                     intrinsicWithId(&I, {Intrinsic::masked_gather})) {
        // Add the branch mask to the existing one
        Value *OldMask = MaskedGather->getArgOperand(1);
        Value *NewMask = irBuilder.CreateAnd(OldMask, MaskToApply,
                                             Twine(OldMask->getName()) +
                                                 ".ripple.branch.mask.apply");
        setRippleShape(NewMask, InstructionShape);
        MaskedGather->setArgOperand(1, NewMask);
      } else if (IntrinsicInst *ExpandLoad =
                     intrinsicWithId(&I, {Intrinsic::masked_expandload})) {
        Value *OldMask = ExpandLoad->getArgOperand(1);
        Value *NewMask = irBuilder.CreateAnd(OldMask, MaskToApply,
                                             "ripple.branch.mask.apply");
        setRippleShape(NewMask, InstructionShape);
        ExpandLoad->setArgOperand(1, NewMask);
      } else if (StoreInst *Store = dyn_cast<StoreInst>(&I)) {
        // Transform into a masked store
        if (!Store->getValueOperand()->getType()->isVectorTy()) {
          // For scalar store, we move it to its own basic block and branch if
          // the mask is true.
          //
          // Interpret the following as: BasicBlock | Instructions
          //
          // BB | ...              BB | ...
          //    | store      ->       | br mask Y N
          //    | rest
          //                        Y | store
          //                          | br N
          //
          //                        N | rest
          assert(!MaskToApply->getType()->isVectorTy());

          // We move everything after the store in another BB
          BasicBlock *AfterStoreBB = SplitBlock(
              BBToProcess, std::next(Store->getIterator()), &DTU, nullptr,
              nullptr,
              Twine(BBToProcess->getName()) + "ripple.after.masked.store");
          // We move the store in it's own BB
          BasicBlock *StoreBB = SplitBlock(
              BBToProcess, Store->getIterator(), &DTU, nullptr, nullptr,
              Twine(I.getName()) + ".ripple.branch.mask.apply");
          setRippleShape(StoreBB->getTerminator(), ScalarShape);

          // We replace the unconditional branch by a conditional one and we
          // are done!
          Instruction *Term = BBToProcess->getTerminator();
          irBuilder.SetInsertPoint(BBToProcess);
          irBuilder.SetCurrentDebugLocation(Store->getDebugLoc());
          BranchInst *Br =
              irBuilder.CreateCondBr(MaskToApply, StoreBB, AfterStoreBB);
          setRippleShape(Br, ScalarShape);
          invalidateRippleDataFor(Term);
          Term->eraseFromParent();

          // The current DTU state after the two SplitBlocks is "BBToProcess ->
          // StoreBB -> AfterStoreBB". Since we change the direct branch into a
          // conditional branch we insert the new link BBToProcess ->
          // AfterStoreBB.
          DTU.applyUpdates({DominatorTree::UpdateType(
              DominatorTree::Insert, BBToProcess, AfterStoreBB)});

          // We are done with this BasicBlock but still need to process what
          // comes after the store
          WorkList.push(AfterStoreBB);
          break;
        } else if (AllocaInst *AllocaPtr =
                       dyn_cast<AllocaInst>(Store->getPointerOperand())) {
          // This could be an aligned masked store depending on the alloca
          Instruction *MaskedStore = irBuilder.CreateMaskedStore(
              Store->getValueOperand(), Store->getPointerOperand(),
              AllocaPtr->getAlign(), MaskToApply);
          setRippleShape(MaskedStore, InstructionShape);
          invalidateRippleDataFor(Store);
          Store->eraseFromParent();
        } else {
          Instruction *MaskedStoreI = irBuilder.CreateMaskedStore(
              Store->getValueOperand(), Store->getPointerOperand(),
              Store->getAlign(), MaskToApply);
          setRippleShape(MaskedStoreI, InstructionShape);
          invalidateRippleDataFor(Store);
          Store->eraseFromParent();
        }
      } else if (IntrinsicInst *MaskedStore =
                     intrinsicWithId(&I, {Intrinsic::masked_store})) {
        // Add the branch mask to the existing one
        Value *OldMask = MaskedStore->getArgOperand(2);
        Value *NewMask = irBuilder.CreateAnd(OldMask, MaskToApply,
                                             Twine(OldMask->getName()) +
                                                 ".ripple.branch.mask.apply");
        setRippleShape(NewMask, InstructionShape);
        MaskedStore->setArgOperand(2, NewMask);
      } else if (IntrinsicInst *MaskedScatter =
                     intrinsicWithId(&I, {Intrinsic::masked_scatter})) {
        // Add the branch mask to the existing one
        Value *OldMask = MaskedScatter->getArgOperand(2);
        Value *NewMask = irBuilder.CreateAnd(OldMask, MaskToApply,
                                             Twine(OldMask->getName()) +
                                                 ".ripple.branch.mask.apply");
        setRippleShape(NewMask, InstructionShape);
        MaskedScatter->setArgOperand(2, NewMask);
      } else if (IntrinsicInst *CompressStore =
                     intrinsicWithId(&I, {Intrinsic::masked_compressstore})) {
        Value *OldMask = CompressStore->getArgOperand(2);
        Value *NewMask = irBuilder.CreateAnd(OldMask, MaskToApply,
                                             "ripple.branch.mask.apply");
        setRippleShape(NewMask, InstructionShape);
        CompressStore->setArgOperand(2, NewMask);
      } else if (SelectInst *Select = dyn_cast<SelectInst>(&I)) {
        if (SelectToMaskWhenIfConvert.contains(Select)) {
          LLVM_DEBUG(dbgs() << "Masking a special select: " << *Select << "\n");
          Value *OldMask = Select->getCondition();
          Value *NewMask = irBuilder.CreateAnd(OldMask, MaskToApply,
                                               ".ripple.branch.mask.apply");
          setRippleShape(NewMask, InstructionShape);
          Select->setCondition(NewMask);
        }
      } else if (VPIntrinsic *VPIntr = dyn_cast<VPIntrinsic>(&I)) {
        // Some VP intrinsics have no mask
        if (Value *OldMask = VPIntr->getMaskParam()) {
          Value *NewMask = irBuilder.CreateAnd(OldMask, MaskToApply,
                                               ".ripple.branch.mask.apply");
          setRippleShape(NewMask, InstructionShape);
          VPIntr->setMaskParam(NewMask);
        }
      }
    }
  }
  if (RemoveInsertionPoint)
    MaskInsertionPoint->eraseFromParent();
}

void Ripple::ifConvert() {
  auto makeUniquePredToPostDom = [&](DenseSet<BasicBlock *> &ToClone,
                                     BasicBlock *BranchBlock,
                                     BasicBlock *PostDom) -> BasicBlock * {
    // If there are multiple edges coming into postDom that can come from the
    // basic blocks we are cloning or the branch block, we insert an extra basic
    // block so that there is only one edge going into postDom
    SmallVector<BasicBlock *, 8> PredsToSplit;
    std::copy_if(pred_begin(PostDom), pred_end(PostDom),
                 std::back_inserter(PredsToSplit),
                 [&](auto BB) { return ToClone.contains(BB); });
    assert(PredsToSplit.size() > 0 &&
           "We should always at least be splitting one predecessor to PostDom");
    BasicBlock *Split = SplitBlockPredecessors(PostDom, PredsToSplit,
                                               ".ripple.predsplit", &DTU);
    // This should not happen because we are testing that the sub-graph is to
    // clone is a SESE region
    assert(Split != nullptr && "Failed to split for if-conversion");
    assert(&*Split->getFirstNonPHIIt() == Split->getTerminator() &&
           "Only expected PHI and a branch from the split");
    assert(Split->getUniqueSuccessor() == PostDom &&
           "Split should have a unique successor");
    setRippleShape(Split->getTerminator(), ScalarShape);
    // Since we split Phis from PostDom, copy their shapes here
    for (PHINode &Phi : Split->phis()) {
      Value *FollowerFromPostDom = *Phi.user_begin();
      assert(isa<PHINode>(FollowerFromPostDom) &&
             cast<PHINode>(FollowerFromPostDom)->getParent() == PostDom);
      setRippleShape(&Phi, getRippleShape(FollowerFromPostDom));
      LLVM_DEBUG(dbgs() << "Shape of PHI " << Phi << " is "
                        << getRippleShape(&Phi) << "\n");
    }
    ToClone.insert(Split);
    return Split;
  };

  auto normalizeBranchEntries = [&](BasicBlock *BranchBlock) -> void {
    // Add a basic block that can be cloned between BranchBlock and every branch
    // targets. This is useful to detect which branches have BranchBlock as
    // direct predecessors and those who don't when pruning PHI entries for some
    // paths.
    // For example:
    //
    // -----> D -----|
    // |             v
    // A -> B -> C ->E
    // |         ^
    // ----------|
    //
    // Where A is BranchBlock.
    //
    // When processing the branch B->C->E we have to prune the branch coming
    // directly to C from A. Adding an extra empty BB between A and all its
    // successors allows us to correctly identify these cases (since A is not
    // part of the BBs we are cloning).
    SmallPtrSet<BasicBlock *, 0> SuccsToProcess(successors(BranchBlock).begin(),
                                                successors(BranchBlock).end());
    for (BasicBlock *Succ : SuccsToProcess) {
      BasicBlock *Split = SplitBlockPredecessors(
          Succ, {BranchBlock}, ".ripple.vecbranch.succ", &DTU);
      setRippleShape(Split->getTerminator(), ScalarShape);
    }
  };

  auto brTerminator = [](BasicBlock *BB) {
    assert(isa<BranchInst>(BB->getTerminator()));
    BranchInst *BI = cast<BranchInst>(BB->getTerminator());
    assert(BI->isUnconditional());
    return BI;
  };

  auto updateSelectInstMaskSet = [&](ValueToValueMapTy &VMap) -> void {
    // Update select to mask set with new arrivals
    SmallPtrSet<SelectInst *, 8> ToMaskAsWell;
    for (SelectInst *SelectToMask : SelectToMaskWhenIfConvert)
      if (SelectInst *SelectClone =
              cast_if_present<SelectInst>(VMap.lookup(SelectToMask)))
        ToMaskAsWell.insert(SelectClone);
    SelectToMaskWhenIfConvert.insert(ToMaskAsWell.begin(), ToMaskAsWell.end());
  };

  auto updateStoreInstMaskSet = [&](ValueToValueMapTy &VMap) -> void {
    // Update store to be skipped in masking set with new arrivals
    SmallPtrSet<Instruction *, 8> ToSkipMaskAsWell;
    for (Instruction *ToSkipMasking : ToSkipMaskingWhenIfConvert)
      if (Instruction *ClonedI =
              cast_if_present<Instruction>(VMap.lookup(ToSkipMasking)))
        ToSkipMaskAsWell.insert(ClonedI);
    ToSkipMaskingWhenIfConvert.insert(ToSkipMaskAsWell.begin(),
                                      ToSkipMaskAsWell.end());
  };

  SmallVector<BasicBlock *, 0> BBsWithVectorBranchOrSwitch;
  for (auto *BB : getFuncRPOT()) {
    if (BB->back().isSpecialTerminator())
      // In ifConvert, we are only concerned with branch or switch instructions.
      continue;
    Value *last = BB->getTerminator();
    BranchInst *Branch = dyn_cast<BranchInst>(last);
    SwitchInst *Switch = dyn_cast<SwitchInst>(last);
    bool IsVectorBranch =
        Branch && Branch->isConditional() && getRippleShape(Branch).isVector();
    bool IsVectorSwitch = Switch && getRippleShape(Switch).isVector();
    if (IsVectorBranch || IsVectorSwitch) {
      BBsWithVectorBranchOrSwitch.push_back(BB);
    }
  }

  // After this we modify the control flow so FuncRPOT is not valid anymore
  delete FuncRPOT;
  FuncRPOT = nullptr;

  // We process vector branches from the inner mostly-nested outwards.  Hence,
  // when we process a vector branch, the sub-graph is vector-branch free. That
  // way, we don't have to track vector branches that would be cloned and need
  // to be processed too.
  std::sort(BBsWithVectorBranchOrSwitch.begin(),
            BBsWithVectorBranchOrSwitch.end(),
            [&](BasicBlock *BB1, BasicBlock *BB2) {
              return domTree.getNode(BB1)->getLevel() >
                     domTree.getNode(BB2)->getLevel();
            });
  // For each vector branch the algorithm works as follows:
  //
  // "entry" is the basic block w/ a vector if conditional and "pdom" is its
  // immediate post dominator.
  //
  // First, we make it so that only one branch from the sub-graph starting at
  // entry enters pdom. "predpdom" is inserted and will consists of PHI
  // functions (if needed) and an unconditional jump to pdom.
  //
  //          entry                                   entry
  //     /      |  ...  \                        /      |  ...  \
  //    br1    br2     brn                      br1    br2     brn
  //    |       |      /                        |       |      /
  //    entry_subgraph     otherpath     ->     entry_subgraph     otherpath
  //        \     |       /                         \     |       /
  //         \    |      /                          predpdom     /
  //          \   |     /                               \       /
  //             pdom                                     pdom
  //
  // Next, we splice entry to pdom, partially separating the sub-graph.
  // Notice that now the the sub-graph is essentially dead code since there is
  // no path from the function entry to it.
  //
  //        entry
  //   /      |  ...  \
  //  br1    br2     brn                    entry  br1    br2     brn
  //  |       |      /                        |     |       |      /
  //  entry_subgraph                  ->      \     entry_subgraph
  //      \     |                              \        \     |
  //      predpdom     otherpath                \       predpdom    otherpath
  //          \       /                          \         |       /
  //             pdom                                     pdom
  //
  // For each branch, we clone the sub-graph starting with the branch entry
  // basic block (e.g. br1) to pdom. We splice the sub-graph in-between entry
  // and pdom (represented below as br1_subG, etc). Finally each branch vector
  // mask is applied to its respective cloned subgraph maskable operators.
  //
  //  entry  br1    br2     brn             entry    br1    br2     brn
  //     \    |       |      /                |        \     |      /
  //      \   entry_subgraph               br1_subG   entry_subgraph
  //       \     \     |                      |          \     |
  //        \    predpdom   otherpath      br2_subG      predpdom   otherpath
  //         \      |      /                  ...           |      /
  //          \     |     /                brn_subG         |     /
  //          \     |     /                        \        |    /
  //                pdom                                   pdom
  //
  // Finally, we process live values created in the subgraph between entry and
  // pdom. Hence, for each PHI function in predpdom, we create a cascade of
  // select instructions to select the values we specialized in [br1_subG,
  // brn_subG] with respect to each branch mask.
  // Note: At this point, we can disconnect the sub-graph {[br1 ... brn] ->
  // predPdom} that we kept connected to simplify cloning and value re-mapping.
  // It will be garbage-collected at the end.
  //
  //   entry                           entry
  //     |                               |
  //  br1_subG                        br1_subG
  //     |                               |
  //  br2_subG                  ->    br2_subG
  //     ...                             ...
  //  brn_subG    otherpath           brn_subG
  //        \    /                        |
  //         pdom                   PHI_to_select  otherpath
  //                                       \        /
  //                                          pdom
  //
  for (auto *BranchingBB : BBsWithVectorBranchOrSwitch) {
    Instruction *BranchOrSwitch = BranchingBB->getTerminator();
    auto BranchSwitchIt = BranchAndSwitchVecCond.find(BranchOrSwitch);
    assert(BranchSwitchIt != BranchAndSwitchVecCond.end());
    Value *VectorConditional = BranchSwitchIt->second;
    BranchAndSwitchVecCond.erase(BranchSwitchIt);
    TensorShape MaskShape = getRippleShape(BranchOrSwitch);
    BasicBlock *BranchPostDom =
        postdomTree.getNode(BranchingBB)->getIDom()->getBlock();
    LLVM_DEBUG(dbgs() << "[Vector if conversion] processing vector branch "
                      << *BranchOrSwitch << " w/ vector condition "
                      << *VectorConditional << " from BasicBlock "
                      << BranchingBB->getName()
                      << " and immediate post-dominator "
                      << BranchPostDom->getName() << "\n");

    // Insert an intermediate basic block between BranchingBB and all its
    // successors. This simplifies cloning by not having to process direct
    // successors of BranchingBB or BranchingBB being a predecessor of
    // BranchPostDom in a special way.
    normalizeBranchEntries(BranchingBB);

    auto BBsToClone = allBasicBlocksFromTo(BranchingBB, BranchPostDom);

    // Prepare for cloning so that there is only one edge to BranchPostDom
    BasicBlock *PredOfBranchPostDomOrigin =
        makeUniquePredToPostDom(BBsToClone, BranchingBB, BranchPostDom);

    LLVM_DEBUG(dbgs() << "Function before if convert\n\n"; F.print(dbgs()));

    /// Handle CFGs of the nature: A<->B->Exit with entry in A or B.
    if (hasTrivialLoopLikeBackEdge(BranchingBB, BranchPostDom, domTree)) {
      // TODO: Generalize this implementation using the "Whole Function
      // Vectorization" algorithm. (See
      // <https://doi.org/10.1007/978-3-642-28652-0_1>)
      auto [LoopIncTarget, LoopCleanupTarget] =
          getIncrementAndCleanupTargets(BranchingBB, BranchPostDom, domTree);
      auto LoopIncBBs =
          allBasicBlocksFromToBFS(LoopIncTarget, BranchingBB, postdomTree);

      Value *LoopIncMask = nullptr;
      if (BranchInst *Branch = dyn_cast<BranchInst>(BranchOrSwitch)) {
        irBuilder.SetInsertPoint(Branch);
        if (Branch->getSuccessor(0) == LoopIncTarget) {
          LoopIncMask = VectorConditional;
        } else {
          assert(Branch->getSuccessor(1) == LoopIncTarget);
          LoopIncMask = irBuilder.CreateNot(
              VectorConditional, "mask.loopinc." + BranchingBB->getName());
          setRippleShape(LoopIncMask, MaskShape);
          // Normalize so that successor "0" is always LoopInc
          Branch->setSuccessor(0, LoopIncTarget);
          Branch->setSuccessor(1, LoopCleanupTarget);
        }

        // Branch into the increment target only if any of the vector lanes take
        // the branch => Use ReduceOr(LoopIncMask).
        auto *MaskReduceOred = irBuilder.CreateOrReduce(LoopIncMask);
        Branch->setCondition(MaskReduceOred);
        setRippleShape(MaskReduceOred, ScalarShape);
      } else {
        llvm_unreachable("This branch is unreachable.");
      }

      // Apply mask to all the basic blocks in the cycle.
      auto ExpectedAllTrueMask = tensorBcast(
          ConstantInt::get(irBuilder.getInt1Ty(), 1), ScalarShape, MaskShape);
      if (!ExpectedAllTrueMask)
        report_fatal_error("Failure to broadcast a scalar constant!");
      auto *AllTrueMask = *ExpectedAllTrueMask;
      DenseMap<BasicBlock *, Value *> LoopIncBB2Mask;
      LoopIncBB2Mask.insert({BranchingBB, LoopIncMask});
      for (auto *LoopIncBB : LoopIncBBs) {
        if (pred_size(LoopIncBB) != 1) {
          irBuilder.SetInsertPoint(&*LoopIncBB->getFirstInsertionPt());
          auto *LoopIncBBMask = irBuilder.CreatePHI(
              VectorConditional->getType(), pred_size(LoopIncBB));
          // Mask operations in LoopIncBB with the mask phi(currentMask, true).
          for (auto *PredLoopIncBB : predecessors(LoopIncBB)) {
            if (LoopIncBB2Mask.contains(PredLoopIncBB))
              LoopIncBBMask->addIncoming(LoopIncBB2Mask.at(PredLoopIncBB),
                                         PredLoopIncBB);
            else
              LoopIncBBMask->addIncoming(AllTrueMask, PredLoopIncBB);
          }
          setRippleShape(LoopIncBBMask, MaskShape);
          LoopIncBB2Mask.insert_or_assign(LoopIncBB, LoopIncBBMask);
        } else {
          auto *PredLoopIncBB = *pred_begin(LoopIncBB);
          assert(LoopIncBB2Mask.contains(PredLoopIncBB));
          LoopIncBB2Mask.insert_or_assign(LoopIncBB,
                                          LoopIncBB2Mask.at(PredLoopIncBB));
        }
      }
      for (auto &[LoopIncBB, LoopIncBBMask] : LoopIncBB2Mask)
        applyMaskToOps({LoopIncBB}, LoopIncBB2Mask.at(LoopIncBB), MaskShape,
                       &*LoopIncBB->getFirstInsertionPt());
      continue;
    }

    // Build the branch masks
    SmallVector<std::pair<BasicBlock *, Value *>, 2> TargetMasks;
    if (BranchInst *Branch = dyn_cast<BranchInst>(BranchOrSwitch)) {
      vectorBranchMasks(Branch, VectorConditional, TargetMasks, MaskShape);
    } else {
      assert(isa<SwitchInst>(BranchOrSwitch));
      SwitchInst *Switch = cast<SwitchInst>(BranchOrSwitch);
      vectorSwitchMasks(Switch, VectorConditional, TargetMasks, MaskShape);
    }

    // We simplify the CFG to prepare the flattening of the control flow
    // (if-conversion) of BB. For each branch going out of BB, the
    // sub-graph leading to postDom will be cloned and masked and inserted
    // in between BB and postDom, sequentially.
    //
    //   BB              BB
    //   / \              |
    //  /   \             |
    // LHS  RHS   ->      |
    //  |    |            |
    //    ...             |
    //     |              |
    // BranchPostDom   BranchPostDom
    std::vector<DominatorTree::UpdateType> DTUList;
    // Insert a new edge from BB to immPostDom
    irBuilder.SetInsertPoint(BranchingBB);
    irBuilder.SetCurrentDebugLocation(BranchOrSwitch->getDebugLoc());
    BranchInst *Br = irBuilder.CreateBr(BranchPostDom);
    setRippleShape(Br, ScalarShape);
    // Remove Edges from BB to LHS & RHS
    invalidateRippleDataFor(BranchOrSwitch);
    BranchOrSwitch->eraseFromParent();
    for (auto TargetAndMask : TargetMasks) {
      DTUList.push_back(
          {DominatorTree::Delete, BranchingBB, TargetAndMask.first});
    }

    // The basic block that precedes immPostDom
    BasicBlock *PredOfBranchPostDomCurrent = BranchingBB;
    // Now we clone and apply the masks
    std::vector<ValueToValueMapTy> VMaps(TargetMasks.size());
    std::vector<BasicBlock *> ClonedBBs;
    ClonedBBs.reserve(BBsToClone.size());
    size_t VMapIndex = 0;
    for (auto TargetAndMask : TargetMasks) {
      BasicBlock *const BranchEntryBlock = TargetAndMask.first;
      Value *const VectorMask = TargetAndMask.second;
      ValueToValueMapTy &VMap = VMaps[VMapIndex++];
      ClonedBBs.clear();
      // Makes it so that BB (that branches into the targets) is
      // replaced by the new entry point in the cloned subgraph
      VMap[BranchingBB] = PredOfBranchPostDomCurrent;

      // Clone the sub-graph for the branch.
      clonePathStartingWith(BranchEntryBlock, BBsToClone, VMap, ClonedBBs);
      updateSelectInstMaskSet(VMap);
      updateStoreInstMaskSet(VMap);

      // Instead of branching to immPostDom, jump to the branch entry
      BasicBlock *BranchEntryClone = cast<BasicBlock>(&*VMap[BranchEntryBlock]);
      BranchInst *ToBranchPostDom = brTerminator(PredOfBranchPostDomCurrent);
      assert(ToBranchPostDom->getSuccessor(0) == BranchPostDom);
      ToBranchPostDom->setSuccessor(0, BranchEntryClone);
      DTUList.push_back({DominatorTree::Insert, PredOfBranchPostDomCurrent,
                         BranchEntryClone});

      // Record changes done to the CFG
      assert(VMap[PredOfBranchPostDomOrigin].pointsToAliveValue());
      BasicBlock *PredOfBranchPostDomClone =
          cast<BasicBlock>(&*VMap[PredOfBranchPostDomOrigin]);
      assert(PredOfBranchPostDomClone->getUniqueSuccessor() == BranchPostDom);
      for (auto *ClonedBB : ClonedBBs) {
        // This will either be updated next iteration (or after loop)
        if (ClonedBB == PredOfBranchPostDomClone)
          continue;
        for (auto *S : successors(ClonedBB)) {
          DTUList.push_back({DominatorTree::Insert, ClonedBB, S});
        }
      }

      DTU.applyUpdates(DTUList);
      DTUList.clear();

      // Apply masks to maskable ops (load/store/reduction/shuffles)
      applyMaskToOps(ClonedBBs, VectorMask, MaskShape, Br);

      PredOfBranchPostDomCurrent = PredOfBranchPostDomClone;
      LLVM_DEBUG(dbgs() << "Function after applying mask " << *VectorMask
                        << " to " << *BranchEntryBlock << "\n";
                 F.print(dbgs()));
    }
    DTUList.push_back(
        {DominatorTree::Insert, PredOfBranchPostDomCurrent, BranchPostDom});

    // Unlinks the original sub-graph from BranchPostDom now that the cloning is
    // finished
    BranchPostDom->replacePhiUsesWith(PredOfBranchPostDomOrigin,
                                      PredOfBranchPostDomCurrent);
    Instruction *Terminator = PredOfBranchPostDomOrigin->getTerminator();
    // We create a new terminator instruction to keep the BasicBlock in a valid
    // state. The original sub-graph is dead code and is garbage collected at
    // the end.
    irBuilder.SetInsertPoint(PredOfBranchPostDomOrigin);
    irBuilder.CreateRetVoid();
    // Erasing the branch instruction removes predOmmPoseDomOrig from the
    // predecessors of immPostDom
    invalidateRippleDataFor(Terminator);
    Terminator->eraseFromParent();
    DTUList.push_back(
        {DominatorTree::Delete, PredOfBranchPostDomOrigin, BranchPostDom});

    // Apply the dominator tree updates before splitting for "PHI to select"
    // insertion
    DTU.applyUpdates(DTUList);

    // For every PHI values in predImmPostDomOrig we create a *select*
    // instruction to get the correct value flowing in immPostDom
    SmallVector<BasicBlock *, 1> PredsToSplit = {PredOfBranchPostDomCurrent};
    BasicBlock *SelectInsertionBlock = SplitBlockPredecessors(
        BranchPostDom, PredsToSplit, "ripple.vecbranch.selectblock", &DTU);
    // There should only be a jump to BranchPostDom in the split BasicBlock
    setRippleShape(SelectInsertionBlock->getTerminator(), ScalarShape);
    for (PHINode &Phi : PredOfBranchPostDomOrigin->phis()) {
      // We have one value flowing in from each branch
      std::vector<Value *> SpecializedPhis;
      for (ValueToValueMapTy &VMap : VMaps) {
        auto SpecializedPhi = VMap.lookup(&Phi);
        assert(SpecializedPhi.pointsToAliveValue() &&
               "Mapping to PhiNode must be alive");
        LLVM_DEBUG(dbgs() << "Specialized phi " << *SpecializedPhi
                          << getRippleShape(SpecializedPhi) << "\n");
        SpecializedPhis.push_back(SpecializedPhi);
      }

      TensorShape PhiShape = getRippleShape(&Phi);
      LLVM_DEBUG(dbgs() << "Applying mask " << MaskShape << " to select of PHI "
                        << Phi << " with shape " << PhiShape << "\n");

      // Create a possible cascade of selects (for switches)
      Value *SelectedValue = SpecializedPhis[TargetMasks.size() - 1];
      for (unsigned MaskIdx = 0; MaskIdx < TargetMasks.size() - 1; ++MaskIdx) {
        irBuilder.SetInsertPoint(SelectInsertionBlock->getFirstInsertionPt());
        irBuilder.SetCurrentDebugLocation(Phi.getDebugLoc());

        auto MaskToApply = reduceBcastMaskToShape(TargetMasks[MaskIdx].second,
                                                  MaskShape, PhiShape);
        LLVM_DEBUG(dbgs() << "Select between " << *SpecializedPhis[MaskIdx]
                          << " and " << *SelectedValue << " using mask "
                          << *MaskToApply << "\n");
        SelectedValue = irBuilder.CreateSelect(
            MaskToApply, SpecializedPhis[MaskIdx], SelectedValue,
            Twine(Phi.getName()) + ".ripple.phi.vectorized", &Phi);

        setRippleShape(SelectedValue, PhiShape);
      }

      Phi.replaceAllUsesWith(SelectedValue);
    }
    LLVM_DEBUG(dbgs() << "[Vector if conversion] Constructed the following "
                         "vector select block:\n";
               SelectInsertionBlock->print(dbgs()));

    // Delete the original sub-graph
    std::vector<BasicBlock *> DeadBlocks(BBsToClone.begin(), BBsToClone.end());
    for (auto &BB : DeadBlocks)
      for (auto &I : *BB)
        invalidateRippleDataFor(&I);

    DeleteDeadBlocks(DeadBlocks, &DTU);
    DTU.flush();
    LLVM_DEBUG(DTU.getDomTree().verify());
    LLVM_DEBUG(DTU.getPostDomTree().verify());
  }
  assert(BranchAndSwitchVecCond.empty());
}

void Ripple::clonePathStartingWith(BasicBlock *Start,
                                   const DenseSet<BasicBlock *> &BBsToClone,
                                   ValueToValueMapTy &VMap,
                                   std::vector<BasicBlock *> &ClonedBBs) {
  std::queue<BasicBlock *> WorkList;
  WorkList.push(Start);
  while (!WorkList.empty()) {
    BasicBlock *BB = WorkList.front();
    WorkList.pop();
    // Don't clone twice (can revisit when loops are present in the SESE region)
    if (VMap.count(BB))
      continue;
    BasicBlock *Clone =
        CloneBasicBlock(BB, VMap, ".ripple.branch.clone", BB->getParent());
    LLVM_DEBUG(dbgs() << "Cloned " << BB->getName() << " as "
                      << Clone->getName() << "\n");
    VMap[BB] = Clone;
    ClonedBBs.push_back(Clone);
    for (BasicBlock *Next : successors(BB)) {
      if (BBsToClone.contains(Next) && !VMap.count(Next))
        WorkList.push(Next);
    }
  }

  LLVM_DEBUG(dbgs() << "MidPoint \n");

  // Fix PHIs with incoming values from blocks that were deleted
  for (BasicBlock *BB : BBsToClone) {
    if (!VMap.count(BB)) {
      for (auto *Succ : successors(BB)) {
        auto ClonedBBIterator = VMap.find(Succ);
        if (ClonedBBIterator != VMap.end()) {
          BasicBlock *ClonedBB = cast<BasicBlock>(&*ClonedBBIterator->second);
          LLVM_DEBUG(dbgs()
                     << "Did not clone " << BB->getName()
                     << ", fixing PHIs in clone of " << Succ->getName() << " ("
                     << ClonedBB->getName() << ")" << *ClonedBB << "\n");
          // we cannot use the "removePredecessors" method because ClonedBB does
          // not have the block as predecessor (that's why we are removing it)
          removeIncomingBlockFromPhis(BB, ClonedBB);
        }
      }
    }
  }

  // Remap all the instructions
  remapInstructionsInBlocks(ClonedBBs, VMap);
  for (const auto &[SourceVal, MappedVal] : VMap)
    if (const Instruction *I = dyn_cast<Instruction>(SourceVal))
      setRippleShape(MappedVal, getRippleShape(I));
}

void Ripple::vectorBranchMasks(
    BranchInst *Branch, Value *VectorCondition,
    SmallVectorImpl<std::pair<BasicBlock *, Value *>> &TargetMasks,
    const TensorShape &MaskShape) {
  // Create additional masks right before the branch instruction
  irBuilder.SetInsertPoint(Branch);
  if (Branch->getSuccessor(0) == Branch->getSuccessor(1)) {
    // True and False branch to the same BasicBlock, the mask is true
    VectorType *ConditionType = cast<VectorType>(VectorCondition->getType());
    Value *TrueMask =
        ConstantVector::getSplat(ConditionType->getElementCount(),
                                 ConstantInt::getTrue(Branch->getContext()));
    TargetMasks.push_back(std::make_pair(Branch->getSuccessor(0), TrueMask));
  } else {
    TargetMasks.push_back(
        std::make_pair(Branch->getSuccessor(0), VectorCondition));
    Value *FalseMask =
        irBuilder.CreateNot(VectorCondition, "ripple.false.branch.mask");
    setRippleShape(FalseMask, MaskShape);
    TargetMasks.push_back(std::make_pair(Branch->getSuccessor(1), FalseMask));
  }
}

void Ripple::vectorSwitchMasks(
    SwitchInst *Switch, Value *VectorCondition,
    SmallVectorImpl<std::pair<BasicBlock *, Value *>> &TargetMasks,
    const TensorShape &MaskShape) {

  using TargetMasksType =
      typename std::remove_reference<decltype(TargetMasks)>::type::value_type;

  assert(VectorCondition->getType()->isVectorTy());
  VectorType *ConditionType = cast<VectorType>(VectorCondition->getType());
  // Create the masks right before the switch instruction
  irBuilder.SetInsertPoint(Switch);
  auto SwitchCases = Switch->cases();
  for (auto &Case : SwitchCases) {
    auto *CaseValue = Case.getCaseValue();
    assert(!CaseValue->getType()->isVectorTy());
    Constant *CaseValueSplat =
        ConstantVector::getSplat(ConditionType->getElementCount(), CaseValue);
    Value *CaseMask = irBuilder.CreateICmpEQ(VectorCondition, CaseValueSplat,
                                             "ripple.switch.case.mask");
    setRippleShape(CaseMask, MaskShape);
    TargetMasks.push_back(std::make_pair(Case.getCaseSuccessor(), CaseMask));
  }

  // Build the default mask as NOT(OR(caseMasks))
  // If only default is present, the default mask will be true
  Value *DefaultMask =
      SwitchCases.empty()
          ? ConstantVector::getSplat(ConditionType->getElementCount(),
                                     ConstantInt::getTrue(Switch->getContext()))
          : TargetMasks[0].second;
  for (unsigned i = 1; i < TargetMasks.size(); ++i) {
    DefaultMask = irBuilder.CreateOr(DefaultMask, TargetMasks[i].second);
    setRippleShape(DefaultMask, MaskShape);
  }
  if (!SwitchCases.empty()) {
    DefaultMask = irBuilder.CreateNot(DefaultMask);
    setRippleShape(DefaultMask, MaskShape);
  }
  DefaultMask->setName("ripple.switch.default.mask");
  TargetMasks.push_back(std::make_pair(Switch->getDefaultDest(), DefaultMask));

  // Merge the masks when jumping to the same target, keeping default last
  SmallPtrSet<TargetMasksType *, 16> Simplified;
  for (auto TargetPairIt = TargetMasks.rbegin(), end = TargetMasks.rend();
       TargetPairIt != end; ++TargetPairIt) {
    auto &TargetPair = *TargetPairIt;
    if (Simplified.contains(&TargetPair))
      continue;
    auto &[BranchBB, BranchMask] = TargetPair;
    for (auto OtherPairIt = TargetPairIt + 1; OtherPairIt != end;
         ++OtherPairIt) {
      auto &OtherTargetPair = *OtherPairIt;
      if (Simplified.contains(&OtherTargetPair))
        continue;
      auto &[OtherBranchBB, OtherMask] = OtherTargetPair;
      // Switch to the same target, we can OR the masks and remove
      // OtherTargetPair
      if (BranchBB == OtherBranchBB) {
        Value *NewMask = irBuilder.CreateOr(BranchMask, OtherMask);
        setRippleShape(NewMask, MaskShape);
        BranchMask = NewMask;
        Simplified.insert(&OtherTargetPair);
      }
    }
  }
  if (!Simplified.empty()) {
    SmallVector<TargetMasksType, 16> KeptPairs;
    std::copy_if(TargetMasks.begin(), TargetMasks.end(),
                 std::back_inserter(KeptPairs),
                 [&](auto &Pair) { return !Simplified.contains(&Pair); });
    TargetMasks.clear();
    TargetMasks.append(KeptPairs.begin(), KeptPairs.end());
  }
}


DenseSet<BasicBlock *> Ripple::allBasicBlocksFromTo(BasicBlock *from,
                                                    BasicBlock *to) const {
  assert(postdomTree.dominates(to, from));
  std::queue<BasicBlock *> toProcess;
  DenseSet<BasicBlock *> visited;
  toProcess.push(from);
  while (!toProcess.empty()) {
    BasicBlock *BB = toProcess.front();
    toProcess.pop();
    visited.insert(BB);
    for (auto *SI : successors(BB)) {
      if (!visited.contains(SI) && SI != to)
        toProcess.push(SI);
    }
  }
  visited.erase(from);
  return visited;
}

Value *Ripple::genMultiDimReduction(Intrinsic::ID reductionId, Value *vector,
                                    const TensorShape &vectorShape,
                                    const BitVector &reductionDimensions,
                                    FMFSource FMFSource) {
  auto reductionBinOp =
      [](Intrinsic::ID redId) -> std::optional<Instruction::BinaryOps> {
    switch (redId) {
    default:
      return std::nullopt;
    case Intrinsic::vp_reduce_add:
      return Instruction::Add;
    case Intrinsic::vp_reduce_or:
      return Instruction::Or;
    case Intrinsic::vp_reduce_xor:
      return Instruction::Xor;
    case Intrinsic::vp_reduce_mul:
      return Instruction::Mul;
    case Intrinsic::vp_reduce_and:
      return Instruction::And;
    case Intrinsic::vp_reduce_fadd:
      return Instruction::FAdd;
    case Intrinsic::vp_reduce_fmul:
      return Instruction::FMul;
    }
  };
  auto cmpReduction = [](Intrinsic::ID redId) -> std::optional<Intrinsic::ID> {
    switch (redId) {
    default:
      return std::nullopt;
    case Intrinsic::vp_reduce_umin:
      return Intrinsic::umin;
    case Intrinsic::vp_reduce_smin:
      return Intrinsic::smin;
    case Intrinsic::vp_reduce_umax:
      return Intrinsic::umax;
    case Intrinsic::vp_reduce_smax:
      return Intrinsic::smax;
    case Intrinsic::vp_reduce_fmin:
      return Intrinsic::minnum;
    case Intrinsic::vp_reduce_fmax:
      return Intrinsic::maxnum;
    case Intrinsic::vp_reduce_fminimum:
      return Intrinsic::minimum;
    case Intrinsic::vp_reduce_fmaximum:
      return Intrinsic::maximum;
    }
  };
  IRBuilder<>::FastMathFlagGuard FMFGuard(irBuilder);
  irBuilder.setFastMathFlags(FMFSource.get({}));

  Constant *NeutralElement = getRippleNeutralReductionElement(
      reductionId, vector->getType()->getScalarType(),
      irBuilder.getFastMathFlags());

  ElementCount VectorCount =
      cast<VectorType>(vector->getType())->getElementCount();
  // Partial reductions
  Constant *NeutralVector =
      ConstantVector::getSplat(VectorCount, NeutralElement);
  // Create a select to introduce a maskable instruction
  Constant *TrueMask = ConstantVector::getSplat(
      VectorCount, ConstantInt::getTrue(vector->getContext()));
  TensorShape CurrentVectorShape = vectorShape;
  SelectInst *SelectedValue = irBuilder.Insert(SelectInst::Create(
      TrueMask, vector, NeutralVector,
      Twine(vector->getName()) + ".ripple.reduction.partial.masking"));
  setRippleShape(SelectedValue, CurrentVectorShape);
  SelectToMaskWhenIfConvert.insert(SelectedValue);

  // Full reduction, use well supported llvm reductions instead
  if (vectorShape.reducedToScalarBy(reductionDimensions)) {
    if (auto IntrinsicId =
            VPIntrinsic::getFunctionalIntrinsicIDForVP(reductionId)) {
      SmallVector<Value *, 2> Args;
      if (*IntrinsicId == Intrinsic::vector_reduce_fadd ||
          *IntrinsicId == Intrinsic::vector_reduce_fmul) {
        Args.push_back(NeutralElement);
      }
      Args.push_back(SelectedValue);
      CallInst *ReductionCall = irBuilder.CreateIntrinsic(
          vector->getType()->getScalarType(), *IntrinsicId, Args, {},
          Twine(vector->getName()) + ".ripple.reduction");
      setRippleShape(ReductionCall, ScalarShape);
      return ReductionCall;
    }
  }

  Value *CurrentVectorValue = SelectedValue;

  std::vector<int> ShuffleMask;
  ShuffleMask.reserve(CurrentVectorShape.flatShape());
  for (auto RedDim : reductionDimensions.set_bits()) {
    NeutralVector = ConstantVector::getSplat(
        ElementCount::getFixed(CurrentVectorShape.flatShape()), NeutralElement);
    // Each non-empty inner dimension that is not being reduced adds a
    // multiple to the number of elements we start shuffling to reduce:
    // normally we reduce by shuffling at distance 1, 2, ..., 2^n
    unsigned ReductionIterCount = Log2_64_Ceil(CurrentVectorShape[RedDim]);
    for (unsigned RedIter = 0; RedIter < ReductionIterCount; ++RedIter) {
      ShuffleMask.clear();
      // Build the shuffle indices vector
      SmallVector<size_t, 0> RotateIndices(tensorRank());
      CurrentVectorShape.foreachIndex([&](auto Indices) {
        auto ReduceWith = Indices[RedDim] + (1 << RedIter);
        if (ReduceWith >= CurrentVectorShape[RedDim])
          // use the neutral element at the same offset
          ShuffleMask.push_back(CurrentVectorShape.getOffsetAt(Indices) +
                                CurrentVectorShape.flatShape());
        else {
          std::copy(Indices.begin(), Indices.end(), RotateIndices.begin());
          RotateIndices[RedDim] = ReduceWith;
          ShuffleMask.push_back(CurrentVectorShape.getOffsetAt(RotateIndices));
        }
      });
      Value *Shuff = irBuilder.CreateShuffleVector(
          CurrentVectorValue, NeutralVector, ShuffleMask,
          Twine(vector->getName()) + ".ripple.reducelog2.shuffle");
      setRippleShape(Shuff, CurrentVectorShape);
      auto BinRedOp = reductionBinOp(reductionId);
      if (BinRedOp) {
        CurrentVectorValue = irBuilder.CreateBinOp(
            *BinRedOp, CurrentVectorValue, Shuff,
            Twine(vector->getName()) + ".ripple.reducelog2.operator");
        setRippleShape(CurrentVectorValue, CurrentVectorShape);
      } else {
        auto CmpPred = cmpReduction(reductionId);
        assert(CmpPred);
        CurrentVectorValue = irBuilder.CreateIntrinsic(
            CurrentVectorValue->getType(), *CmpPred,
            {CurrentVectorValue, Shuff}, {},
            Twine(vector->getName()) + ".ripple.reducelog2.compare.op");
        setRippleShape(CurrentVectorValue, CurrentVectorShape);
      }
    }
    // Now extract the reduced value that is at index 0 (or anywhere really) in
    // the reduced dimension
    ShuffleMask.clear();
    CurrentVectorShape.foreachIndex([&](auto Indices) {
      if (Indices[RedDim] == 0)
        ShuffleMask.push_back(CurrentVectorShape.getOffsetAt(Indices));
    });
    // Get the new shape
    BitVector RedCurrentDim(reductionDimensions.size());
    RedCurrentDim.set(RedDim);
    CurrentVectorShape.reduceDimensions(RedCurrentDim);

    if (CurrentVectorShape.isScalar()) {
      CurrentVectorValue = irBuilder.CreateExtractElement(
          CurrentVectorValue, static_cast<uint64_t>(0), "ripple.red.extract");
    } else {
      CurrentVectorValue = irBuilder.CreateShuffleVector(
          CurrentVectorValue, ShuffleMask,
          Twine(vector->getName()) + ".ripple.reducelog2.dim.final");
    }
    setRippleShape(CurrentVectorValue, CurrentVectorShape);
  }

  assert(CurrentVectorShape.reduceDimensions(reductionDimensions) == false &&
         "The shape should already be a fully reduced tensor");
  return CurrentVectorValue;
}

Value *Ripple::buildLinearSeriesSlope(const LinearSeries *LS) {
  assert(LS->getSlopeShape().isVector());
  const TensorShape &LinearSeriesShape = LS->getShape();

  if (LS->hasZeroSlopes())
    return ConstantAggregateZero::get(
        VectorType::get(LS->getSlope(0)->getType(),
                        LinearSeriesShape.flatShape(), /*Scalable*/ false));

  const TensorShape &SlopeShape = LS->getSlopeShape();
  // Slope should be an integer type by construction
  Value *SlopeValue = nullptr;
  for (unsigned RankIdx = 0; RankIdx < SlopeShape.rank(); ++RankIdx) {
    auto size = SlopeShape[RankIdx];
    if (size > 1) {
      TensorShape SlopeShape =
          TensorShape(LinearSeriesShape.rank(), RankIdx, size);
      // Series [0, 1, ..., size - 1]
      Constant *BaseSeries = LinearSeries::constructLinearSeriesVector(
          cast<IntegerType>(LS->getSlope(RankIdx)->getType()), size);

      // Slope as vector
      Value *SlopeBcast = irBuilder.CreateVectorSplat(
          size, LS->getSlope(RankIdx),
          Twine(LS->getBase()->getName()) + ".ripple.LS.dim.slope.bcast");
      // A Splat is an insert followed by a shuffle
      setRippleShape(SlopeBcast, SlopeShape);
      if (ShuffleVectorInst *ShuffleInst =
              dyn_cast<ShuffleVectorInst>(SlopeBcast))
        setRippleShape(ShuffleInst->getOperand(0), SlopeShape);

      // Slope Series [0, slope, ..., slope * (size - 1)]
      Value *SlopeSeries = irBuilder.CreateBinOp(
          Instruction::Mul, BaseSeries, SlopeBcast,
          Twine(LS->getBase()->getName()) + ".ripple.LS.dim.slope");
      setRippleShape(SlopeSeries, SlopeShape);

      auto BcastedSeries =
          tensorBcast(SlopeSeries, SlopeShape, LinearSeriesShape);
      // Checked during shape propagation
      if (!BcastedSeries)
        report_fatal_error("Broadcast failure during LinearSeries codegen");
      setRippleShape(*BcastedSeries, LinearSeriesShape);

      if (SlopeValue) {
        SlopeValue = irBuilder.CreateBinOp(
            Instruction::Add, SlopeValue, *BcastedSeries,
            Twine(LS->getBase()->getName()) + ".ripple.LS.slope.combine");
        setRippleShape(SlopeValue, LinearSeriesShape);
      } else {
        SlopeValue = *BcastedSeries;
      }
    }
  }
  SlopeValue->setName(Twine(LS->getBase()->getName()) + ".ripple.LS.slope");
  assert(SlopeValue != nullptr);
  return SlopeValue;
}

Constant *LinearSeries::constructLinearSeriesVector(IntegerType *intTy,
                                                    uint64_t size) {
  std::vector<Constant *> cstVectorVals;
  for (uint64_t i = 0; i < size; ++i) {
    cstVectorVals.push_back(ConstantInt::get(
        intTy->getScalarType(), APInt(intTy->getBitWidth(), i, false)));
  }
  return ConstantVector::get(cstVectorVals);
}

IntegerType *LinearSeries::getSlopeTypeFor(const DataLayout &DL,
                                           Type *BaseType) {
  Type *BaseScalarType = BaseType->getScalarType();
  IntegerType *SlopeType = nullptr;
  if (IntegerType *IntTy = dyn_cast<IntegerType>(BaseScalarType))
    SlopeType = IntTy;
  else if (PointerType *PTy = dyn_cast<PointerType>(BaseScalarType)) {
    SlopeType =
        IntegerType::get(BaseType->getContext(),
                         DL.getPointerSizeInBits(PTy->getAddressSpace()));
  }
  return SlopeType;
}

bool Ripple::canConstructSplatSeries(Value *V, const TensorShape &FromShape,
                                     const TensorShape &ToShape) {
  return (isa<PointerType>(V->getType()->getScalarType()) ||
          isa<IntegerType>(V->getType()->getScalarType())) &&
         !FromShape.isBroadcastError(ToShape);
}

Ripple::ConstructedSeries
Ripple::getLinearSeriesFor(Constant *C, const TensorShape &FromShape,
                           const TensorShape &ToShape) {
  return getSplatSeries(C, FromShape, ToShape);
}

Ripple::ConstructedSeries
Ripple::getLinearSeriesFor(Argument *A, const TensorShape &FromShape,
                           const TensorShape &ToShape) {
  return getSplatSeries(A, FromShape, ToShape);
}

Ripple::ConstructedSeries Ripple::getSplatSeries(Value *V,
                                                 const TensorShape &FromShape,
                                                 const TensorShape &ToShape) {
  if (!canConstructSplatSeries(V, FromShape, ToShape))
    return {};

  // We can generate integer or pointer linear series (through GEP)
  Type *SlopeType =
      LinearSeries::getSlopeTypeFor(DL, V->getType()->getScalarType());

  if (!SlopeType)
    return {};

  assert(!(FromShape.isScalar() && V->getType()->isVectorTy()));

  const auto ConstantSlopeOf = [&](uint64_t val) {
    return ConstantInt::get(SlopeType, val,
                            /*signed*/ false);
  };

  // The slope shape is composed of the dimensions of ToShape - FromShape
  TensorShape SlopeShape = ToShape;
  SlopeShape.reduceDimensions(FromShape.nonEmptyDims());
  SmallVector<Value *, 4> NewSlopes;

  NewSlopes.append(ToShape.rank(), ConstantSlopeOf(0));
  LinearSeries *LS = new LinearSeries(V, FromShape, NewSlopes, SlopeShape);
  return {LS, CSState::ValidLinearSeries};
}

Ripple::ConstructedSeries Ripple::tryToPromoteLinearSeries(LinearSeries *LS) {
  Value *Base = LS->getBase();
  const TensorShape &ExpectedBaseShape = LS->getBaseShape();
  if (PHINode *Phi = dyn_cast<PHINode>(Base)) {
    LLVM_DEBUG(dbgs() << "Trying to promote " << *LS << "\n");
    // Process the incoming values that were not inserted yet
    for (unsigned IncomingIdx = 0; IncomingIdx < Phi->getNumIncomingValues();
         ++IncomingIdx) {
      Value *IncomingValue = Phi->getIncomingValue(IncomingIdx);
      BasicBlock *IncomingBlock = Phi->getIncomingBlock(IncomingIdx);
      // Add if it's missing from our slopes
      if (cast<PHINode>(LS->getSlope(0))->getBasicBlockIndex(IncomingBlock) ==
          -1) {
        // The potentially missing incoming values are instructions
        LLVM_DEBUG(dbgs() << "Missing incoming from: "
                          << IncomingBlock->getName() << " value "
                          << *IncomingValue << " from " << *LS->getSlope(0)
                          << "\n");
        Instruction *IncomingInstruction = cast<Instruction>(IncomingValue);
        auto OperandCS = getCachedSeries(IncomingInstruction);
        if (OperandCS.isNotASeries()) {
          LLVM_DEBUG(dbgs() << "Incoming is not a series!\n");
          return {};
        }
        if (OperandCS.LS->getBaseShape() != ExpectedBaseShape) {
          LLVM_DEBUG(dbgs() << "Operand base shape differs: expected "
                            << ExpectedBaseShape << " and operand has "
                            << OperandCS.LS->getBaseShape() << "\n");
          return {};
        }
        LLVM_DEBUG(dbgs() << "Valid or potential linear series: "
                          << *OperandCS.LS << "\n");
        for (unsigned SlopeIdx = 0; SlopeIdx < LS->getSlopeShape().rank();
             ++SlopeIdx) {
          PHINode *SlopePhi = cast<PHINode>(LS->getSlope(SlopeIdx));
          assert(SlopePhi->getBasicBlockIndex(IncomingBlock) == -1);
          SlopePhi->addIncoming(OperandCS.LS->getSlope(SlopeIdx),
                                IncomingBlock);
        }
      }
    }
    // We must check that the roots of values flowing into the PHI are valid
    // linear series before promoting
    if (!hasValidLinearSeriesRoots(LS)) {
      LLVM_DEBUG(
          dbgs() << *LS
                 << " has non linear series root and cannot be promoted!\n");
      return {};
    }
    LLVM_DEBUG(dbgs() << *LS << " has been promoted!\n");
    for ([[maybe_unused]] auto &Slope : LS->slopes()) {
      assert(cast<PHINode>(&*Slope)->isComplete());
    }
    // All incoming values have a matching base, we can promote the PHI
    LsCache.Potential.erase(Phi);
    LsCache.Valid.insert({Phi, LS});
    return {LS, CSState::ValidLinearSeries};
  } else if (Instruction *I = dyn_cast<Instruction>(Base)) {
    for (auto &Operand : I->operands()) {
      if (Instruction *OperandInstr = dyn_cast<Instruction>(Operand)) {
        auto OperandCS = getCachedSeries(OperandInstr);
        if (OperandCS.LS->getBaseShape() != ExpectedBaseShape) {
          LLVM_DEBUG(dbgs() << "This base shape " << ExpectedBaseShape
                            << " differs from other base shape "
                            << OperandCS.LS->getBaseShape());
          return {};
        }
        if (!OperandCS.isValid())
          return {};
      }
    }
    LLVM_DEBUG(dbgs() << *LS << " has been promoted!\n");
    LsCache.Potential.erase(I);
    LsCache.Valid.insert({I, LS});
    return {LS, CSState::ValidLinearSeries};
  } else {
    // Only Instructions should become Potential Series
    report_fatal_error("A linear series base which is not an Instruction can "
                       "not be a potential series");
  }
}

Ripple::ConstructedSeries Ripple::getLinearSeriesFor(Instruction *I) {
  if (!I)
    return {};

  // TODO: Remove when llvm has entirely transitioned to record instead of
  // intrinsics for debug info!
  if (isa<DbgInfoIntrinsic>(I))
    return {};

  const TensorShape &ToShape = getRippleShape(I);

  auto isASlope = [&](Instruction *I) -> bool {
    return I && SlopeInstructions.contains(I);
  };

  IntegerType *SlopeType = LinearSeries::getSlopeTypeFor(DL, I->getType());
  // We can generate integer or pointer linear series (through GEP)
  if (!SlopeType)
    return {};

  // Use already constructed series if possible
  auto CachedCS = getCachedSeries(I);
  if (CachedCS.isValid())
    return CachedCS;

  if (CachedCS.hasPotential()) {
    assert(I == CachedCS.LS->getBase());
    if (auto PromotedCS = tryToPromoteLinearSeries(CachedCS.LS))
      return PromotedCS;
    else
      return CachedCS;
  }

  if (isASlope(I)) {
    LLVM_DEBUG(dbgs() << "We avoid creating LS for slope: " << *I << "\n");
    return {};
  }

  const auto ConstantSlopeOf = [SlopeType](uint64_t val) {
    return ConstantInt::get(SlopeType, val, /*signed*/ false);
  };

  auto getOperandSeries = [&](Value *Operand) -> ConstructedSeries {
    if (Constant *C = dyn_cast<Constant>(Operand)) {
      if (!C->getType()->isVectorTy())
        return getLinearSeriesFor(C, getRippleShape(C), ToShape);
    } else if (Argument *A = dyn_cast<Argument>(Operand)) {
      if (!A->getType()->isVectorTy())
        return getLinearSeriesFor(A, getRippleShape(A), ToShape);
    } else if (Instruction *I = dyn_cast<Instruction>(Operand))
      return getCachedSeries(I);
    return {};
  };

  // Get the linear series of the operands
  std::vector<ConstructedSeries> OperandSeries;
  for (unsigned OperandIdx = 0; OperandIdx < I->getNumOperands();
       ++OperandIdx) {
    Value *Operand = I->getOperand(OperandIdx);
    OperandSeries.push_back(getOperandSeries(Operand));
  }

  SmallVector<Value *, 3> NewSlopes;

  auto processRippleBlockIndex =
      [&](IntrinsicInst *RippleIndexInst) -> ConstructedSeries {
    // For Ripple Index, the base is zero and the shape is carried by the slope
    for (unsigned i = 0; i < ToShape.rank(); ++i)
      NewSlopes.push_back(ToShape[i] > 1 ? ConstantSlopeOf(1)
                                         : ConstantSlopeOf(0));
    LinearSeries *LS =
        new LinearSeries(ConstantSlopeOf(0), ScalarShape, NewSlopes, ToShape);
    return {LS, CSState::ValidLinearSeries};
  };

  auto processRippleGetSize =
      [&](IntrinsicInst *RippleGetSizeInst) -> ConstructedSeries {
    for (unsigned i = 0; i < ToShape.rank(); ++i)
      NewSlopes.push_back(ConstantSlopeOf(0));
    LinearSeries *LS = new LinearSeries(
        ConstantSlopeOf(getRippleGetSizeValue(RippleGetSizeInst)), ScalarShape,
        NewSlopes, ToShape);
    return {LS, CSState::ValidLinearSeries};
  };

  auto processBinOp = [&](BinaryOperator *BinOp) -> ConstructedSeries {
    auto &LhsSeries = OperandSeries[0];
    auto &RhsSeries = OperandSeries[1];
    if (LhsSeries.isNotASeries() || RhsSeries.isNotASeries())
      return {};

    CSState NewState =
        combineStatesBinaryOp(LhsSeries.getState(), RhsSeries.getState());

    const TensorShape &LhsBaseShape = LhsSeries.LS->getBaseShape();
    const TensorShape &LhsSlopeShape = LhsSeries.LS->getSlopeShape();
    const TensorShape &RhsBaseShape = RhsSeries.LS->getBaseShape();
    const TensorShape &RhsSlopeShape = RhsSeries.LS->getSlopeShape();

    // TODO: there is an optimization opportunity when the bases have different
    // shapes: we can broadcast the base if the corresponding broadcast slope
    // dimension is empty (1) or if the slope is zero and the dimension size are
    // equal. E.g. LHS[BaseShape([1, 15]) SlopeShape[5, 1] Slope(0, 0)]
    // RHS[BaseShape([5, 15]), SlopeShape([1, 1])] the LHS base can be
    // broadcasted to match RHS base shape and continue propagating a valid
    // series.
    const TensorShape &NewBaseShape = LhsBaseShape;

    TensorShape NewSlopeShape = ToShape;
    NewSlopeShape.reduceDimensions(NewBaseShape.nonEmptyDims());

    // We have to expand when:
    // 1) the bases have different shapes
    // 2) we cannot combine the slopes w/ a broadcast
    // 3) if the rhs base dimension overlaps with lhs slope dimensions
    // 4) if the lhs base dimension overlaps with rhs slope dimensions
    // Cannot combine the bases; we have to expand
    if (LhsBaseShape != RhsBaseShape ||
        RhsBaseShape.bothNonEmptyDims(LhsSlopeShape).any() ||
        LhsBaseShape.bothNonEmptyDims(RhsSlopeShape).any())
      return {};

    std::function<Value *(unsigned)> LeftSlopeOp, RightSlopeOp;
    std::function<Value *(Value *, Value *)> IRBFun;

    // Handle common signed integer truncation pattern (unsigned -> signed):
    // R2 = shl R1, P; where P = (sizeof_bit(R1) - sizeof_bit(trunc int type))
    // R3 = ashr exact R2, P
    ConstantInt *SrAmount;
    ConstantInt *SlAmount;
    Value *SlVal;
    // AShr(Shl(Any, SlAmount), SrAmount)
    if (match(BinOp, m_AShr(m_Shl(m_Value(SlVal), m_ConstantInt(SlAmount)),
                            m_ConstantInt(SrAmount))) &&
        cast<AShrOperator>(BinOp)->isExact() && SrAmount == SlAmount) {
      // We allow this transformation only if the following ops undefine
      // behavior on overflow
      if (!all_of(BinOp->users(), [](User *U) {
            if (auto *OverflowBinop = dyn_cast<OverflowingBinaryOperator>(U))
              return OverflowBinop->hasNoSignedWrap() ||
                     OverflowBinop->hasNoUnsignedWrap();
            return false;
          }))
        return {};
      // The slopes are the one from the LHS of LhsShift; don't
      // compute them using right shift or we may truncate the slopes!
      auto SlOperandOpSeries = getOperandSeries(SlVal);
      if (SlOperandOpSeries.isValid() || SlOperandOpSeries.hasPotential()) {
        IRBFun = [&](Value *Lhs, Value *) { return Lhs; };
        LeftSlopeOp = [SlOperandOpSeries](unsigned idx) {
          return SlOperandOpSeries.LS->getSlope(idx);
        };
        RightSlopeOp = [](unsigned idx) { /* Not used */ return nullptr; };
      } else {
        return {};
      }
    } else if (ShlOperator *ShlOp = dyn_cast<ShlOperator>(BinOp)) {
      IRBFun = [&, ShlOp](Value *Lhs, Value *Rhs) {
        Value *NewShl = irBuilder.CreateShl(Lhs, Rhs, "ripple.LS.slope.shl",
                                            ShlOp->hasNoUnsignedWrap(),
                                            ShlOp->hasNoSignedWrap());
        return NewShl;
      };

      if (RhsSeries.LS->isScalarOrSplat()) {
        // (ax + b) << s => (a << s)x + (b << s); (<< s) <=> (* 2^x)
        LeftSlopeOp = [&](unsigned idx) { return LhsSeries.LS->getSlope(idx); };
        RightSlopeOp = [&](unsigned idx) { return RhsSeries.LS->getBase(); };
      } else {
        return {};
      }
    } else if (MulOperator *MulOp = dyn_cast<MulOperator>(BinOp)) {
      IRBFun = [&, MulOp](Value *Lhs, Value *Rhs) {
        return irBuilder.CreateMul(Lhs, Rhs, "ripple.LS.slope.mul",
                                   MulOp->hasNoUnsignedWrap(),
                                   MulOp->hasNoSignedWrap());
      };

      // We only allow multiplication by a scalar
      if (LhsSeries.LS->isScalarOrSplat()) {
        // s * (ax + b) => (s * a)x + (s * b)
        LeftSlopeOp = [&](unsigned idx) { return LhsSeries.LS->getBase(); };
        RightSlopeOp = [&](unsigned idx) {
          return RhsSeries.LS->getSlope(idx);
        };
      } else if (RhsSeries.LS->isScalarOrSplat()) {
        // (ax + b) * s => (a * s)x + (b * s)
        LeftSlopeOp = [&](unsigned idx) { return LhsSeries.LS->getSlope(idx); };
        RightSlopeOp = [&](unsigned idx) { return RhsSeries.LS->getBase(); };
      } else {
        return {};
      }
    } else if (SubOperator *SubOp = dyn_cast<SubOperator>(BinOp)) {
      // (ax + b) - (cx + d) => (a - c)x + (b - d)
      IRBFun = [&, SubOp](Value *Lhs, Value *Rhs) {
        return irBuilder.CreateSub(Lhs, Rhs, "ripple.LS.slope.sub",
                                   SubOp->hasNoUnsignedWrap(),
                                   SubOp->hasNoSignedWrap());
      };

      LeftSlopeOp = [&](unsigned idx) { return LhsSeries.LS->getSlope(idx); };
      RightSlopeOp = [&](unsigned idx) { return RhsSeries.LS->getSlope(idx); };
    } else if (AddOperator *AddOp = dyn_cast<AddOperator>(BinOp)) {
      IRBFun = [&, AddOp](Value *Lhs, Value *Rhs) {
        return irBuilder.CreateAdd(Lhs, Rhs, "ripple.LS.slope.add",
                                   AddOp->hasNoUnsignedWrap(),
                                   AddOp->hasNoSignedWrap());
      };

      // (ax + b) + (cx + d) => (a + c)x + (b + d)
      LeftSlopeOp = [&](unsigned idx) { return LhsSeries.LS->getSlope(idx); };
      RightSlopeOp = [&](unsigned idx) { return RhsSeries.LS->getSlope(idx); };
    } else {
      // Cannot pass through a LinearSeries
      return {};
    }

    assert(ToShape.rank() == NewSlopeShape.rank());
    // Construct the new slopes
    for (unsigned i = 0; i < ToShape.rank(); ++i) {
      Value *NewSlope = IRBFun(LeftSlopeOp(i), RightSlopeOp(i));
      NewSlopes.push_back(NewSlope);
      SlopeInstructions.insert(dyn_cast<Instruction>(NewSlope));
      setRippleShape(NewSlope, ScalarShape);
    }
    auto *LS = new LinearSeries(BinOp, NewBaseShape, NewSlopes, NewSlopeShape);
    return {LS, NewState};
  };

  auto processCast = [&](CastInst *CastI) -> ConstructedSeries {
    auto &CastOpSeries = OperandSeries[0];
    if (CastOpSeries.isNotASeries())
      return {};
    auto &SlopeShape = CastOpSeries.LS->getSlopeShape();
    for (unsigned i = 0; i < SlopeShape.rank(); ++i) {
      Value *CastOp =
          irBuilder.CreateCast(CastI->getOpcode(), CastOpSeries.LS->getSlope(i),
                               CastI->getType(), "ripple.LS.slope.cast");
      setRippleShape(CastOp, ScalarShape);
      SlopeInstructions.insert(dyn_cast<Instruction>(CastOp));
      NewSlopes.push_back(CastOp);
    }
    auto *LS = new LinearSeries(CastI, CastOpSeries.LS->getBaseShape(),
                                NewSlopes, SlopeShape);
    return {LS, CastOpSeries.getState()};
  };

  auto processUnaryOp = [&](UnaryOperator *UnOp) -> ConstructedSeries {
    auto &InSeries = OperandSeries[0];
    if (InSeries.isNotASeries())
      return {};

    // Cast first because it may affect the slope type
    if (CastInst *CastI = dyn_cast<CastInst>(UnOp))
      return processCast(CastI);

    // Scalar bypass
    if (InSeries.LS->isScalar()) {
      auto *LS =
          new LinearSeries(UnOp, InSeries.LS->getBaseShape(),
                           InSeries.LS->slopes(), InSeries.LS->getSlopeShape());
      return {LS, InSeries.getState()};
    }

    return {};
  };

  auto processGEP = [&](GetElementPtrInst *GEP) -> ConstructedSeries {
    auto &PointerSeries = OperandSeries[0];
    if (PointerSeries.isNotASeries())
      return {};
    const TensorShape &NewBaseShape = PointerSeries.LS->getBaseShape();
    CSState NewState = PointerSeries.getState();
    for (unsigned Idx = 0; Idx < GEP->getNumIndices(); ++Idx) {
      auto &IdxSeries =
          OperandSeries[Idx + (GEP->idx_begin() - GEP->op_begin())];
      if (IdxSeries.isNotASeries() ||
          IdxSeries.LS->getBaseShape() != NewBaseShape)
        return {};
      NewState = combineStatesBinaryOp(NewState, IdxSeries.getState());
    }
    TensorShape NewSlopeShape = ToShape;
    NewSlopeShape.reduceDimensions(NewBaseShape.nonEmptyDims());

    // Start w/ the pointer slope
    for (unsigned i = 0; i < NewSlopeShape.rank(); ++i) {
      Value *PtrSlope = PointerSeries.LS->getSlope(i);
      assert(PtrSlope->getType() == SlopeType);
      NewSlopes.push_back(PtrSlope);
    }
    SmallVector<Value *, 4> IndicesProcessed;
    for (unsigned Idx = 0; Idx < GEP->getNumIndices(); ++Idx) {
      LinearSeries *IdxSeries =
          OperandSeries[Idx + (GEP->idx_begin() - GEP->op_begin())].LS;
      Value *GEPIndexVal = *(GEP->idx_begin() + Idx);
      // Get the type we are indexing
      IndicesProcessed.push_back(GEPIndexVal);
      Type *IndexedType = GetElementPtrInst::getIndexedType(
          GEP->getSourceElementType(), IndicesProcessed);
      LLVM_DEBUG(dbgs() << "\tProcessing GEP index " << *GEPIndexVal
                        << " affecting type " << *IndexedType << "\n");
      // This should not happen by LLVM IR construction, but better guard
      // against it
      bool IndexingStructField =
          Idx > 0 && GetElementPtrInst::getIndexedType(
                         GEP->getSourceElementType(),
                         ArrayRef(IndicesProcessed).drop_back())
                         ->isStructTy();
      if (IndexingStructField && !IdxSeries->hasZeroSlopes())
        llvm_unreachable(
            "Ripple vector access to a structure field is undefined");
      // For pointers, the base is GEP() and the integer slope indexes a bytes
      // array.
      uint64_t IndexByteSize = DL.getTypeAllocSize(IndexedType);
      ConstantInt *IndexMultiple =
          ConstantInt::get(SlopeType, IndexByteSize, /*signed*/ false);
      for (unsigned SlopeIdx = 0; SlopeIdx < NewSlopeShape.rank(); ++SlopeIdx) {
        Value *IndexSlope = IdxSeries->getSlope(SlopeIdx);
        if (IndexSlope->getType() != SlopeType) {
          IndexSlope = irBuilder.CreateIntCast(IndexSlope, SlopeType,
                                               /*signed*/ false,
                                               "ripple.slope.gep.typefix");
          setRippleShape(IndexSlope, ScalarShape);
          SlopeInstructions.insert(dyn_cast<Instruction>(IndexSlope));
        }
        Value *SlopeInBytes = irBuilder.CreateMul(IndexSlope, IndexMultiple,
                                                  "ripple.slope.gep.inbytes");
        setRippleShape(SlopeInBytes, ScalarShape);
        SlopeInstructions.insert(dyn_cast<Instruction>(SlopeInBytes));
        NewSlopes[SlopeIdx] = irBuilder.CreateAdd(
            NewSlopes[SlopeIdx], SlopeInBytes, "ripple.LS.slope.gep.index");
        setRippleShape(NewSlopes[SlopeIdx], ScalarShape);
        SlopeInstructions.insert(dyn_cast<Instruction>(NewSlopes[SlopeIdx]));
      }
    }
    auto *LS = new LinearSeries(GEP, NewBaseShape, NewSlopes, NewSlopeShape);

    return {LS, NewState};
  };

  auto processPHINode = [&](PHINode *PHI) -> ConstructedSeries {
    LLVM_DEBUG(dbgs() << "Processing PHI for LS: " << *PHI << "\n");

    CSState NewState = CSState::ValidLinearSeries;
    // Get the base shape from any of the operands
    TensorShape NewBaseShape;
    assert(PHI->getNumIncomingValues() > 0);
    for (unsigned IncomingIdx = 0; IncomingIdx < PHI->getNumIncomingValues();
         ++IncomingIdx) {
      auto &OperandLS = OperandSeries[IncomingIdx];
      if (!OperandLS.isNotASeries()) {
        NewBaseShape = OperandLS.LS->getBaseShape();
      }
    }

    for (unsigned IncomingIdx = 0; IncomingIdx < PHI->getNumIncomingValues();
         ++IncomingIdx) {
      auto &OperandLS = OperandSeries[IncomingIdx];
      // When an operand is not a series, still try to potentially build the
      // PHI later
      if (OperandLS.isNotASeries()) {
        NewState =
            combineStatesBinaryOp(NewState, CSState::PotentialLinearSeries);
        continue;
      }
      // Valid keeps the state, any potential will demote
      NewState = combineStatesBinaryOp(NewState, OperandLS.getState());
      if (OperandLS.LS->getBaseShape() != NewBaseShape)
        return {};
    }
    if (NewState == CSState::NotASeries)
      return {};
    // We are creating a PHI for each Slope
    for (unsigned SlopeIdx = 0, EndIdx = ToShape.rank(); SlopeIdx < EndIdx;
         ++SlopeIdx) {
      PHINode *SlopePhi =
          irBuilder.CreatePHI(SlopeType, PHI->getNumIncomingValues(),
                              Twine(PHI->getName()) + ".ripple.slope.phi" +
                                  std::to_string(SlopeIdx));
      NewSlopes.push_back(SlopePhi);
      setRippleShape(SlopePhi, ScalarShape);
      SlopeInstructions.insert(SlopePhi);
      // Pre-populate for known LS
      for (unsigned IncomingIdx = 0; IncomingIdx < PHI->getNumIncomingValues();
           ++IncomingIdx) {
        auto &OperandLS = OperandSeries[IncomingIdx];
        // We can pre-populate the potential and valid LS
        if (!OperandLS.isNotASeries()) {
          SlopePhi->addIncoming(OperandLS.LS->getSlope(SlopeIdx),
                                PHI->getIncomingBlock(IncomingIdx));
        }
      }
    }

    // When the PHI is at the merge point of a vector branch, it will be
    // transformed into a series of select instruction. Don't create a linear
    // series when the mask cannot be applied to the base.
    if (auto *ImmDom = domTree.getNode(PHI->getParent())->getIDom())
      // PHIs always have an immediate dominator!
      if (Instruction *Terminator = ImmDom->getBlock()->getTerminator()) {
        auto &MaskShape = getRippleShape(Terminator);
        if (NewBaseShape.requiredSplat(MaskShape).any())
          return {};
      }

    TensorShape NewSlopeShape = ToShape;
    NewSlopeShape.reduceDimensions(NewBaseShape.nonEmptyDims());
    auto *LS = new LinearSeries(PHI, NewBaseShape, NewSlopes, NewSlopeShape);
    return {LS, NewState};
  };

  auto processRippleBroadcast =
      [&](IntrinsicInst *BcastOp) -> ConstructedSeries {
    // A broadcast can always be representat as a LS!

    auto &InSeries = OperandSeries[2];

    Value *BcastedVal = BcastOp->getOperand(2);

    // Constants are broadcasted to a splat-series, that's what we want!
    if (!InSeries.isNotASeries() && isa<Constant>(BcastedVal))
      return InSeries;

    // Let Arguments be re-processed (for specialization)
    if (!InSeries.isNotASeries() && isa<Argument>(BcastedVal))
      InSeries = {};

    if (!InSeries.isNotASeries())
      for (unsigned i = 0; i < ToShape.rank(); ++i)
        NewSlopes.push_back(InSeries.LS->getSlope(i));
    else
      for (unsigned i = 0; i < ToShape.rank(); ++i)
        NewSlopes.push_back(ConstantSlopeOf(0));

    Value *NewBase =
        InSeries.isNotASeries() ? BcastedVal : InSeries.LS->getBase();
    const TensorShape &NewBaseShape = InSeries.isNotASeries()
                                          ? getRippleShape(NewBase)
                                          : InSeries.LS->getBaseShape();

    auto BroadcastDimSet = InSeries.isNotASeries()
                               ? NewBaseShape.requiredSplat(ToShape)
                               : InSeries.LS->getShape().requiredSplat(ToShape);

    TensorShape NewSlopesShape = ToShape;
    NewSlopesShape.keepDimensions(BroadcastDimSet);
    LinearSeries *LS =
        new LinearSeries(NewBase, NewBaseShape, NewSlopes, NewSlopesShape);

    CSState NewState = InSeries.isNotASeries() ? CSState::ValidLinearSeries
                                               : InSeries.getState();
    return {LS, NewState};
  };

  auto IP = irBuilder.saveIP();
  irBuilder.SetInsertPoint(I);

  ConstructedSeries CS;
  if (IntrinsicInst *RippleIndexInst = intrinsicWithId(
          dyn_cast<Instruction>(I), {Intrinsic::ripple_block_index})) {
    CS = processRippleBlockIndex(RippleIndexInst);
  } else if (IntrinsicInst *RippleGetSizeInst = intrinsicWithId(
                 dyn_cast<Instruction>(I), {Intrinsic::ripple_block_getsize})) {
    CS = processRippleGetSize(RippleGetSizeInst);

  } else if (GetElementPtrInst *GEP = dyn_cast<GetElementPtrInst>(I)) {
    CS = processGEP(GEP);

  } else if (PHINode *PHI = dyn_cast<PHINode>(I)) {
    CS = processPHINode(PHI);

  } else if (UnaryOperator *UnOp = dyn_cast<UnaryOperator>(I)) {
    CS = processUnaryOp(UnOp);

  } else if (BinaryOperator *BinOp = dyn_cast<BinaryOperator>(I)) {
    CS = processBinOp(BinOp);

  } else if (IntrinsicInst *RippleBroadcast = rippleBroadcastIntrinsic(I)) {
    CS = processRippleBroadcast(RippleBroadcast);
  }

  // We can always restart a new series with a vector base since the type is
  // integral
  if (CS.isNotASeries()) {
    NewSlopes.clear();
    for (unsigned i = 0; i < ToShape.rank(); ++i)
      NewSlopes.push_back(ConstantSlopeOf(0));
    auto *LS = new LinearSeries(I, ToShape, NewSlopes, ScalarShape);
    CS = {LS, CSState::ValidLinearSeries};
  }

  irBuilder.restoreIP(IP);

  if (!CS.isNotASeries()) {
    assert(!LsCache.Valid.contains(I) && !LsCache.Potential.contains(I) &&
           "Inserting a value already in the cache");
    switch (CS.getState()) {
    case CSState::ValidLinearSeries:
      LsCache.Valid.insert({I, CS.LS});
      CS.LS->Retain();
      break;
    case CSState::PotentialLinearSeries:
      LsCache.Potential.insert({I, CS.LS});
      CS.LS->Retain();
      break;
    default:
      llvm_unreachable("State should either be Valid or Potential");
    }
  }
  return CS;
}

Ripple::CSState Ripple::combineStatesBinaryOp(CSState S1, CSState S2) {
  switch (S1) {
  case CSState::NotASeries:
  case CSState::PotentialLinearSeries:
    if (S2 == CSState::NotASeries)
      return CSState::NotASeries;
    else
      return S1;
  default:
    return S2;
  }
}

Ripple::ConstructedSeries Ripple::getCachedSeries(const Instruction *I) const {
  if (!I)
    return {};
  LinearSeries *LS = nullptr;
  LS = LsCache.Valid.lookup(I);
  if (LS)
    return {LS, CSState::ValidLinearSeries};
  LS = LsCache.Potential.lookup(I);
  if (LS)
    return {LS, CSState::PotentialLinearSeries};
  return {};
}

void Ripple::simplifySlopes() {
  DenseSet<Instruction *> InstructionsToSimplify;
  for (auto &[_, Ls] : LsCache.Valid) {
    const TensorShape &SlopeShape = Ls->getSlopeShape();
    for (unsigned SlopeIdx = 0; SlopeIdx < SlopeShape.rank(); SlopeIdx++) {
      if (SlopeShape[SlopeIdx] < 2) {
        Value *CurrentSlope = Ls->getSlope(SlopeIdx);
        Constant *Replacement = ConstantInt::get(CurrentSlope->getType(), 0);
        Ls->setSlope(SlopeIdx, Replacement);
        if (Instruction *SlopeInst = dyn_cast<Instruction>(CurrentSlope)) {
          LLVM_DEBUG(dbgs() << "Replacing slope " << *CurrentSlope << " by "
                            << *Replacement << "\n");
          InstructionsToSimplify.insert(SlopeInst);
        }
      }
    }
  }
  for (auto *I : InstructionsToSimplify) {
    Constant *Replacement = ConstantInt::get(I->getType(), 0);
    auto Iterator = I->getIterator();
    invalidateRippleDataFor(I);
    ReplaceInstWithValue(Iterator, Replacement);
  }
}

iterator_range<User::const_op_iterator>
Ripple::vectorizableOperands(const Instruction *I) {
  auto Begin = I->op_begin();
  auto End = I->op_end();
  if (const BranchInst *BrInst = dyn_cast<BranchInst>(I)) {
    // For branches, we skip the basic blocks
    if (BrInst->isConditional())
      End = std::next(Begin);
    else
      Begin = End;
  } else if (isa<SwitchInst>(I))
    // We are only interested in the switch's condition
    End = std::next(Begin);
  else if (rippleBlockIntrinsics(I))
    // No operand to vectorize for block intrinsics
    Begin = End;
  else if (rippleBroadcastIntrinsic(I)) {
    // The value being broadcasted is the third argument
    Begin = std::next(cast<CallInst>(I)->arg_begin(), 2);
    End = std::next(Begin);
  } else if (rippleSliceIntrinsic(I)) {
    // The value being sliced is the first argument
    Begin = cast<CallInst>(I)->arg_begin();
    End = std::next(Begin);
  } else if (rippleReduceIntrinsics(I)) {
    // The value being reduced is the second argument
    Begin = std::next(cast<CallInst>(I)->arg_begin());
    End = std::next(Begin);
  } else if (rippleShuffleIntrinsics(I)) {
    // The value being shuffled is the first argument
    Begin = cast<CallInst>(I)->arg_begin();
    End = std::next(Begin);
    // Pair Shuffle
    if (!cast<ConstantInt>(cast<CallInst>(I)->getArgOperand(2))->isZero())
      End = std::next(End);
  } else if (const CallInst *CallI = dyn_cast<CallInst>(I)) {
    // For other call instructions, skip the Function operand
    Begin = CallI->arg_begin();
    End = CallI->arg_end();
  }
  return make_range(Begin, End);
}

iterator_range<User::op_iterator> Ripple::vectorizableOperands(Instruction *I) {
  iterator_range<User::const_op_iterator> ConstRange =
      vectorizableOperands(const_cast<const Instruction *>(I));
  return make_range(const_cast<User::op_iterator>(ConstRange.begin()),
                    const_cast<User::op_iterator>(ConstRange.end()));
}

// This function almost duplicates the one in HexagonTargetTransformInfo.h
// and it is recommended to keep them in sync or to introduce a common method.
static bool isUsableVectorType(TargetMachine *TM, const Function &F,
                               Type *VecType) {
  const auto *TLI = TM->getSubtargetImpl(F)->getTargetLowering();
  const auto &DL = F.getDataLayout();

  // Unable to reason about it.
  if (!TLI)
    return true;

  EVT EVTType = TLI->getValueType(DL, VecType, true);
  if (!EVTType.isSimple()) {
    LLVM_DEBUG(dbgs() << "  Complex vector type cannot be supported(" << EVTType
                      << ")\n");
    return false;
  }

  // Check if the type is legal
  if (TLI->isTypeLegal(EVTType.getSimpleVT()))
    return true;

  // Maybe we can trivially promote or widen it?
  EVT TransformedType =
      TLI->getTypeToTransformTo(VecType->getContext(), EVTType);
  if (TransformedType.isSimple() && TLI->isTypeLegal(TransformedType)) {
    LLVM_DEBUG(dbgs() << "  Vector type(" << EVTType
                      << ") Can be used if promoted/widened to("
                      << TransformedType << ")\n");
    return true;
  }
  return false;
}

/// This is intended as a comprehensive feedback mechanism to user to help with
/// proper use of Ripple and data types that might not be handled by a specific
/// back end.
bool Ripple::analyzeTensorShape(const TensorShape &TS, Instruction *I,
                                Value *Data, Value *Address) {
  if (!AnalyzeRipple || !TM)
    return true;

  // We are starting with LD/ST verification for now.
  if (!isa<LoadInst>(I) && !isa<StoreInst>(I))
    return true;

  if (!Data || !Address)
    return true;
  LLVM_DEBUG(dbgs() << "analyzeTensorShape TS[" << TS << "] for : " << *I
                    << "\n  DataTy : " << *Data->getType() << "\n");
  if (!isUsableVectorType(TM, *I->getParent()->getParent(), Data->getType())) {
    // Report a warning and continue
    std::string ErrStr;
    raw_string_ostream OSS(ErrStr);
    OSS << "Tensor shape[" << TS << "] will require type(" << *Data->getType()
        << ") and will be scalarized by the back end\n";
    OSS.flush();
    SMDiagnostic Diag("", SourceMgr::DiagKind::DK_Warning, ErrStr);
    Diag.print("Verify Tensor Shape", errs());
    /*return false;*/
  }
  return true;
}

Expected<TensorShape>
Ripple::inferShapeFromOperands(const Instruction *I, bool AllowPartialPhi,
                               bool &RequiresWaitingForSpecialization) {
  RequiresWaitingForSpecialization = false;
  auto partialAliasCheck = [&](const Instruction *I,
                               MemoryLocation &MemLoc) -> Error {
    auto MayAliasAlloca =
        aliasesWithAlloca(MemLoc, PromotableAlloca, AliasResult::MayAlias)
            .second;
    auto PartialAliasAlloca =
        aliasesWithAlloca(MemLoc, PromotableAlloca, AliasResult::PartialAlias)
            .second;
    if (MayAliasAlloca || PartialAliasAlloca) {
      DiagnosticInfoRippleWithLoc DI(
          DS_Error, F, sanitizeRippleLocation(I),
          "Ripple cannot get a precise tensor shape shape for this "
          "instruction because it may alias with a local variable (alloca)."
          " You may avoid such situations by avoiding taking the address of "
          "local variables.");
      F.getContext().diagnose(DI);
      return createStringError(
          inconvertibleErrorCode(),
          "Cannot get precise alloca shape  cannot be promoted");
    }
    return Error::success();
  };

  // Handle Ripple intrinsics calls that have special shape semantics
  if (const IntrinsicInst *II = rippleBlockIntrinsics(I)) {
    switch (II->getIntrinsicID()) {
    case Intrinsic::ripple_block_index: {
      // Get the BS shape
      auto *ShapeII = getBlockShapeIntrinsic(II->getArgOperandUse(0));
      // Checked by checkBlockShapeUsage
      assert(ShapeII);
      PEIdentifier ProcElem = *getConstantOperandValue(ShapeII, 0);
      auto Dimension = *getConstantOperandValue(II, 1);
      auto IndexShape = setShapeToTensorShape(ShapeII);

      // And keep only the dimension asked by the ID call
      BitVector DimToKeep(tensorRank());
      if (Dimension < PERank(ProcElem)) {
        auto TensorIdx = rippleToTensor({ProcElem, Dimension});
        DimToKeep.set(TensorIdx);
      }
      IndexShape.keepDimensions(DimToKeep);
      return IndexShape;
    }
    case Intrinsic::ripple_block_setshape:
    case Intrinsic::ripple_block_getsize:
      return ScalarShape;
    default:
      llvm_unreachable("Unimplemented case");
    }
  } else if (const IntrinsicInst *RippleBroadcast =
                 rippleBroadcastIntrinsic(I)) {
    Value *BroadcastingValue = RippleBroadcast->getArgOperand(2);
    return computeRippleShapeForBitsetIntrinsic(
        RippleBroadcast,
        getRippleShape(BroadcastingValue, /* ShapePropagation */ true));
  } else if (const IntrinsicInst *RippleRed = rippleReduceIntrinsics(I)) {
    Value *ReducedValue = RippleRed->getArgOperand(1);
    return computeRippleShapeForBitsetIntrinsic(
        RippleRed, getRippleShape(ReducedValue, /* ShapePropagation */ true));
  } else if (const IntrinsicInst *RippleSlice = rippleSliceIntrinsic(I)) {
    Value *Slicee = RippleSlice->getArgOperand(0);
    const TensorShape &SliceeShape =
        getRippleShape(Slicee, /* ShapePropagation */ true);

    if (SliceeShape.isScalar())
      return ScalarShape;

    TensorShape SlicedShape = SliceeShape;
    unsigned NArgs = RippleSlice->arg_size();
    BitVector toReduce(NArgs - 1);
    unsigned SetVectorId = lastVectorIdx(RippleSlice, SliceeShape,
                                         /*SpecialArg*/ 0, "slicing");
    if (SetVectorId == tensorRank()) // no non-trivial vector PE block found
      return SlicedShape;
    auto [VectorPE, _] = tensorToRipple(SetVectorId);
    for (unsigned Idx = 1; Idx < NArgs; ++Idx) {
      ConstantInt *SliceArg = cast<ConstantInt>(RippleSlice->getOperand(Idx));
      if (SliceArg->getSExtValue() >= 0) {
        if (Idx - 1 < PERank(VectorPE)) {
          auto TensorIdx = rippleToTensor({VectorPE, Idx - 1});
          toReduce.set(TensorIdx);
        }
      }
    }
    SlicedShape.reduceDimensions(toReduce);
    return SlicedShape;
  } else if (isa<DbgInfoIntrinsic>(I)) {
    return ShapeIgnoredByRipple;
  } else if (auto *Load = dyn_cast<LoadInst>(I)) {
    auto MemLoc = MemoryLocation::get(Load);

    // Check and report error to the user when the tensor shape cannot be
    // precisely inferred
    if (Error Err = partialAliasCheck(Load, MemLoc))
      return std::move(Err);

    // Checks that this
    if (aliasesWithPromotableAlloca(MemLoc)) {
      TensorShape LoadShape = ScalarShape;
      Error Err = Error::success();
      visitAllClobberingInstructions(
          cast<MemoryUse>(MemSSA.getMemoryAccess(Load)),
          [&](Instruction *Clobbering) -> bool {
            // We may be visiting the values early because of MemoryPHI so don't
            // crash because of getRippleShape
            if (AllowPartialPhi && InstructionRippleShapes.find(Clobbering) ==
                                       InstructionRippleShapes.end())
              return true;
            const TensorShape &IShape =
                getRippleShape(Clobbering, /* ShapePropagation */ true);

            auto NewShape = combineShapeBcastWithErrorReporting(
                LoadShape, IShape, "Ripple failed to broadcast the instruction",
                sanitizeRippleLocation(Load), "with the shape coming from  ",
                sanitizeRippleLocation(Clobbering));
            if (!NewShape) {
              // This is needed to check success before overriding
              if (Err)
                llvm_unreachable("Expected Success");
              Err = NewShape.takeError();
              return false;
            }
            std::swap(*NewShape, LoadShape);
            return true;
          });

      if (Err)
        return std::move(Err);
      LLVM_DEBUG(dbgs() << "Load shape final " << LoadShape << "\n");
      return LoadShape;
    }
  } else if (auto *Store = dyn_cast<StoreInst>(I)) {
    auto MemLoc = MemoryLocation::get(Store);

    // Check and report error to the user when the tensor shape cannot be
    // precisely inferred
    if (Error Err = partialAliasCheck(Store, MemLoc))
      return std::move(Err);

    if (aliasesWithPromotableAlloca(MemLoc)) {
      TensorShape StoreShape =
          getRippleShape(Store->getValueOperand(), /*ShapePropagation*/ true);
      Error Err = Error::success();
      visitAllInstructionsBeingClobberedBy(
          cast<MemoryDef>(MemSSA.getMemoryAccess(Store)),
          [&](Instruction *Clobbered) -> bool {
            if (AllowPartialPhi && InstructionRippleShapes.find(Clobbered) ==
                                       InstructionRippleShapes.end())
              return true;
            const TensorShape &ClobberedShape =
                getRippleShape(Clobbered, /* ShapePropagation */ true);

            auto NewShape = combineShapeBcastWithErrorReporting(
                StoreShape, ClobberedShape,
                "Ripple failed to broadcast the instruction",
                sanitizeRippleLocation(Store), "with the shape coming from  ",
                sanitizeRippleLocation(Clobbered));
            if (!NewShape) {
              // This is needed to check success before overriding
              if (Err)
                llvm_unreachable("Expected Success");
              Err = NewShape.takeError();
              return false;
            }
            std::swap(*NewShape, StoreShape);
            return true;
          });

      if (Err)
        return std::move(Err);
      LLVM_DEBUG(dbgs() << "Store shape final " << StoreShape << "\n");
      return StoreShape;
    }
  } else if (auto *Call = dyn_cast<CallInst>(I)) {
    if (rippleVectorizeCall(*Call)) {
      // Here we don't check for masks but we'll raise an error later if the
      // function cannot be masked and requires one.
      // The reason why is that we can't fall-back to non-external function call
      // once we selected one because the shape may be different
      if (auto *ExternalRippleMatch =
              findExternalRippleFunctionFor(Call, /* RequiresMask */ false)) {
        auto CallShape = ExternalRippleMatch->returnTensorShape(Call, *this);
        if (!CallShape) {
          // This can only fail if the call is element-wise and we cannot
          // combine the shape of the operands for the call, otherwise it is the
          // shape of the returned value
          assert(ExternalRippleMatch->isElementWiseFunction());
          std::string ErrMsg;
          llvm::raw_string_ostream RSO(ErrMsg);
          RSO << "call to an element-wise external ripple function with "
                 "missmatching argument shapes: ";
          for (unsigned i = 0u, e = Call->arg_size(); i < e; ++i) {
            RSO << " Argument " << i << " "
                << getRippleShape(Call->getArgOperand(i));
          }
          RSO.flush();
          DiagnosticInfoRippleWithLoc DI(DS_Error, F,
                                         sanitizeRippleLocation(Call), ErrMsg);
          F.getContext().diagnose(DI);
          return CallShape.takeError();
        }
        return *CallShape;
      } else if (canBeSpecialized(Call->getCalledFunction())) {
        auto [Spec, RetShape] = getRippleSpecializationFor(*Call);
        if (Spec) {
          if (isPendingRippleSpecialization(*Spec))
            LLVM_DEBUG(dbgs()
                       << "Pending specialization " << Spec->getName() << "\n");
          if (isPendingRippleSpecialization(*Spec) ||
              shouldWaitForVoidReturnSpecialization(*Spec)) {
            RequiresWaitingForSpecialization = true;
            return ScalarShape;
          }
          return RetShape;
        } else {
          // Ask for Specialization and stop shape propagation
          RequiresWaitingForSpecialization = true;
          if (requestSpecializationFor(*Call))
            return ScalarShape;
        }
      }
    }
  }

  // We assume a broadcast semantics of instruction operands
  TensorShape InstructionShape = ScalarShape;
  bool IsPhiNode = isa<PHINode>(I);
  for (auto &Op : vectorizableOperands(I)) {
    // This is only useful when propagating the shapes initially, when PHI
    // nodes have non-processed input instruction shapes
    if (IsPhiNode && AllowPartialPhi && isa<Instruction>(Op) &&
        InstructionRippleShapes.find(cast<Instruction>(Op)) ==
            InstructionRippleShapes.end())
      continue;
    auto &OperandShape = getRippleShape(Op, /* ShapePropagation */ true);
    auto NewShape = combineShapeBcastWithErrorReporting(
        InstructionShape, OperandShape,
        "Ripple failed to broadcast the instruction", sanitizeRippleLocation(I),
        "with the shape coming from the operand " +
            std::to_string(Op.getOperandNo()),
        isa<Instruction>(Op) ? sanitizeRippleLocation(cast<Instruction>(Op))
                             : DebugLoc());
    RETURN_UNEXPECTED(NewShape);
    std::swap(InstructionShape, *NewShape);
  }
  return InstructionShape;
}

void LinearSeries::print(raw_ostream &O) const {
  O << "LinearSeries[";
  O << "\n  Shape[" << getShape() << "]";
  O << "\n  Base[" << *Base << "]";
  O << "\n  BaseShape[" << baseShape << "]";
  O << "\n  Slopes[";
  for (unsigned i = slopeShape.rank() - 1; i < slopeShape.rank(); --i) {
    if (i < slopeShape.rank() - 1)
      O << ", ";
    O << *SlopeValues[i];
  }
  O << "]";
  O << "\n  SlopeShape[" << slopeShape << "]\n]";
}

bool Ripple::hasValidLinearSeriesRoots(LinearSeries *LS) const {
  LLVM_DEBUG(dbgs() << "Checking Validity of " << *LS << "\n");
  SmallPtrSet<Instruction *, 32> AlreadyProcessed;
  std::queue<Instruction *> WorkList;
  // Constant and Argument linear series are valid, only check instructions
  if (Instruction *BaseInst = dyn_cast<Instruction>(LS->getBase())) {
    WorkList.push(BaseInst);
  }
  while (!WorkList.empty()) {
    Instruction *ToProcess = WorkList.front();
    WorkList.pop();
    if (AlreadyProcessed.contains(ToProcess))
      continue;
    AlreadyProcessed.insert(ToProcess);
    LLVM_DEBUG(dbgs() << "  Checking root " << *ToProcess << "\n");
    auto CS = getCachedSeries(ToProcess);
    if (CS.isNotASeries()) {
      LLVM_DEBUG(dbgs() << "Found non-series parent: " << *ToProcess << "\n");
      return false;
    }
    for (auto &Operand : ToProcess->operands()) {
      if (Instruction *OperandInst = dyn_cast<Instruction>(Operand)) {
        WorkList.push(OperandInst);
      }
    }
  }
  return true;
}

void Ripple::clearValidSerie(const Instruction *I) {
  if (!I)
    return;
  auto It = LsCache.Valid.find(I);
  if (It != LsCache.Valid.end()) {
    It->getSecond()->Release();
    LsCache.Valid.erase(It);
  }
}

void Ripple::clearPotentialSeries() {
  DenseSet<Instruction *> ToPoison;
  for (auto &[_, LS] : LsCache.Potential) {
    // Remove introduced slopes for unused LS
    for (auto &Slope : LS->slopes()) {
      if (Instruction *I = dyn_cast<Instruction>(&*Slope)) {
        invalidateRippleDataFor(I);
        ToPoison.insert(I);
      }
    }
    assert(LS->UseCount() == 1);
    LS->Release();
  }
  LsCache.Potential.clear();
  for (auto *I : ToPoison) {
    auto IBBit = I->getIterator();
    ReplaceInstWithValue(IBBit, PoisonValue::get(I->getType()));
  }
}

void Ripple::clearLinearSeriesCache() {
  clearPotentialSeries();
  for (auto &[_, LS] : LsCache.Valid) {
    assert(LS->UseCount() == 1);
    LS->Release();
  }
  LsCache.Valid.clear();
  LsCache.GeneratedSeries.clear();
}

Value *Ripple::instantiateLinearSeries(const LinearSeries *LS,
                                       bool UseLSCache) {
  Value *CachedValue = LsCache.GeneratedSeries.lookup(LS);
  if (UseLSCache && CachedValue)
    return CachedValue;
  LLVM_DEBUG(dbgs() << "Instantiating " << *LS << "\n");

  const TensorShape &LinearSeriesShape = LS->getShape();
  // Unused RepShape when assert is gone
  [[maybe_unused]] auto [BaseReplacement, RepShape] =
      replacementValueAndShape(LS->getBase());
  // Alloca have shape-shifting capabilities
  assert(*RepShape == LS->getBaseShape() ||
         (isa<AllocaInst>(BaseReplacement) && *RepShape >= LS->getBaseShape()));

  Value *Series = nullptr;
  if (LS->getBase()->getType()->getScalarType()->isPointerTy() &&
      LS->getBaseShape().isScalar() && !LS->getSlopeShape().isScalar()) {
    // Use the fact that GEP auto-splats scalar bases to keep them as scalar
    // (helps hardware scatter/gather needing a base ptr + offset vector)
    auto SlopeValue = buildLinearSeriesSlope(LS);
    Series =
        irBuilder.CreateGEP(irBuilder.getInt8Ty(), LS->getBase(), {SlopeValue});
  } else {
    auto BaseBcast =
        tensorBcast(BaseReplacement, LS->getBaseShape(), LinearSeriesShape);
    if (!BaseBcast)
      report_fatal_error("Broadcast failure during codegen");

    // Return the Broadcast of the base when the slope is 0 or absent
    if (LS->hasZeroSlopes()) {
      Series = *BaseBcast;
    } else {
      setRippleShape(*BaseBcast, LinearSeriesShape);
      // Create the Slope
      auto SlopeValue = buildLinearSeriesSlope(LS);

      if (LS->getBase()->getType()->getScalarType()->isPointerTy()) {
        // Use a GEP to compute the final vector of addresses
        Series = irBuilder.CreateGEP(irBuilder.getInt8Ty(), *BaseBcast,
                                     {SlopeValue});
      } else {
        LLVM_DEBUG(dbgs() << "Adding base " << **BaseBcast << "\n\tto slope "
                          << *SlopeValue << "\n");
        Series =
            irBuilder.CreateBinOp(Instruction::Add, *BaseBcast, SlopeValue);
      }
    }
  }
  Series->setName(Twine(LS->getBase()->getName()) + ".ripple.LS.instance");
  LLVM_DEBUG(dbgs() << "Instantiated Linear Series: " << *LS << " as "
                    << *Series << "\n");
  setRippleShape(Series, LinearSeriesShape);
  if (UseLSCache)
    LsCache.GeneratedSeries.insert({LS, Series});
  return Series;
}

Value *Ripple::instantiateCachedSeries(ConstructedSeries &CS,
                                       Instruction *AfterI) {
  auto IP = irBuilder.saveIP();
  setInsertPointAfter(irBuilder, AfterI);
  irBuilder.SetCurrentDebugLocation(AfterI->getDebugLoc());
  auto SeriesVal = instantiateLinearSeries(CS.LS);
  irBuilder.restoreIP(IP);
  return SeriesVal;
}

Value *Ripple::getCachedInstantiationFor(const Instruction *I) const {
  auto CS = getCachedSeries(I);
  if (CS)
    return LsCache.GeneratedSeries.lookup(CS.LS);
  else
    return nullptr;
}

bool Ripple::setRippleShape(const Value *V, const TensorShape &Shape) {
  return setRippleShape(dyn_cast_if_present<Instruction>(V), Shape);
}

bool Ripple::setRippleShape(const Instruction *I, const TensorShape &Shape) {
  assert(Shape.rank() == tensorRank());
  if (!I)
    return false;
  auto inserted = InstructionRippleShapes.insert(std::make_pair(I, Shape));
  // We modify if the shape changed
  if (!inserted.second && inserted.first->second != Shape) {
    inserted.first->second = Shape;
    if (intrinsicWithId(I, {Intrinsic::ripple_block_setshape}))
      llvm_unreachable("Ripple block shape are set in stone!");
    return true;
  }
  assert(inserted.first->second.rank() == tensorRank());
  return inserted.second;
}

void Ripple::invalidateRippleDataFor(const Value *V) {
  auto *I = const_cast<Instruction *>(dyn_cast_if_present<Instruction>(V));
  if (!I)
    return;
  InstructionRippleShapes.erase(I);
  // Linear series
  SlopeInstructions.erase(I);
  clearValidSerie(I);

  // If-convert
  ToSkipMaskingWhenIfConvert.erase(I);

  // Type-specific
  if (auto *Select = dyn_cast<SelectInst>(I))
    SelectToMaskWhenIfConvert.erase(Select);
  else if (auto *Call = dyn_cast<CallInst>(I))
    MaskedCalls.erase(Call);
  else if (auto *Alloca = dyn_cast<AllocaInst>(I))
    PromotableAlloca.erase(Alloca);
}

bool Ripple::isRippleIntrinsics(const Instruction *I) {
  return rippleBlockIntrinsics(I) || rippleReduceIntrinsics(I) ||
         rippleShuffleIntrinsics(I) || rippleBroadcastIntrinsic(I) ||
         rippleSliceIntrinsic(I);
}

DebugLoc Ripple::sanitizeRippleLocation(const Instruction *I) {
  if (!I)
    return DebugLoc();
  if (Ripple::isRippleIntrinsics(I))
    return stripInliningFromDebugLoc(I->getDebugLoc());
  else
    return I->getDebugLoc();
}

Value *Ripple::reduceBcastMaskToShape(Value *Mask, const TensorShape &MaskShape,
                                      const TensorShape &ExpectedShape) {
  auto ReductionDims =
      MaskShape.reductionDimensionsBeforeBroadcast(ExpectedShape);

  if (ReductionDims.any()) {
    TensorShape ReducedShape = MaskShape;
    ReducedShape.reduceDimensions(ReductionDims);
    // Neutral reduction of mask is a false mask
    Value *ReducedMask = genMultiDimReduction(Intrinsic::vp_reduce_or, Mask,
                                              MaskShape, ReductionDims);
    LLVM_DEBUG(dbgs() << "Mask reduced to " << ReducedShape << " is "
                      << *ReducedMask << "\n");
    auto BcastMask = tensorBcast(ReducedMask, ReducedShape, ExpectedShape);
    // Checked by checkVectorBranch
    if (!BcastMask)
      report_fatal_error("Mask Broadcast failure");
    return *BcastMask;
  } else {
    // The tensorBcast diagnostic has the same info as in this function
    auto BcastMask = tensorBcast(Mask, MaskShape, ExpectedShape);
    if (!BcastMask)
      report_fatal_error("Mask Broadcast failure");
    return *BcastMask;
  }
}

bool Ripple::allInstructionsHaveRippleShapes() const {
  unsigned InstructionCount = 0;
  for (const auto &Inst : instructions(F)) {
    auto Shape = InstructionRippleShapes.find(&Inst);
    InstructionCount++;
    if (Shape == InstructionRippleShapes.end()) {
      LLVM_DEBUG(dbgs() << "Found instruction w/o a ripple shape:" << Inst
                        << "\n");
      return false;
    }
  }
  if (InstructionCount != InstructionRippleShapes.size()) {
    LLVM_DEBUG(dbgs() << "InstructionCount: " << InstructionCount
                      << "\n  Mapping size: " << InstructionRippleShapes.size()
                      << "\n");
    assert(InstructionCount == InstructionRippleShapes.size() &&
           "Mapping to instructions not part of the function");
  }
  return true;
}

bool Ripple::simplifyFunction() {
  bool SimplifiedAny = false;
  for (auto &I : make_early_inc_range(instructions(F))) {
    Instruction *TryToSimplify = &I;
    // This process does not produce new instruction, it will fold instruction
    // and return a simpler form if possible or nullptr if it cannot
    Value *Simplified = simplifyInstruction(TryToSimplify, SQ);
    if (Simplified != nullptr) {
      LLVM_DEBUG(dbgs() << "Simplified " << *TryToSimplify << " with "
                        << *Simplified << "\n");

      // Replace TryToSimplify by Simplified
      invalidateRippleDataFor(TryToSimplify);
      auto TryToSimplifyIt = TryToSimplify->getIterator();
      ReplaceInstWithValue(TryToSimplifyIt, Simplified);
      SimplifiedAny = true;
    }
  }
  return SimplifiedAny;
}

bool LinearSeries::hasZeroSlopes() const {
  return getSlopeShape().isScalar() || all_of(slopes(), [](Value *V) {
           if (Constant *C = dyn_cast<Constant>(V))
             return C->isZeroValue();
           return false;
         });
}

bool LinearSeries::isScalarOrSplat() const {
  // Splat only requires that all slopes be zero
  return getBaseShape().isScalar() && hasZeroSlopes();
}

bool Ripple::hasNoVectorDimension() const {
  return none_of(idTypes.begin(), idTypes.end(),
                 [](auto &entry) { return entry.second == VectorDimension; });
}

Error Ripple::checkRippleBlockIntrinsics(IntrinsicInst *I) {
  if (I->getIntrinsicID() == Intrinsic::ripple_block_getsize ||
      I->getIntrinsicID() == Intrinsic::ripple_block_index) {
    auto DimensionIdx = *getConstantOperandValue(I, 1);
    if (DimensionIdx >= RippleIntrinsicsMaxDims) {
      std::string ErrMsg;
      {
        raw_string_ostream RSO(ErrMsg);
        RSO << "the requested dimension index (" << DimensionIdx
            << ") exceeds the number of dimensions supported by Ripple; "
               "supported values are "
               "in the range [0, "
            << RippleIntrinsicsMaxDims - 1 << "] per block shape";
      }
      DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(I),
                                     ErrMsg);
      F.getContext().diagnose(DI);
      return createStringError(inconvertibleErrorCode(),
                               "Block shape index OOB");
    }
  }

  // TODO: when adding support for other vector "kinds" (e.g., SVE, SME) we will
  // have to check that the tensor shapes with vector.
  return Error::success();
}

Error Ripple::checkRippleReductionIntrinsics(IntrinsicInst *I) {
  // The relevant checks and warnings are already processed by
  // computeRippleReductionShape during shape-propagation
  return Error::success();
}

Error Ripple::checkRippleShuffleIntrinsics(IntrinsicInst *I) {
  Function *ShuffleFunc = dyn_cast<Function>(I->getArgOperand(3));

  bool IsPairShuffle =
      !cast<ConstantInt>(cast<CallInst>(I)->getArgOperand(2))->isZero();

  if (!ShuffleFunc) {
    DiagnosticInfoRippleWithLoc DI(
        DS_Error, F, sanitizeRippleLocation(I),
        "the ripple shuffle instruction expects a function (or lambda) for the "
        "index-mapping argument");
    F.getContext().diagnose(DI);
    return createStringError(
        inconvertibleErrorCode(),
        "Shuffle index mapping function is not a function");
  }

  // Check that the index-mapping function prototype matches what we expect
  FunctionType *ShuffleFuncType = ShuffleFunc->getFunctionType();
  if (!ShuffleFuncType->getReturnType()->isIntegerTy() ||
      ShuffleFuncType->getNumParams() != 2 ||
      !ShuffleFuncType->getParamType(0)->isIntegerTy() ||
      !ShuffleFuncType->getParamType(1)->isIntegerTy()) {
    std::string ErrMsg;
    llvm::raw_string_ostream RSO(ErrMsg);
    RSO << "the ripple shuffle instruction index-mapping operand "
           "must take two integer operands and output an integer; the "
           "provided "
           "prototype is "
        << *ShuffleFunc->getFunctionType();
    RSO.flush();
    DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(I),
                                   ErrMsg);
    F.getContext().diagnose(DI);
    return createStringError(
        inconvertibleErrorCode(),
        "Shuffle index mapping function w/ wrong prototype");
  }

  // Is it defined in this module
  if (ShuffleFunc->empty()) {
    DiagnosticInfoRippleWithLoc DI(
        DS_Error, F, sanitizeRippleLocation(I),
        "the index mapping function (or lambda) operand of ripple shuffle "
        "requires its definition to be accessible in the same module as the "
        "function being processed");
    F.getContext().diagnose(DI);
    return createStringError(inconvertibleErrorCode(),
                             "Shuffle index mapping is not defined in module");
  }

  // No global memory access for compile time evaluation
  bool MappingFunUsingGlobals = false;
  for (auto &BB : *ShuffleFunc) {
    for (auto &I : BB) {
      if (auto *LoadInst = llvm::dyn_cast<llvm::LoadInst>(&I)) {
        llvm::Value *Operand = LoadInst->getPointerOperand();
        if (auto *Global = llvm::dyn_cast<llvm::GlobalVariable>(Operand)) {
          LLVM_DEBUG(dbgs() << "Found a global: " << *Global << "\n");
          if (Global->isConstant())
            continue;
          std::string ErrMsg;
          llvm::raw_string_ostream RSO(ErrMsg);
          RSO << "The ripple shuffle instruction index-mapping function (or "
                 "lambda) cannot be evaluated at compile time because it is "
                 "accessing the value of a non-constant global variable \""
              << Global->getName() << "\"";
          RSO.flush();
          DiagnosticInfoRippleWithLoc DI(DS_Error, F,
                                         sanitizeRippleLocation(&I), ErrMsg);
          F.getContext().diagnose(DI);
          MappingFunUsingGlobals = true;
        }
      }
    }
  }
  if (MappingFunUsingGlobals)
    return createStringError(
        inconvertibleErrorCode(),
        "Shuffle index mapping function accesses non-const globals");

  // Check that there is no out-of-bound access
  DimSize BlockSize = getRippleShape(I).flatShape();
  Evaluator Evaler(DL, &targetLibraryInfo);
  unsigned ReportedIssues = 0;
  for (DimSize IIdx = 0; IIdx < BlockSize; ++IIdx) {
    Constant *RetVal;
    Constant *IdxArg = ConstantInt::get(ShuffleFuncType->getParamType(0), IIdx);
    Constant *BlockSizeArg =
        ConstantInt::get(ShuffleFuncType->getParamType(1), BlockSize);
    SmallVector<Constant *, 2> Args = {IdxArg, BlockSizeArg};

    if (!Evaler.EvaluateFunction(ShuffleFunc, RetVal, Args)) {
      std::string ErrMsg;
      llvm::raw_string_ostream RSO(ErrMsg);
      RSO << "failed to evaluate the index mapping function of ripple "
             "shuffle at compile time, the call was: "
          << ShuffleFunc->getName() << " with arguments (" << IIdx << ", "
          << BlockSize << ")";
      RSO.flush();
      DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(I),
                                     ErrMsg);
      F.getContext().diagnose(DI);
      return createStringError(inconvertibleErrorCode(),
                               "Shuffle index mapping evaluation failed");
    }
    ConstantInt *RetIntVal = cast<ConstantInt>(RetVal);

    auto MaxInputIndex = IsPairShuffle ? BlockSize * 2 : BlockSize;
    if (RetIntVal->uge(MaxInputIndex)) {
      std::string ErrMsg;
      llvm::raw_string_ostream RSO(ErrMsg);
      RSO << "Evaluation of the index mapping function of ripple_shuffle";
      if (IsPairShuffle)
        RSO << "_pair";
      RSO << " returned an out of bound value; the call was to ";
      char *DemangledName = itaniumDemangle(ShuffleFunc->getName().str());
      if (DemangledName) {
        StringRef DemangledString(DemangledName);
        RSO << "\"" << DemangledString << "\"";
        free(DemangledName);
      } else if (!ShuffleFunc->getName().empty()) {
        RSO << "\"" << ShuffleFunc->getName() << "\"";
      }
      RSO << " with arguments (" << *IdxArg << ", " << *BlockSizeArg
          << "). The returned value ("
          << RetIntVal->getValue().getLimitedValue()
          << ") is greater or equal to the size of ";
      if (IsPairShuffle)
        RSO << "two (pair) tensors (" << BlockSize * 2 << ")";
      else
        RSO << "the tensor (" << BlockSize << ")";
      RSO.flush();
      DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(I),
                                     ErrMsg);
      F.getContext().diagnose(DI);
      ReportedIssues++;
    }
  }
  if (ReportedIssues > 0) {
    return createStringError(
        inconvertibleErrorCode(),
        "Shuffle index mapping evaluated to out-of-bound value");
  }

  return Error::success();
}

Error Ripple::checkRippleStore(const StoreInst *Store) const {
  if (getRippleShape(Store).isScalar())
    return Error::success();

  auto &PtrShape = getRippleShape(Store->getPointerOperand());
  auto &ValueShape = getRippleShape(Store->getValueOperand());

  // We are only allowed to broadcast values to match the address but not the
  // address to match the value. This is because it can be ambiguous what the
  // semantics would be in this case. Hence, notify the user if any tensor
  // dimension of the address is smaller than the value dimension
  if (PtrShape
          .testBothDims(ValueShape,
                        [](DimSize PtrDimSize, DimSize ValueDimSize) {
                          return PtrDimSize < ValueDimSize;
                        })
          .any()) {
    std::string ErrMsg;
    llvm::raw_string_ostream RSO(ErrMsg);
    RSO << "ripple does not allow implicit broadcasting of a store address "
           "to the value address; the value has shape '"
        << ValueShape << "' and the address has shape '" << PtrShape
        << "'. Hint: use ripple_id() for the address computation or use a "
           "reduction "
           "operation";
    RSO.flush();
    DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(Store),
                                   ErrMsg);
    F.getContext().diagnose(DI);
    return createStringError(inconvertibleErrorCode(),
                             "Cannot broadcast store address");
  }
  return Error::success();
}

Error Ripple::checkRippleExternCall(const CallInst *Call) {
  if (getRippleShape(Call).isScalar())
    return Error::success();
  bool IsMaskedRippleCall = MaskedCalls.contains(Call);
  if (IsMaskedRippleCall) {
    // Warn when the call has to be masked but the external ripple function
    // does not support masking
    if (!findExternalRippleFunctionFor(Call, true)) {
      if (auto *ExternalF = findExternalRippleFunctionFor(Call, false)) {
        std::string ErrMsg;
        llvm::raw_string_ostream RSO(ErrMsg);
        RSO << "call to an external ripple function ("
            << ExternalF->getFunction()->getName()
            << ") requires masking but no maskable declaration is "
               "available";
        RSO.flush();
        DiagnosticInfoRippleWithLoc DI(DS_Error, F,
                                       sanitizeRippleLocation(Call), ErrMsg);
        F.getContext().diagnose(DI);
        return createStringError(
            inconvertibleErrorCode(),
            "External ripple function call site requires masking and no "
            "masked declaration are available");
      }
    }
  }

  if (auto *ExternF = findExternalRippleFunctionFor(Call, IsMaskedRippleCall))
    if (ExternF->isElementWiseFunction())
      if (Function *CalledF = Call->getCalledFunction())
        if (any_of(CalledF->args(),
                   [](Argument &Arg) { return Arg.hasByValAttr(); })) {
          DiagnosticInfoRippleWithLoc DI(
              DS_Error, F, sanitizeRippleLocation(Call),
              "ripple does not implement slicing of element-wise byval "
              "arguments; please fill up a support request with the Ripple "
              "team");
          F.getContext().diagnose(DI);
        }
  return Error::success();
}

Error Ripple::checkVectorBranch(Instruction *BranchOrSwitch) {
  auto firstInstructionWithValidDebugLoc = [](BasicBlock *BB) -> Instruction * {
    for (auto &I : make_range(BB->getFirstNonPHIOrDbgOrAlloca(), BB->end())) {
      auto DL = I.getDebugLoc();
      if (DL && !(DL.getLine() == 0 && DL.getCol() == 0))
        return &I;
    }
    return nullptr;
  };
  auto &MaskShape = getRippleShape(BranchOrSwitch);
  auto maskCanApplyToShape = [&](const TensorShape &S) -> bool {
    TensorShape ShapeBeforeBcast = MaskShape;
    auto ReductionDims = MaskShape.reductionDimensionsBeforeBroadcast(S);
    if (ReductionDims.any())
      ShapeBeforeBcast.reduceDimensions(ReductionDims);
    if (Error e = ShapeBeforeBcast.isBroadcastError(S)) {
      consumeError(std::move(e));
      return false;
    }
    return true;
  };
  auto maskCanApplyToInstruction = [&](const Instruction *I) -> bool {
    auto &IShape = getRippleShape(I);
    return maskCanApplyToShape(IShape);
  };
  // For vector branch/switch we check that the subgraph between the basic block
  // containing the instruction and the immediate post-dominator is a single
  // entry single exit (SESE) region.
  BasicBlock *BBWithVectorSw = BranchOrSwitch->getParent();
  BasicBlock *BranchPostDom =
      postdomTree.getNode(BBWithVectorSw)->getIDom()->getBlock();
  auto BBsInBetween = allBasicBlocksFromTo(BBWithVectorSw, BranchPostDom);
  bool HasErrors = false;
  for (auto *BB : BBsInBetween) {
    for (auto &I : *BB) {
      bool CheckInstruction = false;
      if (auto *Call = dyn_cast<CallInst>(&I)) {
        if (auto *ExternFun =
                findExternalRippleFunctionFor(Call, MaskedCalls.contains(Call)))
          if (Argument *MaskArg = ExternFun->getTensorMaskArgument()) {
            // For non element-wise functions, the mask must be the broadcast
            // combination of the instruction shape (return shape) and operand
            // shapes. For elementwise, the mask gets sliced
            LLVM_DEBUG(dbgs() << "Mask argument: " << *MaskArg << "\n");
            auto *MaskArgTy = ExternalRippleFunction::getTrueMaskType(MaskArg);
            // We check that on external function creation
            VectorType *MaskTy = cast<VectorType>(MaskArgTy);
            auto MaskShape = ExternFun->maskShape(Call, *this);
            if (!MaskShape ||
                (!ExternFun->isElementWiseFunction() &&
                 MaskShape->flatShape() !=
                     MaskTy->getElementCount().getKnownMinValue())) {
              std::string ErrMsg;
              raw_string_ostream RSO(ErrMsg);
              RSO << "the mask operand of the external ripple function '"
                  << ExternFun->getFunction()->getName()
                  << "' must be compatible with its return and non-mask "
                     "operand shapes, i.e., all operands and return tensor "
                     "shape can be broadcasted to the mask shape: expected "
                     "mask of size "
                  << MaskShape->flatShape() << " but have " << *MaskTy;
              RSO.flush();
              DiagnosticInfoRippleWithLoc DI(
                  DS_Error, F, sanitizeRippleLocation(Call), ErrMsg);
              F.getContext().diagnose(DI);
              HasErrors = true;
            }
          }
      } else {
        CheckInstruction = maskInstructionWhenIfConvert(&I);
      }
      // Check that maskable instructions agree w/ the mask shape
      if (CheckInstruction && !maskCanApplyToInstruction(&I)) {
        std::string ErrMsg;
        raw_string_ostream RSO(ErrMsg);
        RSO << "this instruction, with " << getRippleShape(&I)
            << " is incompatible with a vector "
               "conditional";
        RSO.flush();
        DiagnosticInfoRippleWithLoc DI(DS_Error, F, sanitizeRippleLocation(&I),
                                       ErrMsg);
        F.getContext().diagnose(DI);
        std::string NoteMsg;
        raw_string_ostream NoteRSO(NoteMsg);
        NoteRSO << "for this vector conditional instruction with "
                << getRippleShape(BranchOrSwitch);
        NoteRSO.flush();
        DiagnosticInfoRippleWithLoc Note(
            DS_Note, F, sanitizeRippleLocation(BranchOrSwitch), NoteMsg);
        F.getContext().diagnose(Note);
        HasErrors = true;
      }
    }
    for (auto *IncomingBB : predecessors(BB)) {
      bool ComingFromInBetween = BBsInBetween.contains(IncomingBB);
      bool ComingFromBranchBB = IncomingBB == BBWithVectorSw;
      // There is a branch from Incoming into the subgraph invalidating the SESE
      // assumption
      if (!(ComingFromInBetween || ComingFromBranchBB ||
            hasTrivialLoopLikeBackEdge(BBWithVectorSw, BranchPostDom,
                                       domTree)) ||
          BB->hasAddressTaken()) {
        HasErrors = true;
        // Show that it's a problem related to if-conversion of the branch
        // instruction
        std::string ErrMsg;
        llvm::raw_string_ostream RSO(ErrMsg);
        RSO << "ripple cannot vectorize the vector "
            << (isa<BranchInst>(BranchOrSwitch) ? "branch" : "switch")
            << " because it applies to a non single-entry-single-exit (SESE) "
               "region";
        RSO.flush();
        DiagnosticInfoRippleWithLoc DI(
            DS_Error, F, sanitizeRippleLocation(BranchOrSwitch), ErrMsg);
        F.getContext().diagnose(DI);
      }
      if (!(ComingFromInBetween || ComingFromBranchBB ||
            hasTrivialLoopLikeBackEdge(BBWithVectorSw, BranchPostDom,
                                       domTree))) {
        // pinpoint the illegal branching
        DiagnosticLocation DL(
            sanitizeRippleLocation(IncomingBB->getTerminator()));
        if (DL.isValid()) {
          DiagnosticInfoRippleWithLoc DI(
              DS_Note, F, DL,
              "illegally branching from this instruction into the sub-graph");
          F.getContext().diagnose(DI);
        }

        // if there is a meaningful debug location in BB, display the first
        // target of the illegal branch
        if (Instruction *BranchInstWithDebugInfo =
                firstInstructionWithValidDebugLoc(BB)) {
          DiagnosticLocation TargetDL(
              sanitizeRippleLocation(BranchInstWithDebugInfo));
          if (TargetDL.isValid()) {
            DiagnosticInfoRippleWithLoc DI(
                DS_Note, F, TargetDL,
                "illegally branching to this instruction");
            F.getContext().diagnose(DI);
          }
        }
      }
      if (BB->hasAddressTaken()) {
        // Find the instructions that take the BB address
        bool ReportedNothing = true;
        BlockAddress *BA = BlockAddress::get(BB);
        for (auto *User : BA->users()) {
          if (Instruction *I = dyn_cast<Instruction>(User)) {
            DiagnosticLocation DL(sanitizeRippleLocation(I));
            if (DL.isValid()) {
              DiagnosticInfoRippleWithLoc DI(DS_Note, F,
                                             sanitizeRippleLocation(I),
                                             "illegally taking the address at");
              F.getContext().diagnose(DI);
              ReportedNothing = false;
            }
          }
        }
        if (ReportedNothing) {
          if (Instruction *BranchInstWithDebugInfo =
                  firstInstructionWithValidDebugLoc(BB)) {
            DiagnosticInfoRippleWithLoc DI(
                DS_Note, F, sanitizeRippleLocation(BranchInstWithDebugInfo),
                "illegally taking the address of a basic block starting with "
                "this instruction");
            F.getContext().diagnose(DI);
          } else {
            DiagnosticInfoRippleWithLoc DI(
                DS_Note, F, sanitizeRippleLocation(BranchOrSwitch),
                "illegally taking the address of a basic block");
            F.getContext().diagnose(DI);
          }
        }
      }
    }
  }
  if (HasErrors)
    return createStringError(inconvertibleErrorCode(),
                             "if-conversion SESE violation");
  else
    return Error::success();
}

Error Ripple::checkTypeCanBeVectorized(const Instruction *I) {
  auto checkVectorPromotionTypeValidity = [&](const Value *V) -> bool {
    Type *ValueType = V->getType();
    if (!VectorType::isValidElementType(ValueType)) {
      LLVM_DEBUG(dbgs() << *V << " has an invalid vector type!\n");
      const char *TypeHint = "";
      if (ValueType->isArrayTy())
        TypeHint = "array ";
      else if (ValueType->isStructTy())
        TypeHint = "structure ";
      else if (ValueType->isFunctionTy())
        TypeHint = "function ";
      DiagnosticInfoRippleWithLoc DI(
          DS_Error, F, sanitizeRippleLocation(I),
          Twine("Ripple cannot create a vector type from this instruction's ") +
              TypeHint +
              "type; Allowed vector element types are integer, floating point "
              "and pointer");
      F.getContext().diagnose(DI);
      return false;
    }
    return true;
  };
  bool Valid = true;

  // Check that the instruction itself can be vectorized
  if (!I->getType()->isVoidTy())
    Valid = Valid && checkVectorPromotionTypeValidity(I);

  // And that the instruction operands can be vectorized (broadcasted)
  for (auto &U : vectorizableOperands(I))
    Valid = Valid && checkVectorPromotionTypeValidity(U);

  if (!Valid)
    return createStringError(inconvertibleErrorCode(),
                             "if-conversion SESE violation");
  else
    return Error::success();
}

Error Ripple::checkRippleFunctionReturn(const ReturnInst *Return) const {
  // When we specialize, the return can be vectorized
  if (!isPendingRippleSpecialization(F) && getRippleShape(Return).isVector()) {
    DiagnosticInfoRippleWithLoc DI(
        DS_Error, F, sanitizeRippleLocation(Return),
        "Ripple does not allow vectorization of the return value");
    F.getContext().diagnose(DI);
    return createStringError(inconvertibleErrorCode(),
                             "Function returns tensor");
  }
  return Error::success();
}

Error Ripple::checkRippleSemantics() {
  Error AllErrors = Error::success();
  for (auto &I : instructions(F)) {
    auto &InstructionShape = getRippleShape(&I);

    if (InstructionShape.isVector())
      AllErrors =
          llvm::joinErrors(std::move(AllErrors), checkTypeCanBeVectorized(&I));

    if ((isa<BranchInst>(&I) || isa<SwitchInst>(&I)) &&
        InstructionShape.isVector()) {
      AllErrors = llvm::joinErrors(std::move(AllErrors), checkVectorBranch(&I));

    } else if (IntrinsicInst *RippleBlockI = rippleBlockIntrinsics(&I)) {
      AllErrors = llvm::joinErrors(std::move(AllErrors),
                                   checkRippleBlockIntrinsics(RippleBlockI));

    } else if (IntrinsicInst *RippleRedI = rippleReduceIntrinsics(&I)) {
      AllErrors = llvm::joinErrors(std::move(AllErrors),
                                   checkRippleReductionIntrinsics(RippleRedI));

    } else if (IntrinsicInst *rippleShuffleI = rippleShuffleIntrinsics(&I)) {
      AllErrors = llvm::joinErrors(
          std::move(AllErrors), checkRippleShuffleIntrinsics(rippleShuffleI));
    } else if (CallInst *CallI = dyn_cast<CallInst>(&I)) {
      AllErrors =
          llvm::joinErrors(std::move(AllErrors), checkRippleExternCall(CallI));

    } else if (StoreInst *Store = dyn_cast<StoreInst>(&I)) {
      AllErrors =
          llvm::joinErrors(std::move(AllErrors), checkRippleStore(Store));

    } else if (auto *Return = dyn_cast<ReturnInst>(&I)) {
      AllErrors = llvm::joinErrors(std::move(AllErrors),
                                   checkRippleFunctionReturn(Return));
    }
  }
  return AllErrors;
}

template <typename T>
void Ripple::processDebugIntrinsicOrRecord(DIBuilder &DIB, T &DbgVariable) {
  for (unsigned VarIdx = 0, e = DbgVariable.getNumVariableLocationOps();
       VarIdx < e; VarIdx++) {
    if (Instruction *VariableInst =
            dyn_cast<Instruction>(DbgVariable.getVariableLocationOp(VarIdx))) {
      auto RemappedVariable = InstructionReplacementMapping.find(VariableInst);
      // No replacement, no fix needed!
      if (RemappedVariable == InstructionReplacementMapping.end())
        continue;

      auto [InstBeingReplaced, ReplacementValue] = *RemappedVariable;

      if (ReplacementValue == nullptr) {
        // We removed the instruction => remove the Debug instruction / record
        invalidateRippleDataFor(&DbgVariable);
        DbgVariable.eraseFromParent();
        continue;
      }

      const TensorShape &VecShape = getRippleShape(&*InstBeingReplaced);

      // We expose ripple.block.index as a vector for the debugger, but
      // replace it by the base value 0 for linear series
      if (IntrinsicInst *RippleIndexInst = intrinsicWithId(
              InstBeingReplaced, {Intrinsic::ripple_block_index}))
        ReplacementValue = LinearSeries::constructLinearSeriesVector(
            cast<IntegerType>(RippleIndexInst->getType()),
            VecShape.flatShape());

      // Set the replacement as metadata
      DbgVariable.replaceVariableLocationOp(VarIdx, ReplacementValue);

      // Simple replacement, no need to fix the debug type
      if (InstBeingReplaced->getType() == ReplacementValue->getType())
        continue;

      // We are dealing with scalar -> vector type transformations
      assert(!InstBeingReplaced->getType()->isVectorTy() &&
             "Expected scalar to vector replacement");

      if (DILocalVariable *LocalVectorVar = createVectorLocalFromScalarLocal(
              DIB, DbgVariable.getVariable(), VecShape)) {
        DbgVariable.setVariable(LocalVectorVar);
      }
    }
  }
}

std::string Ripple::tensorizedName(StringRef Name, const TensorShape &Shape) {
  std::string TensorName;
  raw_string_ostream RSO(TensorName);
  RSO << Name << ".ripple";
  if (Shape.rank() > 0) {
    RSO << ".t" << Shape[Shape.rank() - 1];
    for (unsigned RankIdx = Shape.rank() - 2, E = Shape.rank(); RankIdx < E;
         --RankIdx)
      RSO << "x" << Shape[RankIdx];
  }
  RSO.flush();
  return TensorName;
}

Expected<TensorShape> Ripple::combineShapeBcastWithErrorReporting(
    const TensorShape &ShapeToBeBroadcasted, const TensorShape &OtherShape,
    StringRef ShapeToBeBcastedMsg, DebugLoc ShapeToBeBroadcastedLocation,
    StringRef OtherShapeMsg, DebugLoc SecondLocation) {
  TensorShape Bcasted = ShapeToBeBroadcasted;
  if (Error E = Bcasted.combineShapeBcast(OtherShape)) {
    Error Err = handleErrors(std::move(E), [&](StringError &StrErr) {
      std::string ErrMsg;
      llvm::raw_string_ostream RSO(ErrMsg);
      RSO << "broadcast failure: " << StrErr.getMessage();
      RSO.flush();
      DiagnosticInfoRippleWithLoc Diag(DS_Error, F,
                                       ShapeToBeBroadcastedLocation, ErrMsg);
      F.getContext().diagnose(Diag);
    });
    if (Err) {
      LLVM_DEBUG(dbgs() << "Error type unreported to the user: " << Err);
      return std::move(Err);
    }
    if (!ShapeToBeBcastedMsg.empty()) {
      std::string ErrMsg;
      llvm::raw_string_ostream RSO(ErrMsg);
      RSO << ShapeToBeBcastedMsg << " of shape " << ShapeToBeBroadcasted;
      RSO.flush();
      DiagnosticInfoRippleWithLoc Diag(DS_Note, F, ShapeToBeBroadcastedLocation,
                                       ErrMsg);
      F.getContext().diagnose(Diag);
    }
    if (!OtherShapeMsg.empty()) {
      std::string ErrMsg;
      llvm::raw_string_ostream RSO(ErrMsg);
      RSO << OtherShapeMsg << " of shape " << OtherShape;
      RSO.flush();
      DiagnosticInfoRippleWithLoc Diag(DS_Note, F, SecondLocation, ErrMsg);
      F.getContext().diagnose(Diag);
    }
    return createStringError(inconvertibleErrorCode(),
                             "CombineShapeBcast failure");
  }
  return Bcasted;
}

Value *Ripple::instantiateLinearSeriesNoCache(const LinearSeries &LS) {
  return instantiateLinearSeries(&LS, false);
}

void Ripple::visitAllInstructionsBeingClobberedBy(
    const MemoryDef *Def, std::function<bool(Instruction *)> Apply,
    bool VisitUnreachableFromEntry) {
  Instruction *DefInst = Def->getMemoryInst();
  auto DefLocationOpt = MemoryLocation::getOrNone(DefInst);
  if (!DefLocationOpt)
    return;
  auto &DefLocation = *DefLocationOpt;

  std::queue<MemoryAccess *> WorkQueue;
  auto addUsersToQueue = [&](MemoryAccess *Access) {
    for (User *U : Access->users())
      WorkQueue.push(static_cast<MemoryAccess *>(U));
  };

  // Todo: throw warnings when we encounter PartialAlias and MayAlias so that
  // the user knows something may have a non-deterministic shape!
  auto processAccess = [&](MemoryAccess *Access) {
    if (MemoryUse *Use = dyn_cast<MemoryUse>(Access)) {
      Instruction *MemInst = Use->getMemoryInst();
      if (auto UseLocation = MemoryLocation::getOrNone(MemInst))
        if (AA.isMustAlias(DefLocation, *UseLocation) &&
            (VisitUnreachableFromEntry ||
             domTree.isReachableFromEntry(MemInst->getParent())))
          if (!Apply(MemInst))
            // Return early when Apply returns false
            return;
    } else if (MemoryDef *OtherDef = dyn_cast<MemoryDef>(Access)) {
      // Stop processing when a new definition is encountered
      if (auto OtherDefLocation =
              MemoryLocation::getOrNone(OtherDef->getMemoryInst()))
        if (AA.isMustAlias(DefLocation, *OtherDefLocation))
          // Don't visit further because Def and OtherDef clobber
          return;
    } else
      assert(isa<MemoryPhi>(Access));
    addUsersToQueue(Access);
  };

  // Start with the Store users
  addUsersToQueue(MemSSA.getMemoryAccess(DefInst));

  SmallPtrSet<const MemoryAccess *, 0> AlreadyVisited;
  AlreadyVisited.insert(Def);
  while (!WorkQueue.empty()) {
    MemoryAccess *Access = WorkQueue.front();
    WorkQueue.pop();
    if (AlreadyVisited.contains(Access))
      continue;
    AlreadyVisited.insert(Access);
    processAccess(Access);
  }
}

void Ripple::visitAllClobberingInstructions(
    MemoryUse *Use, std::function<bool(Instruction *)> Apply,
    bool VisitUnreachableFromEntry) {
  Instruction *UseInst = Use->getMemoryInst();
  auto UseLocationOpt = MemoryLocation::getOrNone(UseInst);
  if (!UseLocationOpt)
    return;
  auto &UseLocation = *UseLocationOpt;

  // Start with the first clobber (which may be a Phi or Def)
  std::queue<MemoryAccess *> WorkQueue;
  WorkQueue.push(MemSSAWalker.getClobberingMemoryAccess(Use));

  SmallPtrSet<const MemoryAccess *, 0> AlreadyVisited;
  while (!WorkQueue.empty()) {
    MemoryAccess *Access = WorkQueue.front();
    WorkQueue.pop();
    if (AlreadyVisited.contains(Access))
      continue;
    AlreadyVisited.insert(Access);

    assert(!isa<MemoryUse>(Access) &&
           "We shouldn't be processing MemoryUse when looking for clobbering "
           "Instructions");

    if (MemSSA.isLiveOnEntryDef(Access))
      continue;
    else if (MemoryPhi *Phi = dyn_cast<MemoryPhi>(Access))
      for (auto &IncomingVal : Phi->incoming_values()) {
        auto *IncomingAccess = cast<MemoryAccess>(IncomingVal);
        WorkQueue.push(IncomingAccess);
      }
    else if (MemoryDef *Def = dyn_cast<MemoryDef>(Access)) {
      Instruction *MemInst = Def->getMemoryInst();
      if (auto DefLocation = MemoryLocation::getOrNone(MemInst))
        if (AA.isMustAlias(UseLocation, *DefLocation) &&
            (VisitUnreachableFromEntry ||
             domTree.isReachableFromEntry(MemInst->getParent()))) {
          if (Apply(MemInst))
            continue;
          else
            return;
        }
      WorkQueue.push(Def->getDefiningAccess());
    } else {
      auto *Use = cast<MemoryUse>(Access);
      WorkQueue.push(Use->getDefiningAccess());
    }
  }
}

std::pair<AliasResult, AllocaInst *>
Ripple::aliasesWithAlloca(const MemoryLocation &Loc,
                          const DenseSet<AssertingVH<AllocaInst>> &AllocaSet,
                          AliasResult::Kind AliasKind) const {
  for (AllocaInst *Alloca : AllocaSet)
    if (auto Size = Alloca->getAllocationSize(DL)) {
      auto AllocaLoc =
          MemoryLocation(Alloca, LocationSize::precise(DL.getTypeStoreSize(
                                     Alloca->getAllocatedType())));
      auto AAResult = AA.alias(AllocaLoc, Loc);
      // If kind is AliasResult::NoAlias return when any kind of aliasing happen
      bool CheckNoAlias = AliasKind == AliasResult::NoAlias;
      if ((CheckNoAlias && AAResult != AliasResult::NoAlias) ||
          (!CheckNoAlias && AAResult == AliasKind))
        return {AAResult, Alloca};
    }
  return {AliasResult::NoAlias, nullptr};
}

StringRef ExternalRippleFunction::removeRipplePrefix(const Function *Fun) {
  return Fun->getName().split(RipplePrefix).second;
}

bool ExternalRippleFunction::usesRippleNamingConvention(const Function *F) {
  return F->getName().starts_with(RipplePrefix);
}

std::unique_ptr<ExternalRippleFunction>
ExternalRippleFunction::createExternalFunction(Function *F,
                                               unsigned TensorRank) {
  if (!usesRippleNamingConvention(F))
    return nullptr;
  auto Options_and_Basename = removeRipplePrefix(F);
  auto [SignatureAndBaseName, OptionList] = splitOptions(Options_and_Basename);

  SmallVector<ArgumentSignatureInfo> ArgumentSignatures;
  ReturnSignatureInfo ReturnSignature;
  bool IsUniformSignature = false;
  auto BaseNameOpt =
      parseSignature(SignatureAndBaseName, *F, ArgumentSignatures,
                     ReturnSignature, IsUniformSignature);
  if (!BaseNameOpt)
    return nullptr;
  if (BaseNameOpt->empty()) {
    std::string ErrMsg;
    {
      raw_string_ostream RSO(ErrMsg);
      RSO << "the external ripple function '" << F->getName()
          << "' is missing a function name following the function "
             "options and tensor shapes, e.g., '"
          << F->getName() << "myFunction'";
    }
    DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
    F->getContext().diagnose(Diag);
    return nullptr;
  }
  auto &[ReturnMultidimType, ReturnMultidimTS] = ReturnSignature;
  bool AnyVectorType = ReturnMultidimTS.isVector();
  auto *NormalizedFTy = normalizeFunctionType(F);
  if (ReturnMultidimType) {
    LLVM_DEBUG(dbgs() << "Signature return type is " << *ReturnMultidimType
                      << " and tensor shape " << ReturnMultidimTS << "\n");
    // This candidate is not fit for this function
    if (ReturnMultidimTS.rank() > TensorRank)
      return nullptr;
    // Check that the type agrees with the return shape
    if (auto *VecRetTy = dyn_cast<VectorType>(NormalizedFTy->getReturnType())) {
      if (VecRetTy->getElementCount().getFixedValue() !=
          ReturnMultidimTS.flatShape()) {
        std::string ErrMsg;
        {
          raw_string_ostream RSO(ErrMsg);
          RSO << "the external ripple function '" << F->getName()
              << "' return value tensor shape '" << ReturnMultidimTS
              << "' is incompatible with a vector size of "
              << VecRetTy->getElementCount().getFixedValue();
        }
        DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
        F->getContext().diagnose(Diag);
        return nullptr;
      }
    }
  }
  for (auto &[ArgTy, ArgTS, ArgIdx] : ArgumentSignatures) {
    LLVM_DEBUG(dbgs() << "Signature argument " << ArgIdx << " type is "
                      << *ArgTy << " and shape " << ArgTS << "\n");
    if (ArgTS.rank() > TensorRank) {
      LLVM_DEBUG(dbgs() << "External function rank greater than local tensor "
                           "rank, not considering\n");
      return nullptr;
    }
    AnyVectorType = AnyVectorType || ArgTS.isVector();
    if (ArgIdx >= NormalizedFTy->getNumParams()) {
      std::string ErrMsg;
      {
        raw_string_ostream RSO(ErrMsg);
        RSO << "the external ripple function '" << F->getName()
            << "' tensor shape argument index " << ArgIdx
            << " is out of bound; function has only "
            << NormalizedFTy->getNumParams() << " arguments";
      }
      DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
      F->getContext().diagnose(Diag);
      return nullptr;
    }
    if (auto *ArgVecTy =
            dyn_cast<VectorType>(NormalizedFTy->getParamType(ArgIdx))) {
      if (ArgVecTy->getElementCount().getFixedValue() != ArgTS.flatShape()) {
        std::string ErrMsg;
        {
          raw_string_ostream RSO(ErrMsg);
          RSO << "the external ripple function '" << F->getName()
              << "' argument at index " << ArgIdx << " tensor shape '" << ArgTS
              << "' is incompatible with a vector size of "
              << ArgVecTy->getElementCount().getFixedValue();
        }
        DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
        F->getContext().diagnose(Diag);
        return nullptr;
      }
    }
  }
  // Check vector-ness
  AnyVectorType = AnyVectorType || isa<VectorType>(F->getReturnType());
  for (auto &Arg : F->args()) {
    Type *ArgTy = Arg.getType();
    if (Arg.hasByValAttr() || Arg.hasStructRetAttr())
      ArgTy = Arg.getPointeeInMemoryValueType();
    AnyVectorType = AnyVectorType || isa<VectorType>(ArgTy);
  }
  if (!AnyVectorType)
    return nullptr;

  bool IsElementWise =
      any_of(OptionList, [](StringRef S) { return S == ElementWiseOption; });
  if (IsElementWise) {
    // Check that all vectors have the same size
    VectorType *SharedVectorTy = nullptr;
    for (auto &Arg : F->args()) {
      Type *ArgTy = Arg.getType();
      if (Arg.hasByValAttr() || Arg.hasStructRetAttr())
        ArgTy = Arg.getPointeeInMemoryValueType();

      // Check that all vector have the same size for element-wise
      if (VectorType *ArgVecTy = dyn_cast<VectorType>(ArgTy)) {
        if (!SharedVectorTy)
          SharedVectorTy = ArgVecTy;
        else if (SharedVectorTy->getElementCount() !=
                 ArgVecTy->getElementCount())
          return nullptr;
      }
    }
    // Check vector return
    if (VectorType *VectorReturn = dyn_cast<VectorType>(F->getReturnType()))
      if (SharedVectorTy &&
          SharedVectorTy->getElementCount() != VectorReturn->getElementCount())
        return nullptr;
  }

  // If the function is masked the last operand is vector of integer type, most
  // likely i1 or i8 depending on the target or pointer to an integer type that
  // the function can use as a mask.
  bool HasMask =
      any_of(OptionList, [](StringRef S) { return S == MaskedOption; });
  if (HasMask) {
    size_t NumArgs = F->arg_size();
    if (NumArgs < 1)
      return nullptr;
    Argument *MaskArg = F->getArg(NumArgs - 1);
    if (MaskArg->hasStructRetAttr()) {
      if (NumArgs < 2)
        return nullptr;
      MaskArg = F->getArg(F->arg_size() - 2);
    }
    Type *MaskArgTy = getTrueMaskType(MaskArg);
    if (!(MaskArgTy->isVectorTy() &&
          MaskArgTy->getScalarType()->isIntegerTy())) {
      // TODO: show a warning to the user that doesn't turn into an error!
      return nullptr;
    }
  }

  auto NewExternalF =
      std::unique_ptr<ExternalRippleFunction>(new ExternalRippleFunction(
          F, BaseNameOpt.value(), OptionList, TensorRank, ReturnSignature,
          ArgumentSignatures, IsUniformSignature));

  return NewExternalF;
}

const TensorShape &ExternalRippleFunction::elementWiseShape() const {
  assert(isElementWiseFunction());
  if (returnsVoid())
    for (auto &ArgShape : argOperandShapes())
      if (ArgShape.isVector())
        return ArgShape;
  return getReturnShape();
}

template <typename Container>
void ExternalRippleFunction::eraseValueAtSretIndex(const Function *F,
                                                   Container &C) {
  assert(C.size() == F->arg_size());
  // Remove the Sret argument shape
  auto StructRetArgIt = find_if(
      F->args(), [](const Argument &Arg) { return Arg.hasStructRetAttr(); });
  if (StructRetArgIt != F->arg_end())
    C.erase(C.begin() + StructRetArgIt->getArgNo());
}

FunctionType *
ExternalRippleFunction::normalizeFunctionType(const Function *F,
                                              const Argument *MaskArg) {
  return FunctionType::get(getTrueReturnType(F),
                           getTrueArgumentTypes(F, MaskArg, true),
                           F->isVarArg());
}

bool ExternalRippleFunction::matchesFunction(
    StringRef FName, const FunctionType *FType,
    ArrayRef<const TensorShape *> CallArgShapes) const {
  LLVM_DEBUG(dbgs() << "Checking " << FName << " against "
                    << getFunction()->getName() << "'s scalar name "
                    << scalarFunctionName() << "\n");

  // Either both are void or both return something
  bool NameMatch = scalarFunctionName() == FName;
  LLVM_DEBUG(dbgs() << "Name matches: " << NameMatch << "\n");

  // Match return-ness and element types
  Type *ExternalRetTy = FType->getReturnType();
  Type *FunRetTy = NormalizedFunType->getReturnType();
  bool ReturnMatch =
      ExternalRetTy->getScalarType() == FunRetTy->getScalarType();
  LLVM_DEBUG(dbgs() << "Return matches: " << ReturnMatch << "\n");

  // Match argumentCount and element types
  bool ArgumentMatch = all_of_zip(FType->params(), NormalizedFunType->params(),
                                  [](auto &ArgTy, auto &ExternArgTy) {
                                    return ArgTy->getScalarType() ==
                                           ExternArgTy->getScalarType();
                                  });
  LLVM_DEBUG(dbgs() << "Argument matches: " << ArgumentMatch << "\n");

  if (!NameMatch || !ReturnMatch || !ArgumentMatch)
    return false;

  // Check tensor shape
  for (size_t CallArgIdx = 0, E = CallArgShapes.size(); CallArgIdx < E;
       ++CallArgIdx) {

    auto &ArgShape = CallArgShapes[CallArgIdx];
    auto &ExternalShape = getArgOperandShape(CallArgIdx);
    LLVM_DEBUG(dbgs() << "Call arg " << CallArgIdx << " shape " << *ArgShape
                      << "\n"
                      << "External shape " << ExternalShape << "\n");

    if (isElementWiseFunction()) {
      if (!((ArgShape->isVector() && ExternalShape.isVector()) ||
            /*we can bcast scalars if needed*/ ArgShape->isScalar()))
        return false;
    } else {
      // Exact match required
      if (*ArgShape != ExternalShape)
        return false;
    }
  }
  return true;
}

Expected<TensorShape>
ExternalRippleFunction::maskShape(const CallInst *Call,
                                  const Ripple &Ripple) const {
  auto ReturnShape = returnTensorShape(Call, Ripple);
  if (!ReturnShape || isElementWiseFunction())
    return ReturnShape;
  TensorShape RetAndOpBcastShape = *ReturnShape;
  for (auto &Operand : Call->operands()) {
    auto &OpShape = Ripple.getRippleShape(Operand);
    if (Error E = RetAndOpBcastShape.combineShapeBcast(OpShape)) {
      return std::move(E);
    }
  }
  return RetAndOpBcastShape;
}

Expected<TensorShape>
ExternalRippleFunction::returnTensorShape(const CallInst *Call,
                                          const Ripple &R) const {
  // If the external function is element-wise, find the largest broadcast shape
  // of the operands and return this will be our return shape
  if (isElementWiseFunction() && Call->arg_size() > 0) {
    TensorShape BcastShape = R.getRippleShape(Call->getArgOperand(0));
    for (unsigned ArgIdx = 1, E = Call->arg_size(); ArgIdx < E; ++ArgIdx) {
      auto &TShape = R.getRippleShape(Call->getArgOperand(ArgIdx));
      if (TShape.isVector())
        if (Error Err = BcastShape.combineShapeBcast(TShape))
          return std::move(Err);
    }
    // We shouldn't be looking for an external Ripple function if the inputs are
    // all scalar but this could be useful later (for scalar in, tensor out
    // exteral ripple functions).
    if (BcastShape.isScalar())
      return getReturnShape();
    else
      return BcastShape;
  } else
    return getReturnShape();
}

ExternalRippleFunction *
Ripple::findExternalRippleFunctionFor(const CallInst *Call,
                                      bool RequiresMask) const {
  Function *CalledFunction = Call->getCalledFunction();
  // We can only support Function with known number of arguments and Intrinsic
  // calls
  if (!CalledFunction || CalledFunction->isVarArg())
    return nullptr;

  LLVM_DEBUG(dbgs() << "Looking at external ripple candidates for " << *Call
                    << "\n");

  SmallVector<const TensorShape *, 0> CallArgShapes;
  for (auto &Arg : Call->args()) {
    // External ripple function spec does no have support for native vector
    // operands for now
    if (Arg.get()->getType()->isVectorTy())
      return nullptr;
    CallArgShapes.push_back(&getRippleShape(Arg));
    LLVM_DEBUG(dbgs() << "CallInst Arg shape of " << *Arg << ": "
                      << *CallArgShapes.back() << "\n");
  }

  StringRef FunName = Call->getCalledOperand()->getName();
  if (const IntrinsicInst *II = dyn_cast<IntrinsicInst>(Call)) {
    StringRef IntrinsicLibName = intrinsicToLibName(II);
    if (!IntrinsicLibName.empty())
      FunName = IntrinsicLibName;
  }

  // If it's a function we normalize the type and arguments
  ExternalRippleFunction::eraseValueAtSretIndex(CalledFunction, CallArgShapes);
  FunctionType *FunType = CalledFunction->getFunctionType();
  FunType = ExternalRippleFunction::normalizeFunctionType(CalledFunction);

  // Looking for existing matches
  ExternalRippleFunction *MaskedF = nullptr, *UnmaskedF = nullptr;
  for (auto &ExternalFunction : ExternalRippleFunctions) {
    if (ExternalFunction->matchesFunction(FunName, FunType, CallArgShapes)) {
      if (ExternalFunction->isMaskable()) {
        MaskedF = MaskedF ? MaskedF : ExternalFunction.get();
      } else {
        UnmaskedF = UnmaskedF ? UnmaskedF : ExternalFunction.get();
      }
    }
  }
  if (RequiresMask) {
    if (MaskedF) {
      LLVM_DEBUG(dbgs() << "Found masked ripple external function match for "
                        << FunName << " as "
                        << MaskedF->getFunction()->getName() << "\n");
      return MaskedF;
    } else {
      LLVM_DEBUG(
          dbgs() << "Could not find masked ripple external function match for "
                 << FunName << "\n");
      if (UnmaskedF)
        LLVM_DEBUG(dbgs() << "Found non-masked function match "
                          << UnmaskedF->getFunction()->getName());
      return nullptr;
    }
  } else {
    // Prioritize unmasked function calls
    if (UnmaskedF) {
      LLVM_DEBUG(dbgs() << "Found Ripple external function match for "
                        << FunName << " as "
                        << UnmaskedF->getFunction()->getName() << "\n");
      return UnmaskedF;
    } else if (MaskedF) {
      LLVM_DEBUG(
          dbgs()
          << "Found masked ripple external function match for unmasked call "
          << FunName << " as " << MaskedF->getFunction()->getName() << "\n");
      return MaskedF;
    } else {
      LLVM_DEBUG(dbgs() << "Found no Ripple external function match for "
                        << FunName << "\n");
      return nullptr;
    }
  }
}

void Ripple::registerExternalRippleFunction(
    std::unique_ptr<ExternalRippleFunction> ExternFun) {
  if (!ExternFun)
    return;
  LLVM_DEBUG(dbgs() << "Registered external Ripple function:\n"
                    << *ExternFun << "\n");
  auto Lookup = find_if(ExternalRippleFunctions,
                        [&](auto &Other) { return *Other == *ExternFun; });
  // The function has not already been registered
  if (Lookup == ExternalRippleFunctions.end())
    ExternalRippleFunctions.push_back(std::move(ExternFun));
}

void Ripple::loadRippleLibDeclarations() {
  LLVMContext &Context = F.getContext();
  auto &ThisModule = *F.getParent();
  for (auto &Path : llvm::RippleCL::RippleLibs) {
    if (Path.empty())
      continue;
    LLVM_DEBUG(dbgs() << "Loading ripple lib: " << Path << "\n");
    auto BufferOrError = MemoryBuffer::getFile(Path);
    if (!BufferOrError) {
      // Report a warning and continue to next path
      auto ErrCode = BufferOrError.getError();
      Context.diagnose(
          DiagnosticInfoRipple(DiagnosticSeverity::DS_Warning,
                               "failed to load external Ripple library '" +
                                   Path + "': " + ErrCode.message()));
      continue;
    }
    // Parse the bitcode file
    auto ModuleOrErr = getLazyBitcodeModule(
        BufferOrError.get()->getMemBufferRef(), Context, true);
    SMDiagnostic ParseDiag;
    auto AssemblyModule = parseAssembly(BufferOrError.get()->getMemBufferRef(),
                                        ParseDiag, Context);
    const Module *OtherModule = nullptr;
    if (ModuleOrErr) {
      OtherModule = ModuleOrErr.get().get();
    } else if (AssemblyModule) {
      // We successfully loaded an assembly module, no need to report the
      // failure of loading the library
      consumeError(ModuleOrErr.takeError());
      OtherModule = AssemblyModule.get();
    } else {
      // Report a warning and continue
      std::string ErrMsg;
      handleAllErrors(ModuleOrErr.takeError(),
                      [&](const StringError &E) { ErrMsg = E.getMessage(); });
      Context.diagnose(DiagnosticInfoRipple(
          DiagnosticSeverity::DS_Warning,
          "the Ripple external library '" + Path +
              "' is neither a module nor LLVM bitcode. "
              "Module load error: " +
              ErrMsg + "; Bitcode load error: " + ParseDiag.getMessage()));
      continue;
    }

    // The targets must match, otherwise we have a semantics problem
    if (OtherModule->getDataLayout() != ThisModule.getDataLayout()) {
      Context.diagnose(DiagnosticInfoRipple(
          DiagnosticSeverity::DS_Warning,
          "the Ripple external library datalayout mismatches with the current "
          "target datalayout; ignoring '" +
              Path + "'"));
      continue;
    }

    // Now load the ripple functions declarations
    for (const Function &Fun : *OtherModule)
      if (ExternalRippleFunction::usesRippleNamingConvention(&Fun)) {
        LLVM_DEBUG(dbgs() << "External module ripple function: " << Fun
                          << "\n");
        // Duplicate the declaration in this module
        auto ExistingF = ThisModule.getOrInsertFunction(
            Fun.getName(), Fun.getFunctionType(), Fun.getAttributes());
        if (Function *ExistingFCallee =
                dyn_cast<Function>(ExistingF.getCallee())) {

          if (ExistingFCallee->getFunctionType() == Fun.getFunctionType()) {
            LLVM_DEBUG(dbgs() << "Cloned function declaration"
                              << *ExistingFCallee << "\n");
            registerExternalRippleFunction(
                ExternalRippleFunction::createExternalFunction(ExistingFCallee,
                                                               tensorRank()));
          } else {
            std::string ErrStr;
            raw_string_ostream OSS(ErrStr);
            OSS << "A Ripple symbol with name \"" << Fun.getName()
                << "\" is already defined in the current module with type \""
                << *ExistingFCallee->getFunctionType()
                << "\" and cannot be imported with type \""
                << *Fun.getFunctionType() << "\" from the external library '"
                << Path << "'";
            OSS.flush();
            Context.diagnose(
                DiagnosticInfoRipple(DiagnosticSeverity::DS_Warning, ErrStr));
            DiagnosticLocation DL(ExistingFCallee->getSubprogram());
            if (DL.isValid())
              Context.diagnose(DiagnosticInfoRippleWithLoc(
                  DiagnosticSeverity::DS_Note, *ExistingFCallee, DL,
                  "function declared here"));
          }
        }
      }
  }
}

void ExternalRippleFunction::applyOptions(
    const SmallVectorImpl<StringRef> &Options) {
  for (auto &Option : Options) {
    if (Option == ElementWiseOption) {
      LLVM_DEBUG(dbgs() << F->getName()
                        << " is an element-wise Ripple function\n");
      ElementWiseFunction = true;
      [[maybe_unused]] auto &ElShape = elementWiseShape();
      assert(ElShape.isVector() &&
             (getTrueReturnType()->isVoidTy() || getReturnShape() == ElShape) &&
             all_of(ArgShapes,
                    [&](const auto &Shape) {
                      return Shape.isScalar() || Shape == ElShape;
                    }) &&
             "This cannot possibly be element-wise!");
    } else if (Option == MaskedOption) {
      // We already check the argument size in the constructor wrapper
      assert(F->arg_size() >= 1);
      TensorMaskArgument = F->getArg(F->arg_size() - 1);
      if (TensorMaskArgument->hasStructRetAttr()) {
        assert(F->arg_size() >= 2);
        TensorMaskArgument = F->getArg(F->arg_size() - 2);
      }
    } else if (Option == PureOption) {
      IsPureOtherThanSretAndByVal = true;
    }
  }
}

std::pair<StringRef, SmallVector<StringRef, 0>>
ExternalRippleFunction::splitOptions(StringRef OptionsAndBaseName) {
  SmallVector<StringRef, 0> OptionList;
  size_t LastSize;
  do {
    LastSize = OptionsAndBaseName.size();
    for (auto &Option : AllOptions) {
      if (OptionsAndBaseName.starts_with(Option)) {
        OptionList.push_back(Option);
        OptionsAndBaseName = OptionsAndBaseName.substr(Option.size());
      }
    }
  } while (LastSize != OptionsAndBaseName.size());
  // What's left after stripping all the options is the function base name
  return {OptionsAndBaseName, std::move(OptionList)};
}

Type *ExternalRippleFunction::getTrueReturnType(const Function *F) {
  for (auto &Arg : F->args()) {
    if (Arg.hasStructRetAttr())
      return Arg.getParamStructRetType();
  }
  return F->getReturnType();
}

Type *ExternalRippleFunction::getTrueReturnType() const {
  return getTrueReturnType(getFunction());
}

SmallVector<Type *, 0> ExternalRippleFunction::getTrueArgumentTypes(
    const Function *F, const Argument *SkipMask, bool SkipStructRet) {
  SmallVector<Type *, 0> ArgTypes;
  ArgTypes.reserve(F->arg_size());
  for (auto &Arg : F->args()) {
    if (SkipMask == &Arg)
      continue;
    if (Arg.hasStructRetAttr()) {
      if (SkipStructRet)
        continue;
      else
        ArgTypes.push_back(Arg.getParamStructRetType());
    } else if (Arg.hasPassPointeeByValueCopyAttr())
      ArgTypes.push_back(Arg.getPointeeInMemoryValueType());
    else
      ArgTypes.push_back(Arg.getType());
  }
  return ArgTypes;
}

SmallVector<Type *, 0>
ExternalRippleFunction::getTrueArgumentTypes(bool SkipMask,
                                             bool SkipStructRet) const {
  return getTrueArgumentTypes(getFunction(),
                              SkipMask ? getTensorMaskArgument() : nullptr,
                              SkipStructRet);
}

Value *Ripple::genFunctionCallHandleArgAttributes(
    CallInst *CallSite, ExternalRippleFunction &Calling,
    const TensorShape &ToShape, ArrayRef<Value *> FromArgs,
    Value *MaskForExternalFunction) {
  auto getStructRetArg = [](const Function *Fun) -> const Argument * {
    if (!Fun)
      return nullptr;
    for (auto &Arg : Fun->args()) {
      if (Arg.hasStructRetAttr())
        return &Arg;
    }
    return nullptr;
  };

  const Function *From = CallSite->getCalledFunction();
  Function *To = Calling.getFunction();
  auto *FromRetArg = getStructRetArg(From);
  auto *ToRetArg = getStructRetArg(To);
  Value *FunRetStorage;
  if (ToRetArg) {
    if (!FromRetArg) {
      // We allocate a vector buffer for the return
      irBuilder.SetInsertPoint(F.getEntryBlock().getFirstInsertionPt());
      FunRetStorage = irBuilder.CreateAlloca(ToRetArg->getParamStructRetType());
      setRippleShape(FunRetStorage, ScalarShape);
    } else {
      FunRetStorage = FromArgs[FromRetArg->getArgNo()];
      assert(FunRetStorage->getType() == ToRetArg->getParamStructRetType());
    }
  }
  // Get the call arguments right
  SmallVector<Value *, 0> ToCallArgs;
  for (unsigned FromIdx = 0, ToIdx = 0, E = To->arg_size();
       FromIdx < E && ToIdx < E; ++ToIdx, ++FromIdx) {

    Argument *ToArg = To->getArg(ToIdx);
    if (ToArg->hasStructRetAttr()) {
      ToCallArgs.push_back(FunRetStorage);
      // Increment only ToIdx
      --FromIdx;
      continue;
    }

    Argument *FromArg = nullptr;
    Value *ProcessingArg = nullptr;
    if (ToArg == Calling.getTensorMaskArgument()) {
      // We are processing the mask, not a FromArg
      --FromIdx;
      ProcessingArg = MaskForExternalFunction;
    } else {
      ProcessingArg = FromArgs[FromIdx];
      if (From) {
        FromArg = From->getArg(FromIdx);
        if (FromArg->hasStructRetAttr()) {
          // Increment only FromIdx
          --ToIdx;
          continue;
        }
      }
    }

    if (ToArg->hasPassPointeeByValueCopyAttr()) {
      if (FromArg && FromArg->hasPassPointeeByValueCopyAttr()) {
        ToCallArgs.push_back(ProcessingArg);
      } else {
        // We need to pass a pointer but have a value, store it!
        irBuilder.SetInsertPoint(F.getEntryBlock().getFirstInsertionPt());
        Value *ArgStorage =
            irBuilder.CreateAlloca(ToArg->getPointeeInMemoryValueType());
        setRippleShape(ArgStorage, ScalarShape);
        irBuilder.SetInsertPoint(CallSite);
        auto *StoreInst = irBuilder.CreateStore(ProcessingArg, ArgStorage);
        setRippleShape(StoreInst, Calling.getArgOperandShape(ToIdx));
        ToSkipMaskingWhenIfConvert.insert(StoreInst);
        ToCallArgs.push_back(ArgStorage);
      }
    } else {
      if (FromArg && FromArg->hasPassPointeeByValueCopyAttr()) {
        // We need to pass by value but have a pointer, load it!
        irBuilder.SetInsertPoint(CallSite);
        auto *RegisterVal =
            irBuilder.CreateLoad(ToArg->getType(), ProcessingArg);
        setRippleShape(RegisterVal, Calling.getArgOperandShape(ToIdx));
        ToSkipMaskingWhenIfConvert.insert(RegisterVal);
        ToCallArgs.push_back(RegisterVal);
      } else {
        ToCallArgs.push_back(ProcessingArg);
      }
    }
  }
  assert(ToCallArgs.size() == To->arg_size());
  irBuilder.SetInsertPoint(CallSite);
  Value *Call = irBuilder.CreateCall(To->getFunctionType(), To, ToCallArgs);
  setRippleShape(Call, Calling.getReturnShape());
  if (ToRetArg && !FromRetArg) {
    // The original call expects a register, load it!
    auto *LoadedRet = irBuilder.CreateLoad(ToRetArg->getParamStructRetType(),
                                           ToCallArgs[ToRetArg->getArgNo()]);
    setRippleShape(LoadedRet, Calling.getReturnShape());
    ToSkipMaskingWhenIfConvert.insert(LoadedRet);
    return LoadedRet;
  }
  return Call;
}

std::map<AssertingVH<const Instruction>, TensorShape>
Ripple::getInstructionToRippleShape() const {
  return InstructionRippleShapes;
}

ExternalRippleFunction::ExternalRippleFunction(
    Function *Fun, StringRef ScalarName,
    const SmallVectorImpl<StringRef> &Options, size_t TensorRank,
    const std::tuple<Type *, TensorShape> &ReturnSignature,
    const SmallVectorImpl<std::tuple<Type *, TensorShape, unsigned>>
        &ArgumentSignatures,
    bool UniformSignature)
    : F(Fun), ScalarName(ScalarName) {

  // 1D Shape from LLVM vector types
  auto ShapeFromType = [TensorRank](const Type *Ty) -> TensorShape::Shape {
    TensorShape::Shape TypeShape(TensorRank, 1u);

    if (const VectorType *VT = dyn_cast<VectorType>(Ty))
      TypeShape[0] = VT->getElementCount().getKnownMinValue();

    return TypeShape;
  };

  auto &[RetTySig, RetShapeSig] = ReturnSignature;
  if (RetTySig) {
    assert(RetShapeSig.rank() <= TensorRank);
    TensorShape::Shape RS(TensorRank, TensorShape::DimSize(1));
    std::copy(RetShapeSig.begin(),
              RetShapeSig.begin() + std::min(static_cast<unsigned>(TensorRank),
                                             RetShapeSig.rank()),
              RS.begin());
    ReturnShape = TensorShape(std::move(RS));
  } else {
    ReturnShape = TensorShape(ShapeFromType(getTrueReturnType()));
  }
  for (auto &Params : getTrueArgumentTypes()) {
    unsigned ArgIdx = ArgShapes.size();
    if (auto Found = llvm::find_if(
            ArgumentSignatures,
            [ArgIdx](auto &Tuple) { return std::get<2>(Tuple) == ArgIdx; });
        Found != ArgumentSignatures.end()) {
      auto &[ArgTySig, ArgShapeSig, ArgIdx] = *Found;
      assert(ArgShapeSig.rank() <= TensorRank);
      TensorShape::Shape AS(TensorRank, TensorShape::DimSize(1));
      std::copy(
          ArgShapeSig.begin(),
          ArgShapeSig.begin() +
              std::min(static_cast<unsigned>(TensorRank), ArgShapeSig.rank()),
          AS.begin());
      ArgShapes.push_back(TensorShape(std::move(AS)));
    } else {
      if (UniformSignature && RetTySig && Params->isVectorTy()) {
        assert(RetShapeSig.rank() <= TensorRank);
        TensorShape::Shape AS(TensorRank, TensorShape::DimSize(1));
        std::copy(
            RetShapeSig.begin(),
            RetShapeSig.begin() +
                std::min(static_cast<unsigned>(TensorRank), RetShapeSig.rank()),
            AS.begin());
        ArgShapes.push_back(TensorShape(std::move(AS)));
      } else {
        ArgShapes.push_back(TensorShape(ShapeFromType(Params)));
      }
    }
  }

  applyOptions(Options);

  if (!IsPureOtherThanSretAndByVal) {
    // Check for purity (in the presence of sret and byval) when the function
    // doesn't use the "_pure" option.
    if (F->hasFnAttribute(Attribute::ReadNone))
      IsPureOtherThanSretAndByVal = true;
    else {
      MemoryEffects MemEff = F->getMemoryEffects();
      if (MemEff.onlyAccessesArgPointees()) {
        // Only derefs arguments:
        // If only sret and byval are being deref, and byval is not passing a
        // pointer (makes little sense but not impossible?), we can consider
        // this function pure enough to be used in a masked call.
        IsPureOtherThanSretAndByVal = all_of(F->args(), [](Argument &Arg) {
          return !Arg.getType()->isPointerTy() || Arg.hasStructRetAttr() ||
                 (Arg.hasByValAttr() &&
                  !Arg.getPointeeInMemoryValueType()->isPointerTy());
        });
      }
    }
  }
  NormalizedFunType = normalizeFunctionType(F, getTensorMaskArgument());
}

bool ExternalRippleFunction::hasTensorMaskArgument() const {
  return TensorMaskArgument != nullptr;
}

bool ExternalRippleFunction::isPureOtherThanSretAndByval() const {
  return IsPureOtherThanSretAndByVal;
}

bool ExternalRippleFunction::isMaskable() const {
  return isPureOtherThanSretAndByval() || hasTensorMaskArgument();
}

void ExternalRippleFunction::print(raw_ostream &ROS) const {
  auto *ExternF = getFunction();
  ROS << "External ripple function \"" << ExternF->getName()
      << "\" with scalar name \"" << scalarFunctionName() << "\":\n";
  if (isMaskable()) {
    ROS << "  Is a maskable";
    if (Argument *MaskArgument = getTensorMaskArgument())
      ROS << " function with mask argument " << *MaskArgument;
    else {
      assert(isPureOtherThanSretAndByval());
      ROS << " pure function (except for sret/byval arguments)";
    }
  } else
    ROS << " Is not maskable!";
  ROS << "\n  Return type and shape: [" << *getTrueReturnType() << ", "
      << getReturnShape() << "]\n"
      << "  Arg types and shapes: (";
  auto ArgTypes = getTrueArgumentTypes();
  for (size_t Idx = 0, E = ArgTypes.size(); Idx < E; ++Idx) {
    if (Idx > 0)
      ROS << ", ";
    ROS << "[" << *ArgTypes[Idx] << ", " << getArgOperandShape(Idx) << "]";
  }
  ROS << "\n  Normalized function type: " << *NormalizedFunType << "\n";
}

Type *ExternalRippleFunction::getTrueMaskType(const Argument *Mask) {
  if (!Mask)
    return nullptr;
  Type *MaskType = Mask->getType();
  if (Mask->hasByValAttr()) {
    MaskType = Mask->getPointeeInMemoryValueType();
  }
  return MaskType;
}

LibFunc Ripple::intrinsicToLibFunc(const IntrinsicInst *II) {
  // TODO: support constrained_ versions of math intrinsics. It affect
  // rounding and exceptions. We will probably have have to expose that as FP
  // metadata or naming convention?
  // https://llvm.org/docs/LangRef.html#constrained-floating-point-intrinsics
  auto FPIntrinsicLibFunc = [](const IntrinsicInst *II, LibFunc LS, LibFunc LD,
                               LibFunc LL) -> LibFunc {
    switch (II->getArgOperand(0)->getType()->getTypeID()) {
    case Type::FloatTyID:
      return LS;
    case Type::DoubleTyID:
      return LD;
    case Type::X86_FP80TyID:
    case Type::FP128TyID:
    case Type::PPC_FP128TyID:
      return LL;
    case Type::HalfTyID:
      return LibFunc::NotLibFunc;
    default:
      llvm_unreachable("Invalid type in intrinsic");
    }
  };
  switch (II->getIntrinsicID()) {
  default:
    return LibFunc::NotLibFunc;
  case Intrinsic::sqrt:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_sqrtf, LibFunc::LibFunc_sqrt,
                              LibFunc::LibFunc_sqrtl);
  // Intrinsic::powi does not have direct equivalent LibFunc function
  case Intrinsic::asin:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_asinf, LibFunc::LibFunc_asin,
                              LibFunc::LibFunc_asinl);
  case Intrinsic::acos:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_acosf, LibFunc::LibFunc_acos,
                              LibFunc::LibFunc_acosl);
  case Intrinsic::atan:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_atanf, LibFunc::LibFunc_atan,
                              LibFunc::LibFunc_atanl);
  case Intrinsic::atan2:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_atan2f,
                              LibFunc::LibFunc_atan2, LibFunc::LibFunc_atan2l);
  case Intrinsic::sin:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_sinf, LibFunc::LibFunc_sin,
                              LibFunc::LibFunc_sinl);
  case Intrinsic::cos:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_cosf, LibFunc::LibFunc_cos,
                              LibFunc::LibFunc_cosl);
  case Intrinsic::tan:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_tanf, LibFunc::LibFunc_tan,
                              LibFunc::LibFunc_tanl);
  case Intrinsic::sinh:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_sinhf, LibFunc::LibFunc_sinh,
                              LibFunc::LibFunc_sinhl);
  case Intrinsic::cosh:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_coshf, LibFunc::LibFunc_cosh,
                              LibFunc::LibFunc_coshl);
  case Intrinsic::tanh:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_tanhf, LibFunc::LibFunc_tanh,
                              LibFunc::LibFunc_tanhl);
  case Intrinsic::pow:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_powf, LibFunc::LibFunc_pow,
                              LibFunc::LibFunc_powl);
  case Intrinsic::log:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_logf, LibFunc::LibFunc_log,
                              LibFunc::LibFunc_logl);
  case Intrinsic::log10:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_log10f,
                              LibFunc::LibFunc_log10, LibFunc::LibFunc_log10l);
  case Intrinsic::log2:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_log2f, LibFunc::LibFunc_log2,
                              LibFunc::LibFunc_log2l);
  case Intrinsic::exp:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_expf, LibFunc::LibFunc_exp,
                              LibFunc::LibFunc_expl);
  case Intrinsic::exp10:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_exp10f,
                              LibFunc::LibFunc_exp10, LibFunc::LibFunc_exp10l);
  case Intrinsic::exp2:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_exp2f, LibFunc::LibFunc_exp2,
                              LibFunc::LibFunc_exp2l);
  case Intrinsic::fabs:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_fabsf,
                              LibFunc::LibFunc_exp10, LibFunc::LibFunc_fabsl);
  case Intrinsic::copysign:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_copysignf,
                              LibFunc::LibFunc_copysign,
                              LibFunc::LibFunc_copysignl);
  case Intrinsic::floor:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_floorf,
                              LibFunc::LibFunc_floor, LibFunc::LibFunc_floorl);
  case Intrinsic::ceil:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_ceilf, LibFunc::LibFunc_ceil,
                              LibFunc::LibFunc_ceill);
  case Intrinsic::trunc:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_truncf,
                              LibFunc::LibFunc_trunc, LibFunc::LibFunc_truncl);
  case Intrinsic::rint:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_rintf, LibFunc::LibFunc_rint,
                              LibFunc::LibFunc_rintl);
  case Intrinsic::nearbyint:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_nearbyintf,
                              LibFunc::LibFunc_nearbyint,
                              LibFunc::LibFunc_nearbyintl);
  case Intrinsic::round:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_roundf,
                              LibFunc::LibFunc_round, LibFunc::LibFunc_roundl);
  case Intrinsic::roundeven:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_roundevenf,
                              LibFunc::LibFunc_roundeven,
                              LibFunc::LibFunc_roundevenl);
  case Intrinsic::sincos:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_sincosf,
                              LibFunc::LibFunc_sincos,
                              LibFunc::LibFunc_sincosl);
  // Intrinsic::sincospi has no direct mapping in LibFunc
  case Intrinsic::modf:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_modff, LibFunc::LibFunc_modf,
                              LibFunc::LibFunc_modfl);
  // Intrinsic::fptrunc has no direct mapping to LibFunc
  // Intrinsic::canonicalize has no direct mapping to LibFunc
  // Intrinsic::arithmetic_fence has no direct mapping to LibFunc
  // Intrinsic::lround has no direct mapping to LibFunc
  // Intrinsic::llround has no direct mapping to LibFunc
  // Intrinsic::lrint has no direct mapping to LibFunc
  // Intrinsic::llrint has no direct mapping to LibFunc
  case Intrinsic::ldexp:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_ldexpf,
                              LibFunc::LibFunc_ldexp, LibFunc::LibFunc_ldexpl);
  case Intrinsic::frexp:
    return FPIntrinsicLibFunc(II, LibFunc::LibFunc_frexpf,
                              LibFunc::LibFunc_frexp, LibFunc::LibFunc_frexpl);
  }
}
StringRef Ripple::intrinsicToLibName(const IntrinsicInst *II,
                                     const TargetLibraryInfo &TLI) {
  auto FPIntrinsicName = [](const IntrinsicInst *II, StringRef LH = {},
                            StringRef LS = {}, StringRef LD = {},
                            StringRef LL = {}) -> StringRef {
    switch (II->getArgOperand(0)->getType()->getTypeID()) {
    default:
      llvm_unreachable("Invalid type in intrinsic");
    case Type::FloatTyID:
      return LS;
    case Type::DoubleTyID:
      return LD;
    case Type::HalfTyID:
      return LH;
    case Type::X86_FP80TyID:
    case Type::FP128TyID:
    case Type::PPC_FP128TyID:
      return LL;
    }
  };
  LibFunc LF = intrinsicToLibFunc(II);
  if (LF != LibFunc::NotLibFunc) {
    auto Name = TLI.getName(LF);
    if (!Name.empty()) {
      LLVM_DEBUG(dbgs() << "Libfunc name for " << *II << " found as " << Name
                        << '\n');
      return Name;
    }
  }
  LLVM_DEBUG(dbgs() << "Libfunc name for " << *II << " not found!\n");
  // Extrapolate for extra functions not part of TargetLibraryInfo
#define LIB_MATH_NAMES(Name) Name "f16", Name "f", Name, Name "l"
  switch (II->getIntrinsicID()) {
  default:
    break;
  case Intrinsic::lround:
    return FPIntrinsicName(II, LIB_MATH_NAMES("lround"));
  case Intrinsic::llround:
    return FPIntrinsicName(II, LIB_MATH_NAMES("llround"));
  case Intrinsic::lrint:
    return FPIntrinsicName(II, LIB_MATH_NAMES("lrint"));
  case Intrinsic::llrint:
    return FPIntrinsicName(II, LIB_MATH_NAMES("llrint"));
  case Intrinsic::sqrt:
    return FPIntrinsicName(II, LIB_MATH_NAMES("sqrt"));
  case Intrinsic::asin:
    return FPIntrinsicName(II, LIB_MATH_NAMES("asin"));
  case Intrinsic::acos:
    return FPIntrinsicName(II, LIB_MATH_NAMES("acos"));
  case Intrinsic::atan:
    return FPIntrinsicName(II, LIB_MATH_NAMES("atan"));
  case Intrinsic::atan2:
    return FPIntrinsicName(II, LIB_MATH_NAMES("atan2"));
  case Intrinsic::sin:
    return FPIntrinsicName(II, LIB_MATH_NAMES("sin"));
  case Intrinsic::cos:
    return FPIntrinsicName(II, LIB_MATH_NAMES("cos"));
  case Intrinsic::tan:
    return FPIntrinsicName(II, LIB_MATH_NAMES("tan"));
  case Intrinsic::sinh:
    return FPIntrinsicName(II, LIB_MATH_NAMES("sinh"));
  case Intrinsic::cosh:
    return FPIntrinsicName(II, LIB_MATH_NAMES("cosh"));
  case Intrinsic::tanh:
    return FPIntrinsicName(II, LIB_MATH_NAMES("tanh"));
  case Intrinsic::pow:
    return FPIntrinsicName(II, LIB_MATH_NAMES("pow"));
  case Intrinsic::log:
    return FPIntrinsicName(II, LIB_MATH_NAMES("log"));
  case Intrinsic::log10:
    return FPIntrinsicName(II, LIB_MATH_NAMES("log10"));
  case Intrinsic::log2:
    return FPIntrinsicName(II, LIB_MATH_NAMES("log2"));
  case Intrinsic::exp:
    return FPIntrinsicName(II, LIB_MATH_NAMES("exp"));
  case Intrinsic::exp2:
    return FPIntrinsicName(II, LIB_MATH_NAMES("exp2"));
  case Intrinsic::exp10:
    return FPIntrinsicName(II, LIB_MATH_NAMES("exp10"));
  case Intrinsic::fabs:
    return FPIntrinsicName(II, LIB_MATH_NAMES("fabs"));
  case Intrinsic::copysign:
    return FPIntrinsicName(II, LIB_MATH_NAMES("copysign"));
  case Intrinsic::floor:
    return FPIntrinsicName(II, LIB_MATH_NAMES("floor"));
  case Intrinsic::ceil:
    return FPIntrinsicName(II, LIB_MATH_NAMES("ceil"));
  case Intrinsic::trunc:
    return FPIntrinsicName(II, LIB_MATH_NAMES("trunc"));
  case Intrinsic::rint:
    return FPIntrinsicName(II, LIB_MATH_NAMES("rint"));
  case Intrinsic::nearbyint:
    return FPIntrinsicName(II, LIB_MATH_NAMES("nearbyint"));
  case Intrinsic::round:
    return FPIntrinsicName(II, LIB_MATH_NAMES("round"));
  case Intrinsic::roundeven:
    return FPIntrinsicName(II, LIB_MATH_NAMES("roundeven"));
  case Intrinsic::sincos:
    return FPIntrinsicName(II, LIB_MATH_NAMES("sincos"));
  case Intrinsic::modf:
    return FPIntrinsicName(II, LIB_MATH_NAMES("modf"));
  case Intrinsic::ldexp:
    return FPIntrinsicName(II, LIB_MATH_NAMES("ldexp"));
  case Intrinsic::frexp:
    return FPIntrinsicName(II, LIB_MATH_NAMES("frexp"));
  }
#undef LIB_MATH_NAMES
  return StringRef();
}

StringRef Ripple::intrinsicToLibName(const IntrinsicInst *II) const {
  return intrinsicToLibName(II, targetLibraryInfo);
}

static constexpr StringRef RippleFunShapeMetaKey("RippleFunctionShapeMetadata");

void Ripple::setFunctionShapeMetadata(Function &F,
                                      ArrayRef<const TensorShape *> ArgShapes,
                                      const TensorShape &ReturnShape) {
  assert(ArgShapes.size() == F.arg_size());
  SmallVector<Metadata *, 8> OperandsAndReturnShapesMeta;
  for (auto *Shape : ArgShapes)
    OperandsAndReturnShapesMeta.push_back(
        Shape->toConstMetadata(irBuilder.getInt64Ty()));
  OperandsAndReturnShapesMeta.push_back(
      ReturnShape.toConstMetadata(irBuilder.getInt64Ty()));
  MDNode *FunctionShapeMetadata =
      MDNode::get(F.getContext(), OperandsAndReturnShapesMeta);
  F.setMetadata(RippleFunShapeMetaKey, FunctionShapeMetadata);
}

bool Ripple::getFunctionShapeMetadata(
    const Function &F, SmallVectorImpl<std::unique_ptr<TensorShape>> &ArgShapes,
    std::unique_ptr<TensorShape> &ReturnShape) const {

  auto metaToUniquePtr = [this](const Metadata *Meta,
                                std::unique_ptr<TensorShape> &TShape) -> bool {
    if (auto *MDN = dyn_cast<MDNode>(Meta)) {
      if (auto TS = TensorShape::fromConstMetadata(tensorRank(), MDN)) {
        std::swap(TShape, TS);
        return true;
      }
    }
    return false;
  };

  if (!hasRippleShapeMetadata(F))
    return false;

  auto *RippleShapeMeta = F.getMetadata(RippleFunShapeMetaKey);

  assert(F.arg_size() <= RippleShapeMeta->getNumOperands());
  for (auto &Operand : make_range(RippleShapeMeta->op_begin(),
                                  RippleShapeMeta->op_begin() + F.arg_size())) {
    ArgShapes.push_back(nullptr);
    if (!metaToUniquePtr(Operand.get(), ArgShapes.back()))
      return false;
  }
  assert(F.arg_size() + 1 == RippleShapeMeta->getNumOperands() &&
         "Malformed ripple function shape metadata: should have 1 shape for "
         "each argument and 1 for the return");
  if (!metaToUniquePtr(RippleShapeMeta->getOperand(F.arg_size()), ReturnShape))
    return false;
  if (F.getReturnType()->isVoidTy())
    assert(ReturnShape->isScalar());
  return true;
}

bool Ripple::hasRippleShapeMetadata(const Function &F) {
  return F.getMetadata(RippleFunShapeMetaKey) != nullptr;
}

bool Ripple::canBeSpecialized(const Function *F) {
  // TODO: we can probably support varible argument functions later
  return F && F->getIntrinsicID() == Intrinsic::not_intrinsic &&
         !F->isDeclaration() && !F->isInterposable() && !F->isVarArg();
}

bool Ripple::shouldWaitForVoidReturnSpecialization(const Function &Spec) const {
  if (Spec.getReturnType()->isVoidTy()) {
    assert(SpecializationsAvailable.contains(const_cast<Function *>(&Spec)));
    return !isPendingRippleSpecialization(F) && Spec.isDeclaration();
  }
  return false;
}

bool Ripple::isPendingRippleSpecialization(const Function &F) const {
  return SpecializationsPending.contains(const_cast<Function *>(&F));
}

static constexpr StringRef RippleSpecializationPrefix("ripple.specialization.");

std::pair<Function *, TensorShape>
Ripple::getRippleSpecializationFor(const CallInst &Call) const {
  std::pair<Function *, TensorShape> NoMatch(nullptr, ScalarShape);
  if (!Call.getCalledFunction())
    return NoMatch;

  auto checkCandidate =
      [&](Function &Candidate) -> std::pair<Function *, TensorShape> {
    // We look for functions specialization with name
    // "ripple.specialization.XX.name"
    auto OriginalName = getOriginalName(Candidate.getName());
    assert(!OriginalName.empty() &&
           "We should only check specialization candidates");
    if (OriginalName == Call.getCalledFunction()->getName()) {
      // Check if the input tensors match the function call
      SmallVector<std::unique_ptr<TensorShape>, 8> ArgShapes;
      std::unique_ptr<TensorShape> ReturnShape;
      LLVM_DEBUG(dbgs() << "Reading metadata from " << Candidate.getName()
                        << "\n");
      if (getFunctionShapeMetadata(Candidate, ArgShapes, ReturnShape)) {
        LLVM_DEBUG(dbgs() << "Got shape for function!\n");
        // When waiting for specialization, the function has twice the number of
        // arguments
        assert(Call.arg_size() *
                   (1 + isPendingRippleSpecialization(Candidate)) ==
               ArgShapes.size());

        bool Match = true;
        for (unsigned Idx = 0, E = Call.arg_size(); Match && Idx < E; ++Idx)
          if (*ArgShapes[Idx] != getRippleShape(Call.getArgOperand(Idx)))
            Match = false;

        if (Match) {
          LLVM_DEBUG(dbgs() << "Matching ripple specialization: "
                            << Candidate.getName() << ", return shape "
                            << *ReturnShape << "!\n");
          return {&Candidate, *ReturnShape};
        }
      } else {
        llvm_unreachable("a Specialization is missing a shape metadata");
      }
    }
    return NoMatch;
  };
  for (Function *Candidate : SpecializationsAvailable) {
    auto Match = checkCandidate(*Candidate);
    if (Match.first)
      return Match;
  }
  for (Function *Candidate : SpecializationsPending) {
    auto Match = checkCandidate(*Candidate);
    if (Match.first)
      return Match;
  }
  return NoMatch;
}

static constexpr StringRef RippleFinalSpecializationPrefix("final.");
std::string Ripple::getUniqueSpecializationName(StringRef OriginalName,
                                                bool Final) {
  static std::atomic<unsigned> UniqueRippleSpecializationCounter{0};
  std::string NewSpecializationName =
      RippleSpecializationPrefix.str() +
      (Final ? RippleFinalSpecializationPrefix.str() : "") +
      std::to_string(UniqueRippleSpecializationCounter++) + "." +
      OriginalName.str();
  return NewSpecializationName;
}

StringRef Ripple::getOriginalName(StringRef SpecializationName) {
  if (!SpecializationName.starts_with(RippleSpecializationPrefix))
    return StringRef();

  auto SpecName = SpecializationName.substr(RippleSpecializationPrefix.size());
  if (SpecName.starts_with(RippleFinalSpecializationPrefix))
    SpecName = SpecName.substr(RippleFinalSpecializationPrefix.size());
  auto OriginName = SpecName.split(".").second;
  return OriginName;
}

bool Ripple::requestSpecializationFor(const CallInst &Call) {
  Function *CalledFunction = Call.getCalledFunction();
  assert(getRippleSpecializationFor(Call).first == nullptr);
  assert(!CalledFunction->isDeclaration());

  SmallVector<const TensorShape *> ArgShapes;
  ArgShapes.reserve(Call.arg_size() * 2);
  llvm::transform(Call.args(), std::back_inserter(ArgShapes),
                  [&](auto &Arg) { return &getRippleShape(Arg.get()); });

  auto SpecMaskShape = TensorShape::broadcastShapeFromAll(ArgShapes);
  if (!SpecMaskShape) {
    consumeError(SpecMaskShape.takeError());
    std::string ErrMsg;
    {
      llvm::raw_string_ostream RSO(ErrMsg);
      RSO << "The function " << CalledFunction->getName()
          << " does not follow the ripple broadcast rule: "
          << " the operands tensor shapes are not "
             "broadcast-compatible: ["
          << *ArgShapes[0];
      for_each(make_range(std::next(ArgShapes.begin()), ArgShapes.end()),
               [&](auto *TShape) { RSO << ", " << *TShape; });
      RSO << "]";
    }
    DiagnosticInfoRippleWithLoc DI(DS_Warning, F, sanitizeRippleLocation(&Call),
                                   ErrMsg);
    F.getContext().diagnose(DI);
    return false;
  }

  // Clone the function
  Function *FClone;
  {
    ValueToValueMapTy VMap;

    // We clone the function into a temporary function that will be specialized.
    // To do so we duplicate the arguments, the first half are the original
    // function arguments and the next half are the "ripple tensor arguments".
    // We eliminate the original arguments once the specialization has been
    // processed.
    std::vector<Type *> ArgTypes;
    // The original argument types
    for (const Argument &Arg : CalledFunction->args())
      ArgTypes.push_back(Arg.getType());

    // The Ripple argument types
    for (const Argument &Arg : CalledFunction->args()) {
      auto &TShape = getRippleShape(Call.getOperand(Arg.getArgNo()));
      if (TShape.isVector()) {
        // If the argument is a vector we can't promote!
        if (Arg.getType()->isVectorTy() ||
            ((Arg.hasByValAttr() || Arg.hasStructRetAttr()) &&
             Arg.getPointeeInMemoryValueType()->isVectorTy()))
          llvm_unreachable("TODO: migrate this to an error that we cannot "
                           "promote this function's argument");
        Type *PromotedType = VectorType::get(Arg.getType(), TShape.flatShape(),
                                             /*IsScalable*/ false);
        ArgTypes.push_back(PromotedType);
      } else {
        ArgTypes.push_back(Arg.getType());
      }
    }

    FunctionType *FTy = FunctionType::get(CalledFunction->getReturnType(),
                                          ArgTypes, /*IsVarArg*/ false);
    FClone = Function::Create(
        FTy, CalledFunction->getLinkage(), CalledFunction->getAddressSpace(),
        getUniqueSpecializationName(CalledFunction->getName()),
        CalledFunction->getParent());

    // Setup the cloning VMap for arguments
    for (const Argument &Arg : CalledFunction->args())
      VMap[&Arg] = FClone->getArg(Arg.getArgNo());

    SmallVector<ReturnInst *, 8> Returns; // Ignore returns cloned.
    CloneFunctionInto(FClone, CalledFunction, VMap,
                      CloneFunctionChangeType::LocalChangesOnly, Returns, "",
                      nullptr);
    FClone->setVisibility(GlobalValue::DefaultVisibility);
    FClone->setLinkage(GlobalValue::InternalLinkage);
  }
  // Make it a fast call since we are not bound by any ABI!
  FClone->setCallingConv(CallingConv::Fast);
  // Mark for specialization
  SpecializationsPending.insert(FClone);
  {
    // The call arguments that are ripple.set.shape
    SmallVector<std::pair<unsigned, IntrinsicInst *>, 4> BlockShapeAsArg;
    specializationCallBlockShapeArgs(&Call, BlockShapeAsArg);
    // Insert the block shapes that are passed as parameter into the function we
    // are specializing so that the ripple pass can do its job
    auto InsertPoint = FClone->getEntryBlock().getFirstInsertionPt();

    // Used by RemapInstruction to fix the instruction's metadata
    ValueToValueMapTy VMap;
    if (F.getSubprogram())
      if (FClone->getSubprogram())
        VMap.MD()[F.getSubprogram()] = TrackingMDRef(FClone->getSubprogram());

    for (auto &[ArgIdx, BlockShapeII] : BlockShapeAsArg) {
      Instruction *SetBlockShapeClone = BlockShapeII->clone();
      SetBlockShapeClone->insertBefore(InsertPoint);
      RemapInstruction(SetBlockShapeClone, VMap, RF_None);
      FClone->getArg(ArgIdx)->replaceAllUsesWith(SetBlockShapeClone);
    }
  }

  // Copy the shapes for the ripple arguments
  llvm::copy(ArgShapes, std::back_inserter(ArgShapes));
  setFunctionShapeMetadata(*FClone, ArgShapes, ScalarShape);
  LLVM_DEBUG(dbgs() << "Requesting specialization for call " << Call << ":";
             FClone->print(dbgs()));

  {
    // Insert dummy (unused) block shapes so that ripple will run and will
    // select this function's PE rank as minimum for its own PE rank
    IRBuilder<> IRB(&*FClone->getEntryBlock().getFirstInsertionPt());
    auto *Int64Ty = IRB.getInt64Ty();
    auto *One = ConstantInt::get(Int64Ty, 1, false);
    auto *Two = ConstantInt::get(Int64Ty, 2, false);
    for (auto &[PEId, PERank] : PERanks) {
      SmallVector<Value *, RippleIntrinsicsMaxDims + 1> RippleSetShapeArgs;
      RippleSetShapeArgs.push_back(ConstantInt::get(Int64Ty, PEId, false));
      for (unsigned i = 0; i < RippleIntrinsicsMaxDims; ++i)
        RippleSetShapeArgs.push_back(i < PERank ? Two : One);
      IRB.CreateIntrinsic(Intrinsic::ripple_block_setshape, {Int64Ty},
                          RippleSetShapeArgs);
    }
  }

  // When the specialization returns void we preemptively create the declaration
  // of the final specialization (because we have all the type info necessary
  // This allows us to support cycles of void function specialization.
  if (CalledFunction->getReturnType()->isVoidTy()) {
    std::vector<Type *> ArgTypes;
    // Get the final vector argument types from FClone
    for (unsigned ArgIdx = 0, E = CalledFunction->arg_size(); ArgIdx < E;
         ++ArgIdx)
      ArgTypes.push_back(FClone->getArg(ArgIdx + E)->getType());
    // for void we create the final call here so that we can handle cycles
    FunctionType *FTy = FunctionType::get(CalledFunction->getReturnType(),
                                          ArgTypes, /*IsVarArg*/ false);
    Function *ProcessedSpecialization = Function::Create(
        FTy, GlobalValue::InternalLinkage, CalledFunction->getAddressSpace(),
        getUniqueSpecializationName(CalledFunction->getName(), /*Final*/ true),
        CalledFunction->getParent());
    ProcessedSpecialization->copyAttributesFrom(FClone);
    // Mark this declaration as the destination of the final specialization
    registerFinalFunctionNameMetadata(FClone, ProcessedSpecialization);
    ArgShapes.resize(ArgShapes.size() / 2);
    setFunctionShapeMetadata(*ProcessedSpecialization, ArgShapes, ScalarShape);
    // Make this final declaration as an available specialization, we don't need
    // to wait for the specialization to be processed for it to be used
    SpecializationsAvailable.insert(ProcessedSpecialization);

    // Create the masked declaration of this specialization
    assert(!SpecMaskShape->isScalar());
    VectorType *SpecMaskType = VectorType::get(
        irBuilder.getInt1Ty(), SpecMaskShape->flatShape(), /*Scalable*/ false);
    ArgTypes.push_back(SpecMaskType);
    FunctionType *MaskedFTy = FunctionType::get(CalledFunction->getReturnType(),
                                                ArgTypes, /*IsVarArg*/ false);
    Function *MaskedSpecializationFinal =
        Function::Create(MaskedFTy, GlobalValue::InternalLinkage,
                         CalledFunction->getAddressSpace(),
                         getMaskedSpecializationName(*ProcessedSpecialization),
                         CalledFunction->getParent());
    ArgShapes.push_back(&*SpecMaskShape);
    setFunctionShapeMetadata(*MaskedSpecializationFinal, ArgShapes,
                             ScalarShape);
  }
  return true;
}

void Ripple::finishSpecialization() {
  assert(isPendingRippleSpecialization(F));
  const TensorShape *ReturnShape = &ScalarShape;
  Type *ReturnType = Type::getVoidTy(F.getContext());
  if (!F.getReturnType()->isVoidTy()) {
    for (const auto &I : instructions(F))
      if (auto *Return = dyn_cast<ReturnInst>(&I)) {
        ReturnShape = &getRippleShape(Return);
        break;
      }
    if (ReturnShape->isVector()) {
      ReturnType = VectorType::get(F.getReturnType(), ReturnShape->flatShape(),
                                   /*Scalable*/ false);
      // Remove scalar attributes that don't apply to vector types
      AttributeMask Mask;
      Mask.addAttribute(Attribute::AttrKind::ZExt);
      Mask.addAttribute(Attribute::AttrKind::SExt);
      Mask.addAttribute(Attribute::AttrKind::NoExt);
      Mask.addAttribute(Attribute::AttrKind::InReg);
      Mask.addAttribute(Attribute::AttrKind::NoUndef);
      F.removeRetAttrs(Mask);
    } else
      ReturnType = F.getReturnType();
  }
  SmallVector<const TensorShape *> ArgumentShapes;
  ArgumentShapes.reserve(F.arg_size() / 2);
  // We get the tensor shape of the ripple arguments
  for (unsigned ArgIdx = 0, E = F.arg_size() / 2; ArgIdx < E; ArgIdx++) {
    Argument *RippleArg = F.getArg(E + ArgIdx);
    ArgumentShapes.push_back(&getRippleShape(RippleArg));
  }
  auto SpecMaskShape = TensorShape::broadcastShapeFromAll(ArgumentShapes);
  // This is checked when requesting specialization construction
  if (!SpecMaskShape)
    llvm_unreachable("We shouldn't be constructing ripple specializations when "
                     "argument tensor shapes don't broadcast");
  TensorShape &MaskShape = SpecMaskShape.get();
  Function *ProcessedSpecialization = nullptr;
  Function *MaskedSpecialization = nullptr;
  if (ReturnType->isVoidTy()) {
    // For void returning function we pre-created the final function declaration
    // to allow processing in the presence of cycles (shape propagation does not
    // care in this case)
    Function *FinalDecl = retrieveFinalSpecializationDecl(&F);
    assert(FinalDecl && FinalDecl->isDeclaration() &&
           "Pending void returning ripple specialization "
           "must have a final name attached");
    ProcessedSpecialization = FinalDecl;
    MaskedSpecialization = getMaskedSpecialization(*ProcessedSpecialization);
    assert(ProcessedSpecialization &&
           "Missing ripple final specialization function declaration");
  } else {
    // Ripple argument types
    SmallVector<Type *> ArgumentTypes;
    ArgumentTypes.reserve(F.arg_size() / 2);
    for (unsigned ArgIdx = 0, E = F.arg_size() / 2; ArgIdx < E; ArgIdx++)
      ArgumentTypes.push_back(F.getArg(E + ArgIdx)->getType());
    FunctionType *FTy =
        FunctionType::get(ReturnType, ArgumentTypes, /*IsVarArg*/ false);
    // Two functions can't have the same name so we add a "final" to the
    // ripple specialization prefix
    ProcessedSpecialization =
        Function::Create(FTy, GlobalValue::InternalLinkage, F.getAddressSpace(),
                         getUniqueSpecializationName(
                             getOriginalName(F.getName()), /*Final*/ true),
                         F.getParent());
    SpecializationsAvailable.insert(ProcessedSpecialization);

    // Create the masked declaration of this specialization
    VectorType *SpecMaskType = VectorType::get(
        irBuilder.getInt1Ty(), MaskShape.flatShape(), /*Scalable*/ false);
    ArgumentTypes.push_back(SpecMaskType);
    FunctionType *MaskedFTy =
        FunctionType::get(ReturnType, ArgumentTypes, /*IsVarArg*/ false);
    MaskedSpecialization = Function::Create(
        MaskedFTy, GlobalValue::InternalLinkage, F.getAddressSpace(),
        getMaskedSpecializationName(*ProcessedSpecialization), F.getParent());
  }
  assert(MaskedSpecialization && ProcessedSpecialization);

  {
    ValueToValueMapTy VMap;
    for (unsigned ArgIdx = 0, E = ProcessedSpecialization->arg_size();
         ArgIdx < E; ++ArgIdx) {
      assert(F.getArg(ArgIdx)->getNumUses() == 0);
      Argument *NewArg = ProcessedSpecialization->getArg(ArgIdx);
      VMap[F.getArg(ArgIdx)] = NewArg;
      VMap[F.getArg(ArgIdx + E)] = NewArg;
    }
    SmallVector<ReturnInst *, 8> Returns; // Ignore returns cloned.
    // let cloneFunction handle all the metadata mapping, etc instead of
    // re-implementing it here
    CloneFunctionInto(ProcessedSpecialization, &F, VMap,
                      CloneFunctionChangeType::LocalChangesOnly, Returns, "",
                      nullptr);
  }

  eraseFunctionSpecializationRelatedMetadata(*ProcessedSpecialization);
  setFunctionShapeMetadata(*ProcessedSpecialization, ArgumentShapes,
                           *ReturnShape);
  LLVM_DEBUG(dbgs() << "\nFinal specialization of " << F.getName() << ":\n";
             ProcessedSpecialization->print(dbgs()));

  // Create a temporary mask for whole function masking. After cloning the
  // function, we will replace this instruction by the argument (which does not
  // exist here)!
  irBuilder.SetInsertPoint(F.getEntryBlock().getFirstNonPHIOrDbgOrAlloca());
  SelectInst *TemporaryMask = irBuilder.Insert(createMaskSelectToTrueFalse(
      irBuilder.getInt1Ty(),
      ElementCount::get(MaskShape.flatShape(), /*Scalable*/ false),
      "ripple.specialization.mask"));
  setRippleShape(TemporaryMask, MaskShape);

  SmallVector<BasicBlock *> BBs;
  auto refToPtr = [](BasicBlock &BB) { return &BB; };
  applyMaskToOps(make_range(map_iterator(F.begin(), refToPtr),
                            map_iterator(F.end(), refToPtr)),
                 TemporaryMask, MaskShape,
                 &*std::next(TemporaryMask->getIterator()));
  {
    ValueToValueMapTy VMap;
    for (unsigned ArgIdx = 0, E = MaskedSpecialization->arg_size() - 1;
         ArgIdx < E; ++ArgIdx) {
      assert(F.getArg(ArgIdx)->getNumUses() == 0);
      Argument *NewArg = MaskedSpecialization->getArg(ArgIdx);
      VMap[F.getArg(ArgIdx)] = NewArg;
      VMap[F.getArg(ArgIdx + E)] = NewArg;
    }
    Argument *MaskArg =
        MaskedSpecialization->getArg(MaskedSpecialization->arg_size() - 1);
    assert(TemporaryMask->getType() == MaskArg->getType());
    SmallVector<ReturnInst *, 8> Returns; // Ignore returns cloned.
    // let cloneFunction handle all the metadata mapping, etc instead of
    // re-implementing it here
    CloneFunctionInto(MaskedSpecialization, &F, VMap,
                      CloneFunctionChangeType::LocalChangesOnly, Returns, "",
                      nullptr);
    // Replace the use of the temporary mask by the argument!
    auto TemporaryMaskMapping =
        cast<Instruction>(VMap[TemporaryMask])->getIterator();
    ReplaceInstWithValue(TemporaryMaskMapping, MaskArg);
  }

  ArgumentShapes.push_back(&MaskShape);
  setFunctionShapeMetadata(*MaskedSpecialization, ArgumentShapes, *ReturnShape);
  LLVM_DEBUG(dbgs() << "\nMasked specialization of " << F.getName() << ":\n";
             MaskedSpecialization->print(dbgs()));
}

Function *
Ripple::specializationOriginalFunction(const Function &Specialization) {
  auto OriginalName = getOriginalName(Specialization.getName());
  if (OriginalName.empty())
    return nullptr;
  return Specialization.getParent()->getFunction(OriginalName);
}

static constexpr StringRef
    RippleSpecializationFinalName("ripple.specialization.final.fname");

void Ripple::registerFinalFunctionNameMetadata(
    Function *PendingSpecialization, const Function *FinalSpecialization) {
  assert(isPendingRippleSpecialization(*PendingSpecialization) &&
         PendingSpecialization->getReturnType()->isVoidTy());
  PendingSpecialization->setMetadata(
      RippleSpecializationFinalName,
      MDNode::get(PendingSpecialization->getContext(),
                  MDString::get(PendingSpecialization->getContext(),
                                FinalSpecialization->getName())));
}

Function *Ripple::retrieveFinalSpecializationDecl(
    const Function *PendingSpecialization) const {
  if (!PendingSpecialization)
    return nullptr;
  assert(isPendingRippleSpecialization(*PendingSpecialization) &&
         PendingSpecialization->getReturnType()->isVoidTy());
  if (MDNode *MDN =
          PendingSpecialization->getMetadata(RippleSpecializationFinalName)) {
    if (MDString *MDS = dyn_cast<MDString>(MDN->getOperand(0))) {
      Function *FinalSpec =
          PendingSpecialization->getParent()->getFunction(MDS->getString());
      assert(FinalSpec->isDeclaration());
      return FinalSpec;
    }
  }
  return nullptr;
}

void Ripple::eraseFunctionSpecializationRelatedMetadata(Function &F) {
  unsigned KindID = F.getContext().getMDKindID(RippleFunShapeMetaKey);
  F.eraseMetadata(KindID);
  KindID = F.getContext().getMDKindID(RippleSpecializationFinalName);
  F.eraseMetadata(KindID);
}

std::string
Ripple::getMaskedSpecializationName(const Function &Specialization) const {
  assert(Specialization.getName().starts_with(RippleSpecializationPrefix) &&
         "This function doen't use the ripple specialization name mangling");
  return "masked." + Specialization.getName().str();
}

Function *
Ripple::getMaskedSpecialization(const Function &Specialization) const {
  return Specialization.getParent()->getFunction(
      getMaskedSpecializationName(Specialization));
}

IntegerType *Ripple::getSpecializationMaskElementType() {
  return irBuilder.getInt1Ty();
}

template <typename IteratorT>
Expected<TensorShape>
Ripple::getSpecializationMaskShape(llvm::iterator_range<IteratorT> Args) const {
  SmallVector<const TensorShape *, 16> ArgShapes;
  llvm::transform(Args, std::back_inserter(ArgShapes),
                  [&](auto &Arg) { return &getRippleShape(Arg.get()); });

  return TensorShape::broadcastShapeFromAll(ArgShapes);
}

template <bool ReportAmbiguity>
IntrinsicInst *Ripple::getBlockShapeIntrinsic(const Use &RippleBlockShapePtr) {
  std::array<Instruction *, 2> Clobbering = {};
  auto diagnoseAmbiguity = [this, &Clobbering](Instruction *BSAccess) {
    {
      DiagnosticInfoRippleWithLoc DI(
          DS_Error, F, sanitizeRippleLocation(BSAccess),
          "block shape access is ambiguous (multiple shapes apply)");
      F.getContext().diagnose(DI);
    }
    {
      DiagnosticInfoRippleWithLoc DI(DS_Note, F,
                                     sanitizeRippleLocation(Clobbering[0]),
                                     "can come from here");
      F.getContext().diagnose(DI);
    }
    {
      DiagnosticInfoRippleWithLoc DI(
          DS_Note, F, sanitizeRippleLocation(Clobbering[1]), "and here");
      F.getContext().diagnose(DI);
    }
  };
  // Avoid infinite visits w/ cycles
  SmallPtrSet<Value *, 8> Visited;
  std::function<IntrinsicInst *(Value *)> getBlockShapeIntrinsicHelper;
  getBlockShapeIntrinsicHelper = [&](Value *BSPtr) -> IntrinsicInst * {
    if (!Visited.insert(BSPtr).second)
      return nullptr;
    // Most likely case in SSA
    if (auto *II = intrinsicWithId(dyn_cast<Instruction>(BSPtr),
                                   {Intrinsic::ripple_block_setshape})) {
      return II;
      // Most likely case in O0, coming from clang
    } else if (auto *LoadBS = dyn_cast<LoadInst>(BSPtr)) {
      IntrinsicInst *BS = nullptr;
      visitAllClobberingInstructions(
          cast<MemoryUse>(MemSSA.getMemoryAccess(LoadBS)),
          [&](Instruction *Clobber) -> bool {
            LLVM_DEBUG(dbgs() << "Visiting clobber " << *Clobber << "\n");
            if (auto *Store = dyn_cast<StoreInst>(Clobber)) {
              Value *RippleSetShape = Store->getValueOperand();
              if (auto *II =
                      intrinsicWithId(dyn_cast<Instruction>(RippleSetShape),
                                      {Intrinsic::ripple_block_setshape})) {
                LLVM_DEBUG(dbgs()
                           << "Found BS stored by clobber " << *II << "\n");
                if (ReportAmbiguity && BS) {
                  // Multiple clobbers
                  Clobbering[1] = Store;
                  diagnoseAmbiguity(LoadBS);
                  BS = nullptr;
                  return false;
                }
                Clobbering[0] = Store;
                BS = II;
                // Continue only if we look for ambiguity (another clobber)
                return ReportAmbiguity;
              } else if (auto *Load = dyn_cast<LoadInst>(RippleSetShape)) {
                IntrinsicInst *FoundBS = getBlockShapeIntrinsicHelper(Load);
                if (ReportAmbiguity && BS && FoundBS) {
                  // Multiple clobber on the same value, we have a problem
                  Clobbering[1] = Store;
                  diagnoseAmbiguity(Load);
                  BS = nullptr;
                  return false;
                }
                Clobbering[0] = Store;
                BS = FoundBS;
                // Continue if BS is not found or looking for ambiguity
                return ReportAmbiguity || !BS;
              }
            }
            return true;
          });
      return BS;
    } else if (ReportAmbiguity) {
      if (auto *PHI = dyn_cast<PHINode>(BSPtr)) {
        if (PHI->getNumIncomingValues() >= 2) {
          if (auto *IncomingI = dyn_cast<Instruction>(PHI->getIncomingValue(0)))
            Clobbering[0] = IncomingI;
          else
            Clobbering[0] = PHI->getIncomingBlock(0)->getTerminator();
          if (auto *IncomingI = dyn_cast<Instruction>(PHI->getIncomingValue(1)))
            Clobbering[1] = IncomingI;
          else
            Clobbering[1] = PHI->getIncomingBlock(1)->getTerminator();
          diagnoseAmbiguity(
              dyn_cast<Instruction>(RippleBlockShapePtr.getUser()));
        }
      }
    }
    return nullptr;
  };
  return getBlockShapeIntrinsicHelper(RippleBlockShapePtr);
}

TensorShape::DimSize
Ripple::getRippleGetSizeValue(const IntrinsicInst *RippleGetSize) {
  assert(RippleGetSize->getIntrinsicID() == Intrinsic::ripple_block_getsize);
  auto *ShapeII = getBlockShapeIntrinsic(RippleGetSize->getArgOperandUse(0));
  // Checked by checkBlockShapeUsage
  assert(ShapeII);
  PEIdentifier ProcElem = *getConstantOperandValue(ShapeII, 0);
  auto Dimension = *getConstantOperandValue(RippleGetSize, 1);
  if (Dimension < PERank(ProcElem)) {
    const auto &BlockShape = setShapeToTensorShape(ShapeII);
    return BlockShape[rippleToTensor({ProcElem, Dimension})];
  } else
    return 1;
}

TensorShape
Ripple::setShapeToTensorShape(const IntrinsicInst *RippleSetShape) const {
  assert(RippleSetShape->getIntrinsicID() == Intrinsic::ripple_block_setshape);
  TensorShape::Shape BlockShape(tensorRank(), DimSize(1));
  PEIdentifier ProcElem = *getConstantOperandValue(RippleSetShape, 0);
  for (unsigned ArgIdx = 1, E = RippleSetShape->arg_size(); ArgIdx < E;
       ++ArgIdx) {
    DimSize DS = *getConstantOperandValue(RippleSetShape, ArgIdx);
    if (DS > 1) {
      // This should be true by construction; the rank is the maximum of all the
      // ripple_set_block_shape for the same PE in this function
      assert(PERank(ProcElem) > ArgIdx - 1);
      BlockShape[rippleToTensor({ProcElem, ArgIdx - 1})] = DS;
    }
  }
  return TensorShape(std::move(BlockShape));
}

void Ripple::specializationCallBlockShapeArgs(
    const CallInst *CI,
    SmallVectorImpl<std::pair<unsigned, IntrinsicInst *>> &BSArgs) {
  unsigned ArgNo = 0;
  for (auto &Arg : CI->args()) {
    if (IntrinsicInst *BSI = getBlockShapeIntrinsic(Arg))
      BSArgs.push_back({ArgNo, BSI});
    ArgNo++;
  }
}

Error Ripple::checkBlockShapeUsage(const Function &F) {
  for (auto &I : instructions(F)) {
    if (const IntrinsicInst *II = rippleIntrinsicsWithBlockShapeOperand(&I)) {
      auto &BlockShapeOperandUse = II->getArgOperandUse(0);
      auto *BS = getBlockShapeIntrinsic<true>(BlockShapeOperandUse);
      if (!BS) {
        DiagnosticInfoRippleWithLoc DI(
            DS_Error, F, sanitizeRippleLocation(II),
            "Cannot locate the source block shape of this ripple intrinsic");
        F.getContext().diagnose(DI);
        return createStringError(inconvertibleErrorCode(),
                                 "Cannot find BS for ripple construct");
      }
    } else if (auto *CallI = dyn_cast<CallInst>(&I)) {
      for (auto &Arg : CallI->args()) {
        if (getBlockShapeIntrinsic(Arg)) {
          LLVM_DEBUG(dbgs() << "Found call with BS " << *CallI << "\n");
          if (!CallI->getCalledFunction() ||
              (!CallI->getCalledFunction()->isIntrinsic() &&
               CallI->getCalledFunction()->isDeclaration())) {
            DiagnosticInfoRippleWithLoc DI(
                DS_Error, F, sanitizeRippleLocation(CallI),
                "Passing a ripple block shape to a function call with no known "
                "definition is not allowed. Make sure that the function is "
                "available for ripple processing.");
            F.getContext().diagnose(DI);
            return createStringError(inconvertibleErrorCode(),
                                     "Call to declaration with BS");
          }
        }
      }
    }
  }
  return Error::success();
}

OptimizationRemarkEmitter &Ripple::getORE() { return Ripple::ORE; }

bool Ripple::rippleVectorizeCall(const CallInst &CI) const {
  return none_of(CI.args(),
                 [](auto &Use) { return Use->getType()->isVectorTy(); }) &&
         any_of(CI.args(),
                [&](auto &Use) { return getRippleShape(Use).isVector(); });
}

void Ripple::emitRippleRemarks() {
  if (!ORE.enabled())
    return;

  auto sortDILocationByFileLineColumn = [](const DILocation *a,
                                           const DILocation *b) -> bool {
    if (!a || !b)
      return a < b;

    StringRef fileA = a->getFilename();
    StringRef fileB = b->getFilename();
    if (fileA != fileB)
      return fileA < fileB;
    if (a->getLine() != b->getLine())
      return a->getLine() < b->getLine();
    return a->getColumn() < b->getColumn();
  };

  MapVector<DILocation *, Instruction *> UniqueLastDILocation;
  for (auto &I : instructions(F)) {
    DILocation *DIL = I.getDebugLoc();
    bool VectorCall =
        isa<CallInst>(I) && rippleVectorizeCall(cast<CallInst>(I));
    if (DIL && DIL->getLine() > 0 &&
        (getRippleShape(&I).isVector() || VectorCall)) {
      auto [KeyValPair, Inserted] = UniqueLastDILocation.try_emplace(DIL, &I);
      // Keep CallInst because this information is more important than any
      // implicit cast that may occur at this position
      if (!isa<CallInst>(KeyValPair->second))
        KeyValPair->second = &I;
    }
  }
  // Sort them so that we emit the remarks in a file > line > column order
  auto UniqDILocs(UniqueLastDILocation.takeVector());
  llvm::sort(
      UniqDILocs,
      [sortDILocationByFileLineColumn](auto &PairA, auto &PairB) -> bool {
        return sortDILocationByFileLineColumn(PairA.first, PairB.first);
      });

  auto PrintProtoShape =
      [&](
          DiagnosticInfoIROptimization &R, const TensorShape &ReturnShape,
          const SmallVectorImpl<const TensorShape *> &ArgShapes,
          std::function<bool(size_t)> IsBroadcast = [](size_t) {
            return false;
          }) {
        std::string RetShapeStr;
        {
          raw_string_ostream RSOS(RetShapeStr);
          RSOS << ReturnShape;
        }
        R << "(";
        for (unsigned ArgIdx = 0, E = ArgShapes.size(); ArgIdx < E; ++ArgIdx) {
          if (ArgIdx > 0)
            R << ", ";
          if (IsBroadcast(ArgIdx) && *ArgShapes[ArgIdx] != ReturnShape) {
            std::string ArgShapeStr;
            {
              raw_string_ostream ArgS(ArgShapeStr);
              ArgS << *ArgShapes[ArgIdx];
            }
            R << ArgShapeStr << "->" << RetShapeStr;
          } else if (ArgShapes[ArgIdx]->isVector()) {
            std::string ArgShapeStr;
            {
              raw_string_ostream ArgS(ArgShapeStr);
              ArgS << *ArgShapes[ArgIdx];
            }
            R << ArgShapeStr;
          } else {
            R << "unchanged";
          }
        }
        R << ") -> ";
        if (ReturnShape.isVector())
          R << RetShapeStr;
        else
          R << "unchanged";
      };
  if (isPendingRippleSpecialization(F)) {
    ORE.emit([&] {
      SmallVector<const TensorShape *> SpecializationArgShapes;
      transform(
          llvm::make_range(F.arg_begin(), F.arg_begin() + F.arg_size() / 2),
          std::back_inserter(SpecializationArgShapes),
          [&](const Argument &A) { return &getRippleShape(&A); });
      const TensorShape *ReturnShape = nullptr;
      for (const auto &I : instructions(F))
        if (auto *Return = dyn_cast<ReturnInst>(&I)) {
          ReturnShape = &getRippleShape(Return);
          break;
        }

      OptimizationRemark R(DEBUG_TYPE, "IFConvert", &F);
      R << "function '" << getOriginalName(F.getName())
        << "' specialized for tensor operands {shape: ";
      PrintProtoShape(R, *ReturnShape, SpecializationArgShapes);
      R << "}";
      return R;
    });
  }
  for (auto &P : UniqDILocs) {
    Instruction *I = P.second;
    if (isa<BranchInst>(I) || isa<SwitchInst>(I)) {
      auto BranchShape = getRippleShape(I);
      std::string MaskShapeStr;
      {
        raw_string_ostream RSOS(MaskShapeStr);
        RSOS << BranchShape;
      }
      ORE.emit([&] {
        OptimizationRemark R(DEBUG_TYPE, "IFConvert", I);
        R << "branch if-converted to predicated vector form; control flow "
             "flattened using mask from vectorized condition {mask-shape: "
          << MaskShapeStr << "}";
        return R;
      });
    } else if (auto *CallI = dyn_cast<CallInst>(I)) {
      auto CallRetShape = getRippleShape(CallI);
      std::string RetShapeStr;
      {
        raw_string_ostream RSOS(RetShapeStr);
        RSOS << CallRetShape;
      }
      bool IsMaskedCall = MaskedCalls.contains(CallI);
      if (isRippleIntrinsics(CallI)) {
        ORE.emit([&] {
          OptimizationRemark R(DEBUG_TYPE, "VectorPromotion", I);
          SmallVector<const TensorShape *> ArgShapes;
          ArgShapes.reserve(CallI->arg_size());
          transform(CallI->args(), std::back_inserter(ArgShapes),
                    [&](auto &Use) { return &getRippleShape(Use); });

          R << "call to ripple API '" << CallI->getCalledOperand()->getName()
            << "' {shape: ";
          PrintProtoShape(R, getRippleShape(CallI), ArgShapes);
          R << "}";
          return R;
        });
      } else if (auto *ExternFun =
                     findExternalRippleFunctionFor(CallI, IsMaskedCall)) {
        auto ExternFunRetShape = ExternFun->returnTensorShape(CallI, *this);
        // This is checked during the shape propagation phase, in
        // inferShapeFromOperands, CallInst case
        if (!ExternFunRetShape)
          report_fatal_error("Broadcast failure during remark generation");
        ORE.emit([&] {
          OptimizationRemark R(DEBUG_TYPE, "ExternalCallDispatch", I);
          SmallVector<const TensorShape *> ArgShapes;
          ArgShapes.reserve(CallI->arg_size());
          transform(CallI->args(), std::back_inserter(ArgShapes),
                    [&](auto &Use) { return &getRippleShape(Use); });
          std::function<bool(size_t)> isBroadcastable = [](size_t) {
            return false;
          };
          if (!ExternFun->isElementWiseFunction()) {
          } else {
            const ArrayRef<TensorShape> ExternArgOpShapes =
                ExternFun->argOperandShapes();
            isBroadcastable = [ExternArgOpShapes](size_t ArgIdx) -> bool {
              return ExternArgOpShapes[ArgIdx].isVector();
            };
          }
          R << "dispatched call '" << CallI->getCalledOperand()->getName()
            << "' to external ripple function '"
            << ExternFun->getFunction()->getName() << "' {shape: ";
          PrintProtoShape(R, ExternFunRetShape.get(), ArgShapes,
                          isBroadcastable);
          R << "}";
          return R;
        });
      } else if (Intrinsic::ID VectorIntrId =
                     getVectorIntrinsicIDForCall(CallI, &targetLibraryInfo);
                 CallRetShape.isVector() &&
                 VectorIntrId != Intrinsic::not_intrinsic) {
        ORE.emit([&] {
          auto [ArgumentShapes, RetTy] = promotedIntrinsicArgShapesAndReturnTy(
              *CallI, CallRetShape, VectorIntrId);
          OptimizationRemark R(DEBUG_TYPE, "IntrinsicCallPromotion", I);
          R << "scalar call promoted to vector form '"
            << CallI->getCalledOperand()->getName() << "' {shape: ";
          PrintProtoShape(R, CallRetShape, ArgumentShapes);
          R << "}";
          return R;
        });
      } else {
        auto [SpecializedFunction, ReturnShape] =
            getRippleSpecializationFor(*CallI);
        if (SpecializedFunction) {
          // Use temporary since capture of structure binding is >= c++20
          const TensorShape &RetS = ReturnShape;
          ORE.emit([&] {
            OptimizationRemark R(DEBUG_TYPE, "SpecializationCallPromotion", I);
            SmallVector<const TensorShape *> ArgShapes;
            ArgShapes.reserve(CallI->arg_size());
            transform(CallI->args(), std::back_inserter(ArgShapes),
                      [&](auto &Use) { return &getRippleShape(Use); });

            R << "specialized call to '" << CallI->getCalledOperand()->getName()
              << "'; ";
            for (size_t ArgIdx = 0; ArgIdx < CallI->arg_size(); ++ArgIdx) {
              if (ArgIdx)
                R << ", ";
              R << "arg" << std::to_string(ArgIdx)
                << (ArgShapes[ArgIdx]->isVector() ? " promoted" : " unchanged");
            }
            R << " {shape: ";
            PrintProtoShape(R, RetS, ArgShapes);
            R << "}";
            return R;
          });
        } else
          ORE.emit([&] {
            OptimizationRemarkMissed R(DEBUG_TYPE, "CallScalarDemotion", I);
            SmallVector<const TensorShape *> ArgShapes;
            ArgShapes.reserve(CallI->arg_size());
            transform(CallI->args(), std::back_inserter(ArgShapes),
                      [&](auto &Use) { return &getRippleShape(Use); });
            R << "call not vectorized: no suitable vector implementation found "
                 "for '"
              << CallI->getCalledOperand()->getName()
              << "'; falling back to scalar loop {shape: ";
            PrintProtoShape(R, CallRetShape, ArgShapes);
            R << "}";
            return R;
          });
      }
    } else {
      ORE.emit([&] {
        OptimizationRemark R(DEBUG_TYPE, "VectorPromotion", I);
        SmallVector<const TensorShape *> OperandShapes;
        OperandShapes.reserve(I->getNumOperands());
        transform(I->operands(), std::back_inserter(OperandShapes),
                  [&](auto &Use) { return &getRippleShape(Use); });

        R << "instruction promoted to tensor {shape: ";
        auto VOps = vectorizableOperands(I);
        size_t FirstBcastArg = std::distance(I->op_begin(), VOps.begin());
        size_t LastBcastArg =
            FirstBcastArg + std::distance(VOps.begin(), VOps.end());
        auto isBroadcastable = [FirstBcastArg, LastBcastArg](size_t ArgIdx) {
          return ArgIdx >= FirstBcastArg && ArgIdx < LastBcastArg;
        };
        PrintProtoShape(R, getRippleShape(I), OperandShapes, isBroadcastable);
        R << "}";
        return R;
      });
    }
  }
}

std::pair<SmallVector<const TensorShape *>, Type *>
Ripple::promotedIntrinsicArgShapesAndReturnTy(
    const CallInst &CI, const TensorShape &ToShape,
    Intrinsic::ID VectorIntrId) const {
  SmallVector<Type *> BcastedArgTypes;
  BcastedArgTypes.reserve(CI.arg_size());
  transform(CI.args(), std::back_inserter(BcastedArgTypes),
            [&ToShape](auto &Arg) -> Type * {
              Type *ArgTy = Arg->getType();
              if (ArgTy->isVectorTy())
                return ArgTy;
              Type *ScalTy = ArgTy->getScalarType();
              return VectorType::get(ScalTy, ToShape.flatShape(),
                                     /*Scalable*/ false);
            });
  FunctionType *FTy =
      Intrinsic::getType(CI.getContext(), VectorIntrId, BcastedArgTypes);
  SmallVector<const TensorShape *> ArgShapes;
  ArgShapes.reserve(CI.arg_size());
  for (size_t ArgIdx = 0; ArgIdx < CI.arg_size(); ++ArgIdx) {
    if (!CI.getArgOperand(ArgIdx)->getType()->isVectorTy() &&
        FTy->getParamType(ArgIdx) == BcastedArgTypes[ArgIdx])
      ArgShapes.push_back(&ToShape);
    else
      ArgShapes.push_back(&ShapeIgnoredByRipple);
  }
  return std::make_pair(std::move(ArgShapes), FTy->getReturnType());
}

std::optional<std::tuple<Type *, TensorShape, int, StringRef>>
ExternalRippleFunction::parseTensorType(StringRef S, const Function &Fn) {
  LLVM_DEBUG(dbgs() << "Tentative parse tensor type in " << S << "\n");
  constexpr auto NotAnIntegerError = "Expected an integer";
  // unspecified
  int ArgIdx = -1;
  if (S.starts_with(ArgShape)) {
    // Matching an argument index
    const Regex ArgRE("^arg([0-9]+)_");
    SmallVector<StringRef, 2> MatchResults;
    if (!ArgRE.match(S, &MatchResults)) {
      std::string ErrMsg;
      {
        raw_string_ostream RSO(ErrMsg);
        RSO << "the external ripple function '" << Fn.getName()
            << "' 'arg' tensor shape requires an index followed by an "
               "underscore (e.g., 'arg1_t2x4f32'); '"
            << S << "' is invalid";
      }
      DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
      Fn.getContext().diagnose(Diag);
      return std::nullopt;
    }
    // Advance S
    S = S.substr(MatchResults[0].size());
    if (MatchResults[1].getAsInteger(10, ArgIdx))
      llvm_unreachable(NotAnIntegerError);
    LLVM_DEBUG(dbgs() << "Matched argument shape Idx(" << ArgIdx << ")\n");
  } else if (S.consume_front(RetShape)) {
    LLVM_DEBUG(dbgs() << "Matched return shape\n");
    ArgIdx = -2;
  }
  LLVM_DEBUG(dbgs() << "Parsing Tshape: " << S << "\n");

  auto warningContextMessage = [&](raw_string_ostream &RSO) {
    RSO << "the external ripple function '" << Fn.getName() << "' ";
    if (ArgIdx == -2) {
      RSO << "'ret_'";
    } else if (ArgIdx >= 0) {
      RSO << "'arg" << ArgIdx << "_'";
    }
    RSO << " tensor shape";
  };

  // Parsing "t[0-9]+(x[0-9]+)*
  SmallVector<StringRef, 4> ShapeMatchResults;
  const Regex TShapeRE("^t([0-9]+)((x[0-9]+)*)");
  if (!TShapeRE.match(S, &ShapeMatchResults)) {
    if (ArgIdx != -1) {
      std::string ErrMsg;
      {
        raw_string_ostream RSO(ErrMsg);
        warningContextMessage(RSO);
        RSO << " is invalid '" << S
            << "'; expected 't<dims><type>' (e.g., 't2x4f32')";
      }
      DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
      Fn.getContext().diagnose(Diag);
    }
    return std::nullopt;
  }
  LLVM_DEBUG(dbgs() << "Matched: " << ShapeMatchResults[0] << "\n");
  // Advance S
  S = S.substr(ShapeMatchResults[0].size());
  LLVM_DEBUG(dbgs() << "Parsing tensor type: " << S << "\n");
  SmallVector<unsigned, 4> TensorDimShapes;
  TensorDimShapes.push_back(0);
  if (ShapeMatchResults[1].getAsInteger(10, TensorDimShapes.back()))
    llvm_unreachable(NotAnIntegerError);
  StringRef OtherDims = ShapeMatchResults[2];
  {
    // Now we get all the other dimensions
    SmallVector<StringRef> AllXShapes;
    const Regex XDimRE("x([0-9]+)");
    if (XDimRE.match(OtherDims, &AllXShapes)) {
      assert(ShapeMatchResults.size() % 2 == 0);
      for (unsigned Idx = 0; Idx < AllXShapes.size() / 2; Idx += 2) {
        TensorDimShapes.push_back(0);
        if (AllXShapes[Idx + 1].getAsInteger(10, TensorDimShapes.back()))
          llvm_unreachable(NotAnIntegerError);
      }
    }
  }
  TensorShape::Shape TShape(TensorDimShapes.size(), TensorShape::DimSize(1));
  std::copy(TensorDimShapes.begin(), TensorDimShapes.end(), TShape.begin());
  TensorShape TS(std::move(TShape));

  // Parsing the data type section coming after the tensor shape
  const Regex TypeStrRE("^(f16|bf16|f32|f64|i1|i8|i16|i32|i64|u8|u16|u32|u64)");
  SmallVector<StringRef, 2> TypeMatchResult;
  if (!TypeStrRE.match(S, &TypeMatchResult)) {
    std::string ErrMsg;
    {
      raw_string_ostream RSO(ErrMsg);
      warningContextMessage(RSO);
      RSO << "'s type is not valid starting at '" << S
          << "'; expected one of: f16, bf16, f32, f64, i1, i8, i16, i32, i64, "
             "u8, u16, u32 or u64";
    }
    DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
    Fn.getContext().diagnose(Diag);
    return std::nullopt;
  }
  // Advance S
  if (S.size() <= TypeMatchResult[0].size() ||
      S[TypeMatchResult[0].size()] != '_') {
    std::string ErrMsg;
    {
      raw_string_ostream RSO(ErrMsg);
      warningContextMessage(RSO);
      RSO << " expects '_' after '" << ShapeMatchResults[0]
          << TypeMatchResult[0] << "'";
      StringRef Left = S.substr(TypeMatchResult[0].size());
      if (!Left.empty()) {
        RSO << " before '" << Left << "'";
      }
    }
    DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
    Fn.getContext().diagnose(Diag);
    return std::nullopt;
  }
  S = S.substr(TypeMatchResult[0].size() + 1);

  bool IsFloat = TypeMatchResult[1].consume_front("f");
  bool IsBFloat = TypeMatchResult[1].consume_front("bf");
  bool IsInteger = !IsFloat && !IsBFloat;
  if (IsInteger) {
    // Remove the 'u'/'i' part
    TypeMatchResult[1] = TypeMatchResult[1].substr(1);
  }
  unsigned Size;
  if (TypeMatchResult[1].getAsInteger(10, Size))
    llvm_unreachable(NotAnIntegerError);
  Type *DataTy = nullptr;
  auto &C = Fn.getContext();
  if (IsFloat) {
    switch (Size) {
    case 16:
      DataTy = Type::getHalfTy(C);
      break;
    case 32:
      DataTy = Type::getFloatTy(C);
      break;
    case 64:
      DataTy = Type::getDoubleTy(C);
      break;
    default:
      llvm_unreachable("Non supported floating point type");
    }
  } else if (IsBFloat) {
    DataTy = Type::getBFloatTy(C);
  } else {
    DataTy = Type::getIntNTy(C, Size);
  }

  return std::make_tuple(DataTy, std::move(TS), ArgIdx, S);
}

std::optional<StringRef> ExternalRippleFunction::parseSignature(
    StringRef S, const Function &Fn,
    SmallVectorImpl<ArgumentSignatureInfo> &ArgumentShapes,
    ReturnSignatureInfo &ReturnShape, bool &IsUniform) {

  IsUniform = S.consume_front(UniformShape);
  if (IsUniform) {
    if (auto TT = parseTensorType(S, Fn)) {
      auto &[ParsedTy, TS, ArgIdx, NewS] = TT.value();
      // Advance S
      S = NewS;
      if (ArgIdx != -1) {
        // We cannot use uniform shape and use an "arg" or "ret" specifier
        std::string ErrMsg;
        {
          raw_string_ostream RSO(ErrMsg);
          RSO << "in the external ripple function '" << Fn.getName()
              << "' 'uniform_' cannot be combined with '";
          if (ArgIdx == -2)
            RSO << "ret_";
          else
            RSO << "arg" << ArgIdx << "_";
          RSO << "'; use one or the other";
        }
        DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
        Fn.getContext().diagnose(Diag);
        return std::nullopt;
      }
      // Set return shape and type
      auto &[RetTy, RetShape] = ReturnShape;
      RetTy = ParsedTy;
      std::swap(RetShape, TS);
    } else {
      // Warn that we parsed `uniform_` but couldn't find a valid tensor shape
      // following it
      std::string ErrMsg;
      {
        raw_string_ostream RSO(ErrMsg);
        RSO << "the external ripple function '" << Fn.getName()
            << "' uniform shape requires a tensor shape specifier ";
        if (S.empty())
          RSO << "and a function name, e.g., 'uniform_t2x4f32_myFunction'";
        else
          RSO << "between 'uniform_' and '" << S << "', e.g., 'uniform_t2x4f32_"
              << S << "'";
      }
      DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
      Fn.getContext().diagnose(Diag);
    }
  } else {
    // Keep track of the indices for duplicare error reporting
    SmallSet<int, 16> AllIndices;
    // The rule is that we start at zero, assign and increment when
    // undefined or take the next index when defined
    unsigned ArgIdx = 0;
    while (auto TT = parseTensorType(S, Fn)) {
      auto &[ParsedTy, TS, SigIndex, SR] = TT.value();
      if (SigIndex >= 0)
        ArgIdx = SigIndex;

      // Check for duplicate shape definitions
      auto [_, Inserted] = AllIndices.insert(SigIndex == -2 ? -2 : ArgIdx);
      if (!Inserted) {
        std::string ErrMsg;
        {
          raw_string_ostream RSO(ErrMsg);
          RSO << "the external ripple function '" << Fn.getName()
              << "' has duplicate tensor shape specified for '";
          if (SigIndex == -2)
            RSO << "ret";
          else
            RSO << "arg" << ArgIdx;
          RSO << "'; second definition starts at '" << S << "'";
        }
        DiagnosticInfoRipple Diag(DS_Warning, ErrMsg);
        Fn.getContext().diagnose(Diag);
        return std::nullopt;
      }
      // Advance S
      S = SR;
      if (SigIndex == -2) {
        auto &[RetTy, RetShape] = ReturnShape;
        RetTy = ParsedTy;
        std::swap(RetShape, TS);
      } else {
        ArgumentShapes.push_back(
            std::make_tuple(ParsedTy, TS, static_cast<unsigned>(ArgIdx)));
        // Increment the index for the next potential parsing
        ArgIdx++;
      }
    }
  }
  return S;
}

////////////////////////////////////////////////////////////////////////////////
///                               RipplePass                                 ///
////////////////////////////////////////////////////////////////////////////////

PreservedAnalyses RipplePass::run(Function &F, FunctionAnalysisManager &AM) {
  LLVM_DEBUG(dbgs() << "Applying Ripple pass to '" << F.getName() << "'\n");

  // WIP machine model w/ 1 vector dimension
  std::vector<std::pair<Ripple::PEIdentifier, Ripple::DimType>> dimensionTypes =
      {{0, Ripple::VectorDimension}};

  Ripple rippleExpander(TM, F, AM, dimensionTypes, PS, SpecializationsPending,
                        SpecializationsAvailable);
  // Check that the Ripple intrinsics are adhere to the specification
  Error e = rippleExpander.validate();
  if (e) {
    std::string errMsg = toString(std::move(e));
    report_fatal_error(StringRef(errMsg), false);
  }

  PreservedAnalyses PA = rippleExpander.run();

  return PA;
}
