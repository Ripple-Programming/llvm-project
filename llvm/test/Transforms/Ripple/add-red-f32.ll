; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -passes='module(ripple,function(dce))' -S < %s | FileCheck %s --implicit-check-not="warning:"

target datalayout = "e-m:e-p:32:32:32-a:0-n16:32-i64:64:64-i32:32:32-i16:16:16-i1:8:8-f32:32:32-f64:64:64-v32:32:32-v64:64:64-v512:512:512-v1024:1024:1024-v2048:2048:2048"

; Function Attrs: nounwind
define dso_local void @vadd_red(i32 noundef %N, ptr noalias nocapture noundef readonly %A, ptr noalias nocapture noundef writeonly %OUT) local_unnamed_addr {
; CHECK-LABEL: @vadd_red(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[CMP21:%.*]] = icmp ugt i32 [[N:%.*]], 32
; CHECK-NEXT:    br i1 [[CMP21]], label [[FOR_BODY:%.*]], label [[FOR_COND_CLEANUP:%.*]]
; CHECK:       for.cond.cleanup:
; CHECK-NEXT:    [[RES_0_LCSSA_RIPPLE_VECTORIZED:%.*]] = phi <32 x float> [ zeroinitializer, [[ENTRY:%.*]] ], [ [[ADD3_RIPPLE_VECTORIZED:%.*]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = and i32 [[N]], 31
; CHECK-NEXT:    [[CMP5:%.*]] = icmp eq i32 [[TMP0]], 0
; CHECK-NEXT:    br i1 [[CMP5]], label [[IF_THEN:%.*]], label [[IF_END:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[RES_023_RIPPLE_VECTORIZED:%.*]] = phi <32 x float> [ [[ADD3_RIPPLE_VECTORIZED]], [[FOR_BODY]] ], [ zeroinitializer, [[ENTRY]] ]
; CHECK-NEXT:    [[I_022:%.*]] = phi i32 [ [[ADD4:%.*]], [[FOR_BODY]] ], [ 0, [[ENTRY]] ]
; CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr float, ptr [[A:%.*]], i32 [[I_022]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <32 x float>, ptr [[ARRAYIDX2]], align 4
; CHECK-NEXT:    [[ADD3_RIPPLE_VECTORIZED]] = fadd <32 x float> [[RES_023_RIPPLE_VECTORIZED]], [[TMP1]]
; CHECK-NEXT:    [[ADD4]] = add i32 [[I_022]], 32
; CHECK-NEXT:    [[ADD:%.*]] = add i32 [[I_022]], 64
; CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 [[ADD]], [[N]]
; CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY]], label [[FOR_COND_CLEANUP]]
; CHECK:       if.then:
; CHECK-NEXT:    [[DIV7:%.*]] = lshr i32 [[N]], 5
; CHECK-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr float, ptr [[A]], i32 [[DIV7]]
; CHECK-NEXT:    [[TMP3:%.*]] = load <32 x float>, ptr [[ARRAYIDX8]], align 4
; CHECK-NEXT:    [[ADD9_RIPPLE_VECTORIZED:%.*]] = fadd <32 x float> [[RES_0_LCSSA_RIPPLE_VECTORIZED]], [[TMP3]]
; CHECK-NEXT:    br label [[IF_THEN]]
; CHECK:       if.end:
; CHECK-NEXT:    [[RES_1_RIPPLE_VECTORIZED_RIPPLE_REDUCTION_PARTIAL_MASKING:%.*]] = phi <32 x float> [ [[ADD9_RIPPLE_VECTORIZED]], [[IF_END]] ], [ [[RES_0_LCSSA_RIPPLE_VECTORIZED]], [[FOR_COND_CLEANUP]] ]
; CHECK-NEXT:    [[RES_1_RIPPLE_VECTORIZED_RIPPLE_REDUCTION:%.*]] = call reassoc float @llvm.vector.reduce.fadd.v32f32(float -0.000000e+00, <32 x float> [[RES_1_RIPPLE_VECTORIZED_RIPPLE_REDUCTION_PARTIAL_MASKING]])
; CHECK-NEXT:    store float [[RES_1_RIPPLE_VECTORIZED_RIPPLE_REDUCTION]], ptr [[OUT:%.*]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %BS = tail call ptr @llvm.ripple.block.setshape(i32 0, i32 32, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1)
  %0 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %cmp21 = icmp ult i32 %0, %N
  br i1 %cmp21, label %for.body, label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %for.body, %entry
  %res.0.lcssa = phi float [ 0.000000e+00, %entry ], [ %add3, %for.body ]
  %1 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %div = udiv i32 %N, %1
  %2 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %mul = mul i32 %2, %div
  %cmp5 = icmp ult i32 %mul, %N
  br i1 %cmp5, label %if.then, label %if.end

for.body:                                         ; preds = %entry, %for.body
  %res.023 = phi float [ %add3, %for.body ], [ 0.000000e+00, %entry ]
  %i.022 = phi i32 [ %add4, %for.body ], [ 0, %entry ]
  %3 = tail call i32 @llvm.ripple.block.index(ptr %BS, i32 0)
  %4 = getelementptr float, ptr %A, i32 %i.022
  %arrayidx2 = getelementptr float, ptr %4, i32 %3
  %5 = load float, ptr %arrayidx2, align 4
  %add3 = fadd float %res.023, %5
  %6 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %add4 = add i32 %6, %i.022
  %7 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %add = add i32 %7, %add4
  %cmp = icmp ult i32 %add, %N
  br i1 %cmp, label %for.body, label %for.cond.cleanup

if.then:                                          ; preds = %for.cond.cleanup
  %8 = tail call i32 @llvm.ripple.block.index(ptr %BS, i32 0)
  %9 = getelementptr float, ptr %A, i32 %div
  %arrayidx8 = getelementptr float, ptr %9, i32 %8
  %10 = load float, ptr %arrayidx8, align 4
  %add9 = fadd float %res.0.lcssa, %10
  br label %if.end

if.end:                                           ; preds = %if.then, %for.cond.cleanup
  %res.1 = phi float [ %add9, %if.then ], [ %res.0.lcssa, %for.cond.cleanup ]
  %call = tail call float @llvm.ripple.reduce.fadd(i64 1, float noundef %res.1)
  store float %call, ptr %OUT, align 4
  ret void
}
