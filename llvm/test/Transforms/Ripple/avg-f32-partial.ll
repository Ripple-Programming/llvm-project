; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -passes=ripple,dce -S < %s | FileCheck %s --implicit-check-not="warning:"

target datalayout = "e-m:e-p:32:32:32-a:0-n16:32-i64:64:64-i32:32:32-i16:16:16-i1:8:8-f32:32:32-f64:64:64-v32:32:32-v64:64:64-v512:512:512-v1024:1024:1024-v2048:2048:2048"

; Function Attrs: nounwind
define dso_local void @avg(i32 noundef %n, ptr noalias nocapture noundef readonly %in, ptr noalias nocapture noundef writeonly %out) local_unnamed_addr {
; CHECK-LABEL: @avg(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[CONV:%.*]] = uitofp i32 [[N:%.*]] to float
; CHECK-NEXT:    br label [[FOR_COND1_PREHEADER:%.*]]
; CHECK:       for.cond1.preheader:
; CHECK-NEXT:    [[I_036:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[INC:%.*]], [[IF_END:%.*]] ]
; CHECK-NEXT:    [[CMP2_NOT33:%.*]] = icmp ult i32 [[N]], 32
; CHECK-NEXT:    br i1 [[CMP2_NOT33]], label [[FOR_COND_CLEANUP3:%.*]], label [[FOR_BODY4_LR_PH:%.*]]
; CHECK:       for.body4.lr.ph:
; CHECK-NEXT:    [[TMP0:%.*]] = mul nsw i32 [[I_036]], [[N]]
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds float, ptr [[IN:%.*]], i32 [[TMP0]]
; CHECK-NEXT:    br label [[FOR_BODY4:%.*]]
; CHECK:       for.cond.cleanup:
; CHECK-NEXT:    ret void
; CHECK:       for.cond.cleanup3:
; CHECK-NEXT:    [[AVG_VAL_0_LCSSA_RIPPLE_VECTORIZED:%.*]] = phi <32 x float> [ zeroinitializer, [[FOR_COND1_PREHEADER]] ], [ [[ADD7_RIPPLE_VECTORIZED:%.*]], [[FOR_BODY4]] ]
; CHECK-NEXT:    [[SUB:%.*]] = add i32 [[N]], 31
; CHECK-NEXT:    [[MUL:%.*]] = and i32 [[SUB]], -32
; CHECK-NEXT:    [[CMP10:%.*]] = icmp ult i32 [[MUL]], [[N]]
; CHECK-NEXT:    br i1 [[CMP10]], label [[IF_THEN:%.*]], label [[IF_END]]
; CHECK:       for.body4:
; CHECK-NEXT:    [[J_035:%.*]] = phi i32 [ 0, [[FOR_BODY4_LR_PH]] ], [ [[ADD8:%.*]], [[FOR_BODY4]] ]
; CHECK-NEXT:    [[AVG_VAL_034_RIPPLE_VECTORIZED:%.*]] = phi <32 x float> [ zeroinitializer, [[FOR_BODY4_LR_PH]] ], [ [[ADD7_RIPPLE_VECTORIZED]], [[FOR_BODY4]] ]
; CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr float, ptr [[ARRAYIDX]], i32 [[J_035]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <32 x float>, ptr [[ARRAYIDX6]], align 4
; CHECK-NEXT:    [[ADD7_RIPPLE_VECTORIZED]] = fadd <32 x float> [[AVG_VAL_034_RIPPLE_VECTORIZED]], [[TMP2]]
; CHECK-NEXT:    [[ADD8]] = add i32 [[J_035]], 32
; CHECK-NEXT:    [[ADD:%.*]] = add i32 [[J_035]], 64
; CHECK-NEXT:    [[CMP2_NOT:%.*]] = icmp ugt i32 [[ADD]], [[N]]
; CHECK-NEXT:    br i1 [[CMP2_NOT]], label [[FOR_COND_CLEANUP3]], label [[FOR_BODY4]]
; CHECK:       if.then:
; CHECK-NEXT:    [[TMP3:%.*]] = mul nsw i32 [[I_036]], [[N]]
; CHECK-NEXT:    [[ARRAYIDX11:%.*]] = getelementptr inbounds float, ptr [[IN]], i32 [[TMP3]]
; CHECK-NEXT:    [[ARRAYIDX13:%.*]] = getelementptr float, ptr [[ARRAYIDX11]], i32 [[MUL]]
; CHECK-NEXT:    [[TMP5:%.*]] = load <32 x float>, ptr [[ARRAYIDX13]], align 4
; CHECK-NEXT:    [[ADD14_RIPPLE_VECTORIZED:%.*]] = fadd <32 x float> [[AVG_VAL_0_LCSSA_RIPPLE_VECTORIZED]], [[TMP5]]
; CHECK-NEXT:    br label [[IF_END]]
; CHECK:       if.end:
; CHECK-NEXT:    [[AVG_VAL_1_RIPPLE_VECTORIZED:%.*]] = phi <32 x float> [ [[ADD14_RIPPLE_VECTORIZED]], [[IF_THEN]] ], [ [[AVG_VAL_0_LCSSA_RIPPLE_VECTORIZED]], [[FOR_COND_CLEANUP3]] ]
; CHECK-NEXT:    [[AVG_VAL_1_RIPPLE_VECTORIZED_RIPPLE_REDUCTION:%.*]] = call reassoc float @llvm.vector.reduce.fadd.v32f32(float -0.000000e+00, <32 x float> [[AVG_VAL_1_RIPPLE_VECTORIZED]])
; CHECK-NEXT:    [[DIV15:%.*]] = fdiv float [[AVG_VAL_1_RIPPLE_VECTORIZED_RIPPLE_REDUCTION]], [[CONV]]
; CHECK-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds nuw float, ptr [[OUT:%.*]], i32 [[I_036]]
; CHECK-NEXT:    store float [[DIV15]], ptr [[ARRAYIDX16]], align 4
; CHECK-NEXT:    [[INC]] = add nuw nsw i32 [[I_036]], 1
; CHECK-NEXT:    [[EXITCOND_NOT:%.*]] = icmp eq i32 [[INC]], 32
; CHECK-NEXT:    br i1 [[EXITCOND_NOT]], label [[FOR_COND_CLEANUP:%.*]], label [[FOR_COND1_PREHEADER]]
;
entry:
  %BS = tail call ptr @llvm.ripple.block.setshape(i32 0, i32 32, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1)
  %add9 = add i32 %n, -1
  %conv = uitofp i32 %n to float
  br label %for.cond1.preheader

for.cond1.preheader:                              ; preds = %entry, %if.end
  %i.036 = phi i32 [ 0, %entry ], [ %inc, %if.end ]
  %0 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %cmp2.not33 = icmp ugt i32 %0, %n
  br i1 %cmp2.not33, label %for.cond.cleanup3, label %for.body4.lr.ph

for.body4.lr.ph:                                  ; preds = %for.cond1.preheader
  %1 = mul nsw i32 %i.036, %n
  %arrayidx = getelementptr inbounds float, ptr %in, i32 %1
  br label %for.body4

for.cond.cleanup:                                 ; preds = %if.end
  ret void

for.cond.cleanup3:                                ; preds = %for.body4, %for.cond1.preheader
  %avg_val.0.lcssa = phi float [ 0.000000e+00, %for.cond1.preheader ], [ %add7, %for.body4 ]
  %2 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %sub = add i32 %add9, %2
  %3 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %div = udiv i32 %sub, %3
  %4 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %mul = mul i32 %4, %div
  %cmp10 = icmp ult i32 %mul, %n
  br i1 %cmp10, label %if.then, label %if.end

for.body4:                                        ; preds = %for.body4.lr.ph, %for.body4
  %j.035 = phi i32 [ 0, %for.body4.lr.ph ], [ %add8, %for.body4 ]
  %avg_val.034 = phi float [ 0.000000e+00, %for.body4.lr.ph ], [ %add7, %for.body4 ]
  %5 = tail call i32 @llvm.ripple.block.index(ptr %BS, i32 0)
  %6 = getelementptr float, ptr %arrayidx, i32 %j.035
  %arrayidx6 = getelementptr float, ptr %6, i32 %5
  %7 = load float, ptr %arrayidx6, align 4
  %add7 = fadd float %avg_val.034, %7
  %8 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %add8 = add i32 %8, %j.035
  %9 = tail call i32 @llvm.ripple.block.getsize(ptr %BS, i32 0)
  %add = add i32 %9, %add8
  %cmp2.not = icmp ugt i32 %add, %n
  br i1 %cmp2.not, label %for.cond.cleanup3, label %for.body4

if.then:                                          ; preds = %for.cond.cleanup3
  %10 = mul nsw i32 %i.036, %n
  %arrayidx11 = getelementptr inbounds float, ptr %in, i32 %10
  %11 = tail call i32 @llvm.ripple.block.index(ptr %BS, i32 0)
  %12 = getelementptr float, ptr %arrayidx11, i32 %mul
  %arrayidx13 = getelementptr float, ptr %12, i32 %11
  %13 = load float, ptr %arrayidx13, align 4
  %add14 = fadd float %avg_val.0.lcssa, %13
  br label %if.end

if.end:                                           ; preds = %if.then, %for.cond.cleanup3
  %avg_val.1 = phi float [ %add14, %if.then ], [ %avg_val.0.lcssa, %for.cond.cleanup3 ]
  %call = tail call float @llvm.ripple.reduce.fadd(i64 1, float noundef %avg_val.1)
  %div15 = fdiv float %call, %conv
  %arrayidx16 = getelementptr inbounds float, ptr %out, i32 %i.036
  store float %div15, ptr %arrayidx16, align 4
  %inc = add nuw nsw i32 %i.036, 1
  %exitcond.not = icmp eq i32 %inc, 32
  br i1 %exitcond.not, label %for.cond.cleanup, label %for.cond1.preheader
}
